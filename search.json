[{"path":"index.html","id":"abstract-and-forward","chapter":"Abstract and forward","heading":"Abstract and forward","text":"Additionally, book least partially inspired vestigial lament youth. took math class always forced show work, hypocritical writers textbooks didn’t feel though precept applied . invariably tricked test algebraic edge-case entirely new . also incumbent blame teachers . uninterested laboring concepts vain hope understanding deeper meaning. stuff formulaic. Just demonstrate exactly solve problems can similar problems. people wanted understand concept. noble goal, contend need overblown ineffective many cases. want right wrong limited way. want show naked truth. book exposure. end serves four main purposes:give analytically grounded toolbox can build onTo teach sports team’s business worksTo demonstrate apply knowledge real problems achieve desired outcomesTo build reference manual solutions common problemsThe first three items list foundations core business strategy. need experience put together effective strategies. text give head-start working analytical tasks help support strategy. won’t strategy analytics expert, ’ll better understand approach many common problems. Analytics (field) great tool understanding many problems, limits. keep quote mind:hammer, everything looks like nail.famous Law Instrument1. Analytics data answer every question. Although many people seem think way. Don’t person. better . book written people want demonstrable grasp application analytics sports business setting. Due technical nature material, better suited people want learn work. less-well suited people want understand application theory.technology democratized analytics, analytics still black-box many managers. people depend tools. Instead, retreat . Consider quote: “Expert Power influence wielded result expertise, special skill, knowledge.” (Stephen P. Robbins 2012) Understanding tools techniques gives degree expert power. However, manager without knowledge idiom “tail wags dog” applies. don’t understand concepts, want use can lose credibility employees look less competent people managing. going explain basic analytics tangible way demonstrate related business strategy context professional sports team.don’t need know script code leverage book, programming/scripting knowledge enormous benefit. said, want understand basic scripting make full use text. going heavily exposed need learn working knowledge . want work implementing solutions field, still necessary prerequisite. code ’ll find book written R (R Core Team 2022). fact, entire book written using R libraries using free software. ’ll mostly follow best practices defined “Advanced R” (Wickham 2014). Python become popular recent years, find R suitable non-programmers.R great ways weird others. idiosyncrasies make operate little differently many programming languages. noticeable difference flow control, R loops avoided favor suite vectorized functions. don’t know means, going learn , don’t worry. examples ’ll use common constructs won’t penalized run-time. data-science perspective, python makes use many packages essentially duplicate R’s advantages (Pandas Numpy). ’ll use base R many examples also make use certain packages superior base methods. ’ll come away appreciation language whether want appreciation .Additionally, hope expose lot reading material concepts. fact, believe undergraduate education really give broad cursory knowledge base consisting facile concepts. probably didn’t put lot concepts use practice, know things like “want find area curve can probably use Integral.” great. means can Google-problems really quickly. mention lot books text. ones come across trying solve problems. mention make life little easier. Analytics large field know everything. just need know find answers people probably done . Hopefully, introduced enough concepts able speak number subjects verticals. ’ll make simple references Wikipedia pages whenever talk specific subjects don’t require -depth information --scope. enough get started.Furthermore, like teach little bit main business functions professional sports team. Sports unique business. ’s fortunes often outside control employees. strategies need reflect fact. don’t think efforts sell tickets going effective, ? strategy comes play. know team going dismantled year, change strategy?also going frame business strategy context club’s core business. Strategy means kinds different things. club mean plan achieve goal $x valuation. Sports clubs act little differently assets. Additionally, rapid proliferation SPACs designed invest directly teams. Valuations beginning reach levels single owner may soon thing past. change way clubs ran. club worth whatever tech oligarch, hedge-fund managing pirate, real-state heir, company, consortium wants pay thousands groups. necessarily worth discounted cash flow based revenue.Another important note although interesting, book isn’t corporate strategy. aren’t going talk setting shell companies, owning different parts value chain, human resources structure, shady finance accounting, new business opportunities extensions, agency agreements make businesses function. going take close look strategy makes wheels turn dollars exchanged club fans.perspective can create provisional definition type strategy hoping implement:Strategy discipline amplifying revenue generating efforts, operationalizing improved procedures, consultation decisions interact build cohesive business systems, longer shorter-term planning coordinated informed decision making.mouthful deconstructable, don’t want narrow broad. also don’t want extend platitudes. definition serves explain think problems specific context. serving function, also biased. Although try fair, may always honest. skeptical anything find , good thing. Please don’t take word. analyst, demand convinced. refer Sports Business, referring fundamental business verticals professional sports team. specific, referring Sales, Marketing, Operations, Sponsorships. functions Retail Media administration functions Accounting Human Resources work much like business function. Almost revenue professional sports team derived selling tickets2 primary focus book. covering base fundamentals hope covering fundamentals offer foundation heuristic value.Furthermore, book concerned . don’t understand Atkinson cycle 3 makes car work able drive . Many analytic techniques can thought way. don’t need understand underlying mechanisms, just need understand appropriately wield . gas pedal, brake. analyst strategist mechanic business systems club. everything always worked wouldn’t need . ’ll cover commonly used tools techniques apply realistic specific problems.first chapter 1 cover important elements analytics intersection information technology. also look distinction disciplines Business Intelligence Analytics. ’ll also discuss technologies integrating disciplines scale discuss functions continue evolve future. chapter helps frame foundation rest book.ability leverage analytic techniques predicated access quality data. second chapter 2 familiarize data using going exercise creating . formats closely resemble data found --wild. includes customer data along common demographic data formats. ’ll also build database ticketing data primary secondary market used pricing exercise. ’ll also build activity data demonstrate common pitfalls ’ve seen building sales marketing plans. data constructed based imaginary professional baseball team. Baseball operates little differently sports high number games. However, lessons learned can applied almost sport. fact, lessons contained book can applied business problems variety industries. boring chapter. book isn’t coding, demonstrate exactly everything show . way pay debt old math teachers owe .chapter 3 cover explore data. B.. technologies Tableau make performing ad hoc analysis relatively easy, writing analysis code huge advantages. chapter covers many common graphs encounter shows build understand . also demonstrate summarize consolidate data. incredibly common task easy ignore . Using programming language manipulate data much better using excel can’t express well enough. R excel military-grade performance enhancing drugs. excel spreadsheet programs place, begin using paradigms found R avoid whenever possible. might even resent people use slope-headed troglodytes unworthy respect, time, pity.chapter 4 take look frame projects. basic project management knowledge useful ’ll cover little bit . also easy chapter avoid, thought important include . includes good examples frame project asking right questions. tries avoid pedantic MBA speak, definitely covers arena.chapter 5 demonstrate couple different methods building customer segmentation schemes. Consumer segmentation crucial ingredient integrated sales marketing, research strategy. myriad ways accomplish effective segmentation strategy. much art science. cover important concepts chapter working example demonstrate fact. One important skills analyst toolbox understanding deal missing data. ’ll cover first hand .chapter 6 cover pricing forecasting. Pricing complex exercise, cover basics computed. Increasingly, pricing exercises becoming commodity. means may working much directly, critical understand methods prices decided upon practice. chapter also goes little deeper another critical tool use, regression. Regression gold standard estimating numerical data understanding ’s complexity important. ’ll also cover forecasting chapter. Forecasting also art science. cover forecasts useful discuss think .Chapter 7 demonstrate couple methods Lead Scoring. Lead scoring also fundamental integrated sales marketing strategy. consider extension segmentation. Lead scoring can also slightly controversial. different way think going sales. chapter discuss commonly used techniques demonstrate build machine learning model. also demonstrate important concepts endemic sports marketing world.extension pricing, ’ll take look promotions chapter 8. Promotions critical component marketing, good bad ways looking . also make many assumptions working marketing. Economics referred “dismal science.” Economics dismal, Marketing just dismal tends lack science thought practice. also walk basic components marketing strategy chapter discuss problems attributing sales marketing functions.Methods conducting research covered chapter 9. Conducting research tedious, time consuming, often thankless task, fundamental business strategy. Research also enormous subject. ’ll examine useful techniques go beyond examining facile attitudinal questions. chapter introduce critical concepts haven’t conducted formal research. said, chapter isn’t nearly long enough. simply glances enormous subjects hypothesis testing sampling. can take one thing away chapter, want know sampling critical correctly always problem.Chapter 10 cover Operations. Operations broad topic discuss important concepts. Simulation queuing discussed context real problems faced stadium ballpark operations. Simulations bread--butter operations problems need understand work. often best way understand system. don’t operations background like mathy subjects, ’ll enjoy chapter.Furthermore, book isn’t trying comprehensive. might gathered preceding paragraphs, book heavily concerned sales marketing. good reason. Professional sports teams high fixed variable costs enormous amount operational leverage stuck payroll. main sources revenue (Ticket sales, F&B sales, Retail Sales, Merchandising, Media, Sponsorship). sources derivative fans attend games watching games media outlets. concerned fundamentals strategy, start getting approach selling marketing fans correct. Get structural parts correct. executives don’t think maximizing opportunities around sales marketing, won’t able focus higher-order projects might prove valuable long short term.addition cover, Social data, Marketing mix, Retail, Media, staffing, F&B pricing, corporate sponsorship, operational components experience gate entry parking impact revenue strategy. Furthermore, structural issues throughout value-chain aren’t considering. instance, league -sized influence technology stack even digital rights fans. 4 strategy must take factors consideration. CRM technology strategy also direct influence success. CRM perspective, touch tangentially. covered every one subjects even went detail ones cover, book many times longer. However, tools introduced applicable many different types problems. believe come away great foundation working variety positions throughout sports organization.Finally, book quickly become outdated. many ways, already outdated. field analytics progressed rapidly past decade. New technologies (upgrades hardware software) make performing certain tasks easier. Amazon web Services Google Cloud Platform offer sophisticated tools analytics expect much low-hanging analytics fruit sports industries harvested platforms coming years. Research uncovers new methods approaching problems higher-order skill-sets make certain tasks commodities. Additionally, consumer behavior makes performing tasks obsolete. factors completely control. hope text give deeper understanding sports industry ’ll quickly able eclipse find push discipline industry along.","code":""},{"path":"index.html","id":"book-liscense","chapter":"Abstract and forward","heading":"0.1 Book Liscense","text":"work licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Just give credit due. Nothing new, everything built came . aren’t trying advance theory , just show things.","code":""},{"path":"index.html","id":"code-liscense","chapter":"Abstract and forward","heading":"0.2 Code Liscense","text":"\n\n\n\nextent possible law,\n\nJustin Watkins\nwaived copyright related neighboring rights \nwork.\nwork published :\n\nUnited States.\nwhatever want .","code":""},{"path":"index.html","id":"contact-information","chapter":"Abstract and forward","heading":"0.3 Contact Information","text":"Justin Watkins: watkinsjudo@gmail.com","code":""},{"path":"chapter1.html","id":"chapter1","chapter":"1 Analytics and sports business strategy","heading":"1 Analytics and sports business strategy","text":"chapter give background ideologies around analytics sports business club level. also want separate disciplines strategy analytics little. Analytics serves strategy, strategy derived analytics. chapter outline approach analytics. often many ways approach problem problems may worth solving many reasons. Despite inherent difficulties sports, analytic approach problem solving always core business strategy.Analytics reconciling opportunities. Many problems face old yet solved. especially true Operations. convinced unlimited applications analytics within industry business. Additionally, analytics can take many forms improve decision making many functions already mentioned:Marketing: marketing mix, brand strategy, content, CRM, positioning, pricingBusiness Intelligence: descriptive prescriptive reportingFinance: capital expenditure, forecasting, corporate financeSales: lead scoring, sales strategy, customer journey mappingSponsorship: asset valuation, asset creationOperations: ingress egress, staffing, concessionsTechnology: supporting systemsAnalytics can even assist tasks outside core business functions human resources. instance, can optimize staffing compensation packages. Ad hoc analytic tasks typically involve nothing simple spreadsheet organizes data. Techniques application varied may include techniques :SegmentationSimulationStatisticsOptimizationCognitive Science behavioral economicsProgrammingDesigning experiments (DOE)lists endlessly long. Versatility hallmark strong analyst strategist. aren’t going find many books general strategy. Strategy amalgam knowledge focused specific problems. People good understand business fundamental level understand structure problems may solved intelligent ways. forging alloy increase profitability, making business efficient resilient.","code":""},{"path":"chapter1.html","id":"understanding-the-meaning-of-strategy","chapter":"1 Analytics and sports business strategy","heading":"1.1 Understanding the meaning of strategy","text":"Strategy complex term, can distilled plan. strategy goal goal can vary. Like companies (especially public companies) driving firm valuations may goal. typically drive valuation higher increasing revenue reinforcing strategic moat. example, streaming wars Netflix, Disney, others race produce content. Content strategic moat technology problems solved consumer behavior shifted. play highly competitive space dog-fight every competitor.often heard people describe strategy turning everyone’s head right direction. suppose component strategy. Strategy can offensive defensive involve public relations lobbying efforts. can include manipulating market forces supply demand structuring media rights deals. can divest -performing assets create entirely new asset classes. can hedge risks diversifying approach partnerships. might mean evaluating capabilities. ’s broad term gets overused, often confused something else, simply abstraction isn’t framed around specific goals.Strategy also communication. can’t stressed enough. want learn strategy suggest take negotiations course. can also read couple books teach talk people. “win friends influence people” (Carnegie 1981) good place start. classic written nineteen-thirties relevant today . Another great book consider “Getting Yes, Negotiating Agreement Without Giving ” (Roger Fisher 2011). book positioning arguments. critical skill set. can’t , might well understand addition works. Working understanding people, presentation skills, understanding frame arguments without condescending critical strategy. can’t stress importance soft skills enough. important paragraph book. Stop strong technical skill sets take etiquette course learn act rhetoric course learn think. liberal-arts -appreciated field detriment many practitioners.academic standpoint, lot’s strategic frameworks. Harvard’s Michael Porter descried firms “collection activities performed design, produce, market, deliver, support products.” (Keller 2003) exactly , product represents something complex simple good service. famous contribution business strategy likely “Porter’s five forces.” 5 interesting framework surface doesn’t appear imminently useful sports team. come useful thinking strategic problems Point--Sale system use throughout venue?. case, model can used consider might select based market forces. consolidation, mean, powerful position terms negotiating? product commodity, vendor compete price. Perhaps interested marketing candidates sponsorship. Maybe purchase company. ’s classic strategy problem: “Build, Buy, Lease.” can get creative.ever messed around stock market ’ll lot talk fundamentals. type analysis might find trading platform illusion analysis. Applying techniques like triple exponential smoothing6 moving averages stocks looks great, probably doesn’t matter. make novice better trader? might make worse. Analysis great understand . contextual. don’t understand context can misleading damaging. Data solution perhaps . depends. Keep mind think critically problems going cover. mechanical part analysis isn’t important part . commodity. Focus application communication.","code":""},{"path":"chapter1.html","id":"technologies-place-in-strategy-and-analytics","chapter":"1 Analytics and sports business strategy","heading":"1.2 Technologies place in strategy and analytics","text":"consider application technology analytics along five ordered dimensions. can jump around degree, step generally built step . ascend steps, execution tends become difficult complex. Execution strategy fall apart. Read last sentence . Additionally, strategy flow top . means keep destination mind move steps.ridiculous may sound, begin data structure. building engine correct parts. Data invariably messy. much effort goes cleaning, structuring, storing data use represents bulk time spent across spectrum analytics. must begin . Additionally, probably multiple levels maturity front. instance, ticketing data may well structured CRM data may lacking. view analytics within structure proposed figure 1.1.\nFigure 1.1: Data strategy heirarchy needs\nFurthermore, consider technology “means end,” “end unto .” tends common confusion. technology strategy flow well-articulated business strategy. spend capital allow fans enter park leveraging facial recognition, problem solve? scanning ticket bottleneck system slows process? fans want ? regulatory environment permit ? sounds like complex system camera database. Focus system. technology probably easy part solve. innovation sometimes bred throwing something wall see sticks, haven’t found primary vehicle technology adoption. organization takes misguided technology-first approach, found results painfully unremarkable. Determine want accomplish want accomplish deploying technology solution.Perhaps want revolutionize business. problems solved revolutionary ways. internet feels much twenty years ago. now talking web 3.0 metaverse. already metaverse. called internet. isn’t revolutionary. really represents platform big tech wants use transact. Maybe better way things maybe simply worse way everything. want early mover, amazing advantages, ’ll taking risk. jury next decade metaverse. Perhaps 3D television. Remember ? course don’t. Look business model underlies marketing schemes. Perhaps metaverse simply way help bolster augmented reality products. understand mechanism, ’ll able plan comes next intelligently take risks.don’t want disparaging toward Information Technology. Strong .T. skill sets incredibly useful. skills progress ’ll exposed myriad technologies:work Windows Apple, ’ll still need understanding Linux7Shell scripting critical skill dev-ops task8Start using git repository code9Tools docker allow package programs web development10You don’t need server. Google, Amazon, Microsoft massive deployment platforms.Don’t shy away spending time get familiar technology components. ’ll rewarded highly regarded .main difference internet today twenty years ago people mostly use phone access browse . Incremental improvements time tends way changes take place. Core business procedures can always improved. people rich didn’t get brilliant product idea. just things rich people , executed properly, little bit luck. Think analytics business strategy way. ’ll walk component figure 1.1 discuss detail.","code":""},{"path":"chapter1.html","id":"data","chapter":"1 Analytics and sports business strategy","heading":"1.2.1 Data","text":"Getting foundational elements data place critical components hierarchy. won’t discus Master Data Management specific technologies Customer Data Platforms, talk foundations data mean use word. organizations solved portions data management, ’s never-ending problem. always newer better systems always need incorporate new data sales marketing infrastructure. process accelerating Google, Amazon, others vie cloud. Amazon Web Services Google Cloud Platform disruptive sense can deliver capabilities financially unfeasible many firms trying build house. -prem DBMS slowly go extinct11. also old battle. twenty five years ago discussion may Thin-client vs. Fat-Client computing. isn’t much different.Let’s go ahead make assumption sports team doesn’t data perfect spot. Getting data useful position may take number varied techniques considerations:systems important incorporate?data “Big,” meaning engineering challenge hold data velocity necessitate special approach?housed?manages process (internal, partner, etc)?much cost?necessary skill sets accomplish goals?understands manages data structure?Another important consideration plan data. can used? dictate approach making available ETL (Extract, Transform, Load) procedures work. instance, need data available times need current? current mean? Latency may important. Indeed, isn’t situations sports. twenty four hours latency probably good enough. Many data points may considered certain intervals. gives indication prioritize tasks. Let’s illustrate problem.","code":""},{"path":"chapter1.html","id":"understanding-data-structures","chapter":"1 Analytics and sports business strategy","heading":"1.2.1.1 Understanding data structures","text":"’ll encounter numerous data structures working sports, none important ticketing system. Regardless vendor (TicketMaster probably common), ’ll encounter form following ERD 1.2.\nFigure 1.2: Ticketing system data structure\nlooking ? many different database systems 12 available, venerable relational database one encounter . AWS Google improved optimized models, basic concept going look familiar across platforms. ’ll ignore even high-level discourse technical parts database construction cardinality normal forms discuss stuff examples.Think database collection excel workbooks formally linked ID column. ’ll call workbooks tables. tables allow get information transactions, historical purchase data, customer service rep data, ticketing information, . can read “crow’s-foot” “Many”, cross “one.” example, one customer can many plans. relationships can much complicated, basic level data encounter look something like . database relational (likely), basic SQL statements can used retrieve information:output query might look something like :reason afraid type SQL isn’t caps. don’t know , isn’t case sensitive doesn’t matter. data complex, example demonstrate basic table structure encounter. can see, appears Ted multiple accounts. Duplication bane database engineer. always confounds analysis one way another.Let’s take quick aside discuss Structured Query Language, SQL. SQL lingua-Franca database world. Although many technologies use “” SQL, ’ll get mileage SQL prerequisite want work data almost capacity. good news relatively simple learn basic level functional fluency can achieved relatively easily. also plenty free resources available learn practice . W3 schools excellent one. 13The integration CRM ticketing systems incredibly important. , subject give multiple things think discuss:ETL constructed? API, direct database connection, etc.parity checks considered?heavy .T. tasks. Although end-product important component analyst, understanding stuff works important. make better. Get good SQL, don’t choice want good analysis business setting.\nFigure 1.3: CRM system data structure\ndata figure 1.3 look little different. Different CRM systems may also different ways querying data. Salesforce uses SOQL looks similar SQL, forces traverse relationships little differently. Let’s take look data.data might begin look confusing relationships little complex.? appears Mr. Williams created (created) one account accounts merged one. simple example, demonstrates duplication problem constantly faced. going guess one Ted Williams, system regarding two people. Business rules can mitigate issues. instance, ticketing system forbid users using email already system. using email_address primary key advantages, always downstream issues consider. Nothing perfect. Someone may simply use different email. one person, may impossible tell apart. rules may make problem worse.Garbage-, garbage-. Without getting data clean, won’t able effectively move next level hierarchy. take look B.., want cover important simple query ever need. core B.. work. asked similar question interviews years almost everyone fails answer correctly despite simplicity. gain proficiency SQL, run code engine. written SQL server syntax answers question:Can create list Companies Industry ordered revenue?looks simple, make sure understand happening. Everyone fails test fail think invariably try aGROUP giving confused look faces wishing access Google.’ll constantly need exercise working large data sets leveraging SQL preferable leveraging methods since optimized sorts data gymnastics.","code":"------------------------------------------------------------------\n-- SQL example\n------------------------------------------------------------------\nSELECT  A.customer_id\n       ,A.email_addr\n       ,B.plan_id\n       ,B.price\nFROM Customer A LEFT JOIN Plans B ON A.customer_id = B.customer_id\nWHERE A.email_addr = \"Ted.Williams@someserver.com\"------------------------------------------------------------------\n-- SQL example example\n------------------------------------------------------------------\nSELECT  A.customer_id\n       ,A.ticketing_system_id\n       ,B.deal_id\n       ,C.opportunity_id\nFROM Customer A LEFT JOIN deal B ON A.customer_id = B.customer_id\n                LEFT JOIN opportunity C ON B.deal_id = C.deal_id\nWHERE A.email_addr = \"Ted.Williams@someserver.com\"------------------------------------------------------------------\n-- BI SQL example \n------------------------------------------------------------------\n\nCREATE TABLE #company (\n    company VARCHAR(20),\n    industry VARCHAR(20)\n)\nCREATE TABLE #revenue (\n    company VARCHAR(20),\n    revenue NUMERIC(12,2)\n)\n\nINSERT INTO #company (company, industry) VALUES\n('Coca-Cola','Beverages'),\n('Home Depot','Retail'),\n('Lockheed Martin','Aerospace'),\n('Boeing','Aerospace'),\n('BOA','Banking'),\n('Wal-Mart','Retail'),\n('Amazon','Retail'),\n('Total Wine','Retail')\n\nINSERT INTO #revenue (company, revenue) VALUES\n('Coca-Cola','2000000'),\n('Home Depot','1500000'),\n('Lockheed Martin','3000000'),\n('Boeing','5000000'),\n('BOA','900000'),\n('Wal-Mart','8500000'),\n('Amazon','1425000'),\n('Total Wine','75000')\n\nSELECT \n  RANK() OVER(PARTITION BY A.industry ORDER BY B.revenue DESC) [rank],\n  A.industry,\n  A.company,\n  B.revenue\nFROM #company A LEFT JOIN #revenue B ON A.company = B.company\nORDER BY industry, [rank]"},{"path":"chapter1.html","id":"business-intelligence","chapter":"1 Analytics and sports business strategy","heading":"1.2.2 Business Intelligence","text":"Business Intelligence loaded phrase can mean many different things. Enabling B.. capability possible ’ve established good data structure. portion chapter discuss B.. high level discuss differences data structure enables sophisticated reporting.usually place two categories:ReportingResearchA component Customer Relationship Management (CRM) also falls Business Intelligence umbrella practice clubs. many reasons , biggest simply legacy. Another reason CRM system typically houses much data may used reporting. ’s natural match company relatively small people need wear multiple hats. Reporting systems Tableau, Qlik, Looker, Business Objects, etc. depend well structured data. cautions tools well. can easily abuse get results looking . Philosophy use important. data good spot ready tell people . Gathering insight data can take many forms, often placed one four categories seen figure 1.4.\nFigure 1.4: Four categories reporting\nfirst stop reactive reporting Description Diagnosis. data structured appropriately, ’ll able produce backwards looking reports. reports likely bread--butter Business Intelligence department. answering questions much rep sold many tickets sold specific time frame.Prediction Perscription forward looking. might integrate predictive models reporting indicate whether sales goals likely met. ’ll talk might accomplished next section. Prescriptive reports might tell problem diagnosed. context, prescriptive report might enable manager reroute marketing dollars efficient channels. instance, report identify diminishing returns marketing spend particular social channel suggest one demonstrably greater efficiency.","code":""},{"path":"chapter1.html","id":"business-intelligence-data-structure","chapter":"1 Analytics and sports business strategy","heading":"1.2.2.1 Business Intelligence data structure","text":"Data structure BI system doesn’t necessarily different may find typical relational database. can plug system Tableau 14 database likely get good capability. However, practice data usually restructured facts dimensions. Additionally, data structures can also take complex forms data cubes 15, JSON-like hierarchical data, exotic forms can handle array-like data within specific database fields. ’ll focus simple fact dimension.can think fact something aggregated. number. Dimensions features use understand numbers. Consider diagram figure 1.5.\nFigure 1.5: Business Intelligence data structure\ntables don’t look much different saw figure 1.3. main difference conceptual. earlier diagram tables necessarily prioritized others. might look tables consider customer central table features radiating specific customer. customer rep, purchases tickets, may plan, etc. diagram Ticket central. Since B.. tools heart aggregation machines, structure extremely important. want perform math feature ticket price, put fact table. allows perfromantly answer kinds questions :much customer spend tickets 2018?much spend tickets 2021?many customers rep much spend 2019?rule, tend like structure data way want within database. isn’t always feasible efficient, advantages. can efficient flattening relationships fewer tables can make software run quickly. removed need traverse relationship. also means less dependent learning capabilities B.. platform. makes things simpler perspective. tends work well person B.. work also data engineer. problem isn’t scaleable can get confusing. might even lead build custom systems aren’t constrained ways.also pitfalls aware . can sometimes work around issues. However, can also run problems Cartesian joins 16. means can double-count value aren’t careful. common constructed snowflake schema facts dimensions tables.","code":""},{"path":"chapter1.html","id":"analytics","chapter":"1 Analytics and sports business strategy","heading":"1.2.3 Analytics","text":"term analytics least broad business intelligence. context, distinguish business intelligence less concerned displaying information concerned interpretation. Additionally, usually easier get base B.. functionality running begin applying analytic techniques data. Ultimately, business intelligence analytics going work together form backbone (antiquated valid term) decision support systems.Analytics refers applying operation data gaining additional insight modification. Obviously, getting data structured appropriately critical. typically put analytics tools one two categories:RegressionMachine learningLet’s take minute explain terms. definitely overlap. muddy term. may may familiar regression. Regression can get technical performing regression analysis tends dogmatic rigorous. ’ll make heavy use need understand works. recommend getting reference book subject. many. favorite “R Companion Applied Regression.” (John Fox 2019) Let’s try quick explanation ordinary least squares regression demonstrate power. going build explanation R, resist showing actual code next chapter.going explain regression simplest way possible. mathematician don’t one leverage regression. Also, aren’t working clinical trials drug. working fuzzy business problems. Exacting rigor simply isn’t necessary. following section take simple explanation basic form regression explain way like think .","code":""},{"path":"chapter1.html","id":"regression","chapter":"1 Analytics and sports business strategy","heading":"1.2.3.1 Regression","text":"Examine following meme created online meme generator:\nFigure 1.6: future thoughts regression\nincredibly reductive meme. Obviously can’t use linear regression generative art computer vision. However, going use lot lots advantages. Make sure understand . Eventually, agree meme working problems face. following section explains like think .familiar linear equation takes form:\\[\\begin{equation}\n\\ {y} = {m}{x} + {b}\n\\end{equation}\\]equation Y explained x m equal slope line b equal y intercept. apply list x values (-5,-4,-3,-2,-1,0,1,2,3,4,5) linear equation slope 2 y-intercept 5 get graph figure 1.7.\\[\\begin{equation}\n\\ {y} = {2}{x} + {5}\n\\end{equation}\\]\nFigure 1.7: Output linear equation\npractice, data points likely fit perfect linear equation. Regression looking line points minimizes sum squared errors (see figure 1.8. sum squared errors represents ( SSE ) distance point line. Look word orthogonal see can vary. square errors negative numbers don’t impact results.\nFigure 1.8: Output linear equation\nmultiple linear regression simply switching linear equation around adding terms:\\[\\begin{equation}\n\\ {y} = {b} + {m_1}{x_1} + {m_2}{x_2}\n\\end{equation}\\]actual form typically denoted similarly following equation:\\[\\begin{equation}\n\\ \\hat{y} = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\end{equation}\\]definition might read like “idea express class linear combination attributes predetermined weights.” (Ian H. Witten 2011) just think finding best average line data x explains y. standard error represents normal distribution split line. , don’t want push reading much , highly recommend picking book statistics Googling multiple-linear-regression aren’t fairly familiar . ’ll use lot analyst. Additionally, familiarize different forms regression. Orthoganal, Poissan, Ridge, etc. lots problems can solved apply correct technique.deep want go . going use tool lot subsequent chapters go detail. ’d like take simple approach explaining Machine Learning.","code":""},{"path":"chapter1.html","id":"machine-learning","chapter":"1 Analytics and sports business strategy","heading":"1.2.3.2 Machine Learning","text":"Machine learning little different concept, hood just statistics. explained many ways. ’s heart, looking patterns data make predictions. Also, don’t worry .. taking job. Worry person knows use taking job.three main types machine learning:Unsupervised LearningSupervised LearningReinforcement LearningEach variations useful solving different types problems. ’ll cover detail subsequent chapters. Additionally, number techniques fall machine learning umbrella including:Decision TreesRandom forestsGradient boostingSupport vector machinesNeural networksYou can also ensemble techniques. typically explain machine learning basic explanation completed decision tree. -depth explanation decision trees get’s mathy. basic explanation comes book “Data Mining” (Ian H. Witten 2011). book utilizes program called WEKA, concepts implementations R Python.going begin looking data. digging code, now time install R R studio. talk little bit next chapter. can install FOSBASS library accompanies book using following command:Type name package editor multiple functions data sets appear type two semicolons. Type question mark front data sets functions see documentation behind .decision tree simply organized set cascading questions answers simple understand high level. Let’s consider simple data set:Table 1.1: Renewal data setWe’ll see data several times book. Basically, just number factors column states whether fan renewed season tickets . Let’s apply decision tree data. ’ll look one factor, distance. example used two libraries: rpart (Therneau Atkinson 2022) rpart.plot (Milborrow 2022).\nFigure 1.9: Decision tree example\ntree (figure 1.9) simple read. top node, 2,564 people renew (nr) 11,300 renew. first split separates people live 49 miles park. people, 906 renew 663 renew. third split separates people live 67 miles park. live 67 miles park, 515 renew.decision tree performs splits one multiple ways lots resources can illustrate methods. stage, main thing understand nodes similar nodes. node homogeneous.machine learning simple form. set cascading questions split formulaic ways classifies response variable. methods differ, heart many machine learning processes similar.","code":"\n#-----------------------------------------------------------------\n# Install FOSBAAS Library\n#-----------------------------------------------------------------\nlibrary(devtools)\ndevtools::install_github(\"Justin-Watkins/FOSBAAS\"\n                         ,ref=\"master\"\n                         ,auth_token = NULL\n)\n#-----------------------------------------------------------------\n# View data set documentation\n#-----------------------------------------------------------------\n?FOSBAAS::customer_renewals\n#-----------------------------------------------------------------\n# Customer renewal data\n#-----------------------------------------------------------------\nrenewal_data <- FOSBAAS::customer_renewals\n#-----------------------------------------------------------------\n# Customer renewal data\n#-----------------------------------------------------------------\nd_tree <- \nrpart::rpart(formula = renewed ~ distance, \n             method  = \"class\",\n             data    = renewal_data)\nrpart.plot::rpart.plot(d_tree,\n                       type  = 4,\n                       extra = 101)"},{"path":"chapter1.html","id":"automation-and-integration","chapter":"1 Analytics and sports business strategy","heading":"1.2.4 Automation and Integration","text":"developed basic B.. analytics capabilities ’ll quickly want find ways put work. Ad hoc analytics ’s place, truly reap rewards work going need build engine allows automate story telling. analytics intersects .T. work. operationalizing analytics procedures requires little different knowledge set. also different approaches.AWS Google taken huge strides building frameworks natively integrate analytics DBMS. Gone days writing SQL wrappers R python script sitting server basement. Let’s take simple approach explaining mean .two functions (Automation Integration) mostly self-explanatory. Automation refers removing human interaction. Integration refers operationalizing outputs extending data commerce engines.Automating procedures provides several benefits:labor multiplierIt enables strategic thought go staffing decisionsIt keeps reports --dateAutomation relies number interlocked technologies related data engineering mostly belongs information technology group.Integration refers two elements:Integrating solutions across organizationIntegrating third parties extend capabilitiesInterestingly, integrating third parties easy part. Integrating solutions internally much difficult. typically requires change management sponsor upper levels management.Figure 1.10 demonstrates simplified version entire process data sources way feedback loop creating marketing channel partners. refer marketing channel partners, referring Google’s add network, Facebook, many others.\nFigure 1.10: Operationalizing analytical procedures\nfeatures used fairly distinct, new technologies making much easier link activities together one system. someone leading efforts, job think applying measures instead accomplish . Additionally, focused tech . Content creation, collateral, timing, verbiage, budgets, many ruminations impact automation integration. instance, distribute channel ’ll need artwork created, clear message call action, website may need updated, communication may considered. Nothing ever easy.","code":""},{"path":"chapter1.html","id":"key-performance-indicators","chapter":"1 Analytics and sports business strategy","heading":"1.3 Key Performance Indicators","text":"section explain application KPI. piece feedback critical understanding well performing (typically arbitrary historic benchmark). ’ll also refer back earlier paragraph talked stock market. Analytics contextual. KPI? KPI figure links business performance desired outcome. Baseball number walks player takes KPI likely help predict -base-percentage. Analytics groups might able establish causal link -base-percentage wins17. market player, might weight walks heavily metrics. -base-percentage might KPI walks might key increasing -base-percentage leading wins. following paragraphs discuss criticize commonly used KPI clubs: Percap, average price paid per ticket.Analytics work must tempered industry knowledge; must put correct context. ’ll say million times . easy make incorrect judgments armed blunt instrument data. Per-Cap (average ticket price) likely worst. Per-Cap commonly used KPI used comparing effectiveness sales pricing strategy:\\[\\begin{equation} TotalTicketRevenue / NumberOfSeatsSold \\end{equation}\\]surface, metric seems interesting. However, number issues :denominator changes every game sold . case, comparing fractions different denominators. mix tickets vary wildly game--game. alters interpretation leads issue number two.number becomes diluted tends decrease tickets sold. tickets sold, less-expensive tickets sold tends drive per-cap. high per-cap good? answer depends. Let’s illustrate mean:\\[\\begin{equation} \\$1,400,000 Ticket Revenue / 34,000 Tickets sold = \\$41.80 \\end{equation}\\]\n\\[\\begin{equation} \\$1,600,000 Ticket Revenue / 40,000 Tickets sold = \\$40.00 \\end{equation}\\]reconcile $1.80 difference per-cap? higher number indicate priced efficiently lower revenue scenario? answer depends many factors. Taken alone, number doesn’t much meaning. better metric Yield. Yield just simple:\\[\\begin{equation} TotalTicketRevenue / AvailableTickets \\end{equation}\\]Yield intuitive increases every sale since isn’t penalized unsold inventory. previous example look like yield perspective?\\[\\begin{equation} \\$1,400,000 Ticket Revenue / 40,000 Available Tickets = \\$35.00 \\end{equation}\\]\n\\[\\begin{equation} \\$1,600,000 Ticket Revenue / 40,000 Available Tickets = \\$40.00 \\end{equation}\\]Yield higher revenue higher now hold denominator constant. Hopefully, indicates efficient selling seats perhaps demand higher prices buoyed revenue. number much easier interpret leverage hypothesis testing. easiest way consider metric visualize . simple scatter plot (figure 1.11) trick. real life scenario average ticket price isn’t highly correlated overall revenue games begin approach sellout percap approximates yield.\nFigure 1.11: Scatterplot revenue percap\nAnother poor metric often used Sales date. metric asks many sales particular date previous year. metric may biggest liar commonly used KPI. can fraught distortion baseball schedule. likely validity sport fewer games higher FSE base. FSE stands Full Season Equivalent represents number tickets sold individuals season (modified season) basis.Sales date problematic different reasons:comparing different team doesn’t consider admixture tickets. team 20,000 FSEs look dramatically different team 6,000 FSEs.schedule begun, admixture games significant influence outcomes. mean?’ll see chapter 6 elements biggest influence ticket sales. Sales--date doesn’t really consider game dates, opponents, -field-success, seasonality, game times, etc. Additionally, people choose purchase tickets different times different reasons. differences, sales specific ticket class may look great first twenty games, terrible first forty games. Take look line graph chapter 3 3.6. built cumulative line lines results look different. Don’t benchmark sales--date. Instead, leverage forecasts consider underlying elements schedule. Let’s contemplate simple example (figure: 1.12). illustration created publicly available data ’ll demonstrate create little later.\nFigure 1.12: Relationship avg salary ticket sales\ncertainly appears relationship average salary player total number tickets sold. relationship might even stronger teams larger markets. case? top-approach forecasting demonstrates amount players paid (players paid lot money) ticket sales tend higher. aren’t seeing tight clusters based market size. Higher pay may demonstrate better performance. Better performance might translate wins. wins attracts earned media fans. Ticket sales increase.obtuse example, demonstrates point. underlying mechanisms likely reasonable job explaining ticket sales. didn’t even cover bottom-forecasting elements marketing efforts!point section now clear. Performance dictated number factors. within control, . Running team similar running hedge fund. Sometimes ’s , sometimes ’s . However, need understand ’s can make smarter decisions. Bad KPIs don’t help make smarter decisions. Leveraging appropriate KPIs many salutary impacts good analytics always looks underlying mechanism work.","code":"\n#-----------------------------------------------------------------\n# percap scatter plot\n#-----------------------------------------------------------------\nset.seed(714)\npercap_data <- tibble::tibble(\n  percap = rnorm(81,40,10),\n  revenue = rnorm(81,1000000,200000)\n)\nx_label  <- ('\\n Percap')\ny_label  <- ('Revenue \\n')\ntitle    <- ('Revenue vs. Percap')\nscatter_percap <- \n  ggplot2::ggplot(data  = percap_data, \n                  aes(x = percap, \n                      y = revenue))                   +\n  geom_point(alpha = .9, color = 'dodgerblue')        +\n  geom_rug(color = 'coral')                           +\n  scale_y_continuous(label = scales::dollar)          +\n  scale_x_continuous(label = scales::dollar)          +\n  xlab(x_label)                                       + \n  ylab(y_label)                                       + \n  ggtitle(title)                                      +\n  graphics_theme_1"},{"path":"chapter1.html","id":"why-do-people-buy-tickets-to-sporting-events","chapter":"1 Analytics and sports business strategy","heading":"1.4 Why do people buy tickets to sporting events?","text":"Sports fandom irrational. irrationality can make sports challenging infuriating work analytics standpoint. can determine marketing efforts making difference? simply slaves whims fancy mob? answer complex explore little chapter forecasting pricing, chapter 6. Additionally, answer may (degree) exist genes.“ability (special conditions) trancend self-interest lose (temporarily ecstatically) something larger .”— Johnathan Haidt, “Riteous Mind”interesting quote “Righteous Mind” (Haidt 2012). Humans programmed participate groups. Maybe people buy tickets lonely, bored. mechanisms “need belong” driving sports fandom, perhaps can manipulated. Perhaps manipulated. Ultimately, many reasons someone might buy tickets sporting event:buy businessThey just want something doThey brand loyalty driven upbringingThey motivated associate winning groupThey genuinely like appreciate sportThey feel good come gameThey want something familyThere also brand components associated history logos. one interesting. ’ve recently seen teams abandon logos nicknames response societal corporate pressure. 18 mean analytics context? much brand worth? Obviously, brand equity must pondered. least partially drives corporate sponsorship. Borrowed equity associating brand another brand positive image fundamental major revenue driver clubs. Brand value might calculated using one several techniques Royalty Relief Method. 19 main idea brands tremendous value.number answers question someone might purchase tickets baseball, football, basketball, hockey, soccer game. People United States across world complex emotional relationships sports brands. emotions may better job explaining behavior model predicting likelihood purchase.side sports marketing often overlooked -served. Behavioral economics huge field. least one technique use later book roots behavioral economics lot literature subjects. “Power Moments” (Chip Heath 2017) outstanding book imminently useful one consider specific mechanisms engender loyalty. Major themes pride connection almost instantly accessible marketers club level. can analyst help drive desired outcomes? answer may depend solutions may wheelhouse data scientist business analyst.","code":""},{"path":"chapter1.html","id":"key-concepts-and-chapter-summary","chapter":"1 Analytics and sports business strategy","heading":"1.5 Key concepts and chapter summary","text":"chapter explains rationale approach analytics strategy organizational level. Strategy broad term include many elements business development hedging strategies. focused analytics strategy covered main points related analytics hierarchy:Technologies integral relationship analyticsBasic SQL data structures importance data integrityBusiness IntelligenceAnalyticsAutomation IntegrationKey performance indicatorsBehavioral economicsUltimately, chapter serves give high-level overview potential musings analytics business strategy:Technology isn’t analytics. tool serves business functions. Don’t consider technology vacuum.now understand basics data structure databases. start . Without good data, project going nowhere. ’ll want acquire knowledge SQL going limited capability.Business Intelligence tends focus reporting research. ’ll likely begin descriptive reports organization matures become forward looking incorporating analytics functions.Analytics mostly focuses trying guess future outcomes. variety regression machine learning tools disposal. tools commoditized past couple decades. world much easier navigate even ten years ago.Automating tasks force-multiplier. currently arms race going big-tech companies continue bolster analytics database systems. Google Cloud Platform Amazon Web Services two key examples. hundreds tools consuming, manipulating, deploying data.difficult part determining KPIs probably agreeing . KPI easy. instance, NOI revenue-per-square-foot might great KPIs dealing real-estate. KPIs contextual.People things different reasons. Modeling consumer behavior difficult, plays major role sports. touched Behavioral Economics, rounding analytics toolbox reading field highly recommended.","code":""},{"path":"chapter2.html","id":"chapter2","chapter":"2 Constructing our data sets","heading":"2 Constructing our data sets","text":"Feel free skip chapter. considered including . Although, important chapter. important informative, gives us raw material divine insights. Data fuel analytics work, business strategy derived objective justifications arrived systematic evaluation. don’t much coding experience, gives opportunity familiarize code data might encounter. also lets give introduction R language use throughout book. said, book . Understanding important despite said . ’ll cover basics exploring analyzing data set next chapter. now, ’ll just cover data means, interpret sports data, demonstrate code created .Unfortunalty, going need discuss coding. People afraid . shouldn’t . learn . can’t read book coding understand , just dive . Every piece code book available, run machine, can hacked adapted similar problems. using R R Studio 20 primary analysis tool. R great choice analyzing sports data. find R bit idiosyncratic, don’t dream C++ use . like script little bit, don’t want full-developer, R ticket. Additionally, R Studio great IDE analysis. ton useful features make life much easier. highly recommend .chapter ’ll cover subjects:Understanding common data sets context professional sportsConstructing sample data sets mimic data setsBuilding R package reference data sets throughout rest bookThe particulars ticketing datasets vary small number companies dominate sports industry. TicketMaster common pro sports. Paciolin another player arena, common college sports venues. Ultimately, system won’t matter. ’ll transform data format useful analytics process.purposes, ’ll invent professional baseball team, Nashville Game Hens. Game Hens expansion team 2017 play typical baseball schedule 162 games.","code":""},{"path":"chapter2.html","id":"some-basic-notes-on-the-r-language","chapter":"2 Constructing our data sets","heading":"2.1 Some basic notes on the R language","text":"chapter introduce R code. Ultimately, doesn’t matter tool use. Python, Julia, R, Stata, SAS, Matlab provide similar functionality. However, confronted code can little overwhelming first. leveraging code, particularly R reasons.analysis done code step readily reproducible documentedR free, easy use, large user base, great IDER massively extensible smaller-scaled tasksIt great tool protyping something might put production perfomant enviornmentI found best get good one tool stick . However, say note caution. Let’s consider quote horrifying poem called “Second Coming.”“Things fall apart; center hold”Willaim Butler Yeats, “Second Coming”Technologies doomed obsolescence. many legacy technologies endured decades (Fortran, Cobal, R, Python, C, C++), evolved. Additionally, many technologies eclipsed better tools. Tools tend fairly specialized. instance, R isn’t best tool build large-scaled web applications. Don’t rigid devote personal brand particular technology. might find scrap-pile … “Things fall apart.”R relatively simplistic outside relative many languages. limited number data structures, doesn’t use scalars, single-threaded, tends avoid standard flow-control. little peculiar. programming experience another language might find little difficulty . new programming, going look weird. book isn’t programming. simply writing scripts piggyback programmers done fabulous job creating free tool currently rivals others analytics space 21.Additionally, recommend using code instead point--click tool. Code demonstrates exactly done easy communicate anyone knowledge language. make life easier even makes difficult beginning. best thing R huge user base years many resources can leverage learn can . Ultimately, decision tool use needs driven want get . prototyping need operationalize code? dealing huge datasets? want something built speed comfort? going cover anything language book. just many resources available better job . basic recommendations like use R:Get IDE like. RStudio currently best choice R wide margin.Download R (R Core Team 2022).22 works major platforms use favorite search engine find free classes use . hundreds -tos available online. can also purchase many books teach language works. favorite “Advanced R” (Wickham 2014) highly recommend must-reference book.Practice ! doesn’t take long get --hump terms getting functional fluency. consider similar playing instrument. “Flight bumblebee” 23 doesn’t need first song learn piano. Start simple build knowledge. eventually develop intuitive understanding tool can .also important consider R drawbacks.language won’t tell . provides methods execute functions. still need understand approach solve problem. R won’t help unlike statistical tools Minitab24.R can relatively slow don’t use correctly. constructed 25. However, ways speed significantly. Rcpp (Eddelbuettel et al. 2022) almost seamless api C++ allow build leverage C++ functions R environment. highly leveraged important tool R world.R may best choice putting something operation. Many people prototype R leverage another tool put work use. However, great tools smaller scale web deployments built Shiny framework.’s user base may decline favor tools. Python become popular recent years. Languages tend benefit network effects users , features built . User-base size important.following sections demonstrate datasets book created. think important couple different reasons.may familiar R. code sections get familiar looks.don’t work data sports, data going foreign . opportunity explain data sets.One comment want make coding -general. easier write code read somebody else’s code. end, like comments. many cases code can act documentation, like add explicit descriptors. don’t make detailed. just give enough know code block . ’ll follow practice throughout book.data sets code used create available R package FOSBAAS publicly available, reason type following sections, suit . can download file code book : https://github.com/Justin-Watkins/FOSBAAS/blob/master/FOSBAAS_code.R.Additionally, book make use many libraries. can run following code make sure installed:Keep mind certain functions deprecate time. Additionally, can end problems certain functions names functions packages. Although ways make sure using library version used, going ignore now. expect slowly fail. Furthermore, packages download long list dependencies. Installing libraries may take minute. Just warning.","code":"\n#-----------------------------------------------------------------\n# Install libraries\n#-----------------------------------------------------------------\nlibraries <- \nc('AlgDesign','anacor','car','caret','data.table','devtools','dplyr','forcats',\n  'ggplot2','GPArotation','Hmisc','knitr','lubridate','maps','mgcv','mice',\n  'mlr','mlr3','nFactors','nnet','plyr','poLCA','pricesensitivitymeter','pROC',\n  'pscl','psych','purrr','pwr','ranger','RColorBrewer','Rcpp','reshape2',\n  'reticulate','roxygen2','rpart','rpart.plot','rsample','scales','shiny',\n  'stargazer','stats','tibble','tidymodels','tidyr','tidyverse','viridis',\n  'xtable')\n\ninstall.packages(setdiff(libraries, \n                         rownames(installed.packages()))) "},{"path":"chapter2.html","id":"renewaldata","chapter":"2 Constructing our data sets","heading":"2.2 Simulating customer renewal data","text":"three main things code book:Write functionsApply functions dataGraph outputThe first data set create related building model estimate likelihood season ticket holder renew season tickets. data difficult create need build certain patterns data build . goal section build function produce data set command. Due fact builds , difficult generalize function. end, doesn’t demonstrate good programming practice, many elements need progress .build function, denote prefix f-underscore word separated underscore. Columns data sets follow camel case (camelCase) first letter word capitalized spaces words. don’t like , found works . Whatever , choose one way . ’ll glad .begin creating function FOSBAAS::f_create_lead_scoring_data. best practice name function way tells exactly . aren’t familiar building functions, easy understand. simple function outputs y terms x linear function m = slope b = y intercept:\\[\\begin{equation}\n\\ {y} = {m}{x} + {b}\n\\end{equation}\\]can think functions write build data sets way. input variable (multiple variables) function process variables output value. R function output y value x based linear equation look like .input values x, slope, y-intercept, function return value y. simplistic example, demonstrates functions well. Let’s try .Now simple function can use get y value x = 2, slope line 10, y intercept 7. Input, process, output. now understanding way function works, need way repetitively apply function data. can lot different ways. programming languages ’ll use loop. wrapped folowing snippet system.time() function demonstrate differences speed operation.loop thing, open-ended generally used much less frequently. python, loops typically discouraged. However, find easier read methods found .third approach, one preferred use apply function. See ?apply. functions might confusing first, useful typically work much quickly.apply functions even improved upon specific ways. following snippet uses imap function purrr (Henry Wickham 2020) package.basic tools use everything going going forward. also tradeoffs nned contemplate. Speed readability important considerations. Ultimately, can use whatever feel comfortable .following sections demonstrate functions used build data sets find book. won’t go full detail every data set. Full documentation can found help section FOSBAAS package. Additionally, something little strange first function, feed helper functions. One helper functions requires use function well. won’t data sets, important feature R understand. Everything R can based function, can use similar way methods languages.already built functions. first one simply creates data lead scoring. Lead scoring means going use data predict groups people likely purchase ticket.function also accepts several arguments:seed variable enables us create reproducible data sets.number rows want.Five functions create tenure, spend, ticket usageTwo functions help assign renewalA renewal argument produce column indicating account renewed.want see code function uses, can use following command (Assuming installed package):Calling function produce data set looks something like . use parameters, get exactly data.Table 2.1: customer renewal dataThis data set includes several columns:account id representing specific individualIs account used company individualThe season yearThe plan type (Partial season Full season)Ticket usage percentageThe number years account usThe amount spent tickets 2021Distance ballparkDid account renew end seasonThis process little involved (see figure 2.1). ’ll walk step step.\nFigure 2.1: Data creation process\n","code":"\n#-----------------------------------------------------------------\n# Linear equation function\n#-----------------------------------------------------------------\nf_linear_equation <- function(x,slope,yIntercept){\n  y <- slope*x + yIntercept\n  return(y)\n}\n#-----------------------------------------------------------------\n# Linear equation function inputs\n#-----------------------------------------------------------------\nf_linear_equation( x          = 2,\n                   slope      = 10,\n                   yIntercept = 7) \n#> [1] 27\n#-----------------------------------------------------------------\n# fake data\n#-----------------------------------------------------------------\nx        <- seq(from=1,to=1000000,by=1)    # x values\nm        <- 10                             # Slope\nb        <- 7                              # Y Intercept\n\n#-----------------------------------------------------------------\n# 1. For Loop\n#-----------------------------------------------------------------\nline_value <- list()\n\nsystem.time(\nfor(i in x){\n line_value[i] <- x[i]*m + b\n}\n)\n#>    user  system elapsed \n#>    0.99    0.04    1.03\n\nline_value[1:3]\n#> [[1]]\n#> [1] 17\n#> \n#> [[2]]\n#> [1] 27\n#> \n#> [[3]]\n#> [1] 37\n#-----------------------------------------------------------------\n# 2. While Loop\n#-----------------------------------------------------------------\ni <- 1                               # Iterator\nline_value <- list()\n\nsystem.time(\nwhile(i <= length(x)){\n line_value[i] <- x[i]*m + b\n i <- i + 1\n}\n)\n#>    user  system elapsed \n#>    0.92    0.03    1.03\n\nline_value[1:3]\n#> [[1]]\n#> [1] 17\n#> \n#> [[2]]\n#> [1] 27\n#> \n#> [[3]]\n#> [1] 37\n\n#-----------------------------------------------------------------\n# 3. lapply\n#-----------------------------------------------------------------\nsystem.time(\nline_value <- lapply(1:length(x), function(i) x[i]*m + b)\n)\n#>    user  system elapsed \n#>    0.86    0.05    0.91\n\nline_value[1:3]\n#> [[1]]\n#> [1] 17\n#> \n#> [[2]]\n#> [1] 27\n#> \n#> [[3]]\n#> [1] 37\n\n#-----------------------------------------------------------------\n# 4. purrr:imap\n#-----------------------------------------------------------------\nsystem.time(\nline_value <- purrr::imap(x,~ .x*m + b)\n)\n#>    user  system elapsed \n#>    1.01    0.00    1.02\n\nline_value[1:3]\n#> [[1]]\n#> [1] 17\n#> \n#> [[2]]\n#> [1] 27\n#> \n#> [[3]]\n#> [1] 37\n#-----------------------------------------------------------------\n# Create lead scoring data\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\nf_create_lead_scoring_data(714, \n                           5000,\n                           \"2021\",\n                           f_calculate_tenure,\n                           f_calculate_spend,\n                           f_calculate_ticket_use,\n                           f_renewal_assignment,\n                           f_assign_renewal,\n                           renew = T)\n#-----------------------------------------------------------------\n# View your functions\n#-----------------------------------------------------------------\nedit(getAnywhere('f_create_lead_scoring_data'), \n     file = 'f_create_lead_scoring_data.r')"},{"path":"chapter2.html","id":"building-our-function","chapter":"2 Constructing our data sets","heading":"2.2.1 Building our function","text":"following code creates dataframe nine columns assigns list names column. Think dataframe excel workbook. R uses <- assignment, however can use = sign prefer. use sapply function create sequence random letters numbers represent account ids. apply functions incredibly important. can type ?sapply console R studio want learn . said twice. important.looks weird. sth_data[,1] references first column data frame dataframe[row,column]. first argument sapply giving function list rows traverse. second argument uses something called anonymous function, confusing. Look want deeper understanding . begin make sense play . paste(sample(c(0:9, LETTERS), 12, replace=TRUE),collapse = \"\")) simply creates random twelve digit alphanumeric string. follow assigning season season column.Many season ticket accounts owned corporations. ’ll build list called corporate sample order assign “c” corporate “” individual account. use set.seed() function reproducibility. ’ll use sample function sample c list rate 20% corporations 80% individuals using prob argument.Season ticket holders can purchase full partial plans. proportions change based corporate individual account. statements generalized turned function. However, copy--paste , left alone. Always look opportunities generalize functions. function operates exactly way previous one.simulate distance stadium ’ll leverage rexp() function give us exponentially distributed list numbers can modify sample account. density pattern common many urban areas population density much higher certain centralized areas. ’ll show visualize pattern subsequent chapter. outcome individuals tend live away corporations.Next, ’ll build list numbers refer number season tickets purchased account. ’ll assign number tickets based distributions denoted prob argument sample() function. Basically, want corporations purchase tickets.tenure, set renew argument = TRUE, leverage f_tenure function assign number years based list arguments created within main function. Flow control conditions really important understand. simple way explain like : “(condition == TRUE), something. (condition == FALSE), something else”. R, == used compare things. = used assignment.function use mapply. works like sapply, accepts multiple arguments. also used statement. just means type less. tells R everything within belongs one frame data.f_calculate_tenure() function accepts four arguments. arguments constructed code chunks running. function simply long -else statement. like specific patterns like construct within tenure column.based season ticket holder spend tenure, plan-type, account type.function f_calculate_spend uses rnorm() function. function accepts mean standard deviation argument allows us sample number specific normal distribution. , generalized function little , since ’s just helper function one purpose hard-coded numbers .Ticket usage represents percentage tickets used:\\[\\begin{equation}\n\\ {ticketUsage} =  {totalTicketsUsed} / {totalTickets}\n\\end{equation}\\]Similarly previous examples, building ticket usage particular way.function f_calculate_ticket_use uses runif() function produces random number uniform distribution based minimum maximum value. numbers also hard-coded can produce specific patterns.Finally, check renew argument (using statement) true, determine account renewed tickets based values dataframe sth_data. renew_ = _F, return dataframe without renewed field.following statement uses two functions: f_assign_renewal() f_renewal_assignment(), little confusing. f_assign_renewal() helper function assigns renewal value based cluster assignment designated f_renewal_assignment().f_renewal_assignment() accepts f_assign_renewal() argument. couple things. First, clusters ticket_usage distance using kmeans() function. ’ll cover clustering exercises chapter 4. cluster assignments added together biased renewals assigned based high low number . Higher numbers likely renew. use loop instead mapply() function example. Vectorizing loop apply() function better method using R.function complex others dependency dplyr. also call list() used data structure R. list simply indexed array. Dataframes collections lists.just walked everything figure 2.1. process much succinct patterns constructing weren’t based specific features creating. However, able take look several programming features features endemic R.Building functionsThe apply family functionsIf statementsWhile/loopskmeans clusteringrnorm rexp functions distributionsrunif creating random numberssubsetting dplyrWe can now call function different input variables produce different result:Calling function produce data set looks something like :Table 2.2: customer renewal dataThis typical data set find wild. data also beautiful. beautiful mean isn’t missing features easy prep analysis. something typically find. ’ll discuss incomplete data problems book.can now use function produce many different data sets like. ’ll follow procedure building helper functions build data sets.","code":"#-----------------------------------------------------------------\n# 1. Create a data frame to hold our data\n#-----------------------------------------------------------------\n  sth_data <- data.frame(matrix(nrow=num_purchasers,ncol=9))\n  names(sth_data) <- c(\"accountID\",\"corporate\",\"season\", \n                       \"planType\",\"ticketUsage\",\"tenure\",\n# 2. Build ids and append to customer data frame\n  set.seed(seed)\n  sth_data[,1] <- sapply(seq(nrow(sth_data)), function(x)\n    paste(sample(c(0:9, LETTERS), 12, replace=TRUE),\n          collapse = \"\"))\n# 3. Assign a season year to the data \n  sth_data$season <- season\n#-----------------------------------------------------------------\n# 4. Assign corporate or individual to each account\n#-----------------------------------------------------------------\n  set.seed(seed)\n  corporate <- c(\"c\", \"i\")\n  sth_data$corporate <-  \n  sapply(seq(nrow(sth_data)), \n         function(x) sample(corporate, \n                            1, \n                            replace = TRUE, \n                            prob = c(.20, .80)))\n#-----------------------------------------------------------------\n# 5. Assign a plan type to each account\n#-----------------------------------------------------------------\n# Corporations\nset.seed(seed)\n  planType <- c(\"f\",\"p\")\n  sth_data[which(sth_data$corporate == \"c\"),]$planType <- \n    sapply(seq(nrow(sth_data[which(sth_data$corporate == \"c\"),])), \n           function(x) sample(planType, \n                              1, \n                              replace = TRUE, \n                              prob = c(.95, .5)))\n# Individuals\n  planType <- c(\"f\",\"p\")\n  sth_data[which(sth_data$corporate == \"i\"),]$planType <- \n    sapply(seq(nrow(sth_data[which(sth_data$corporate == \"i\"),])), \n           function(x) sample(planType, \n                              1, \n                              replace = TRUE, \n                              prob = c(.60, .40)))\n#-----------------------------------------------------------------\n# 6. Calculate the distance from the stadium\n#-----------------------------------------------------------------\n  set.seed(seed)\n  distances_corp <- rexp(num_purchasers) * 12\n  distances_indv <- rexp(num_purchasers) * 30\n# Corporate\n  set.seed(seed)\n  sth_data[which(sth_data$corporate == \"c\"),]$distance <- \n    sapply(seq(nrow(sth_data[which(sth_data$corporate == \"c\"),])), \n           function(x) sample(distances_corp, \n                              1, \n                              replace = TRUE))\n# Individuals\n  sth_data[which(sth_data$corporate == \"i\"),]$distance <- \n    sapply(seq(nrow(sth_data[which(sth_data$corporate == \"i\"),])), \n           function(x) sample(distances_indv, \n                              1, \n                              replace = TRUE))\n#-----------------------------------------------------------------\n# 7. Determine the number of tickets each account has purchased\n#-----------------------------------------------------------------\n  tickets <- c(10,8,6,5,4,3,2,1)\n  set.seed(seed)\n# Corporations\n  sth_data[which(sth_data$corporate == \"c\"),]$tickets <- \n    sapply(seq(nrow(sth_data[which(sth_data$corporate == \"c\"),])), \n           function(x) sample(tickets, 1, replace = TRUE, \n             prob = c(.02,.08,.10,.05,.50,.05,.20,0)))\n# Individuals\n  sth_data[which(sth_data$corporate == \"i\"),]$tickets <- \n    sapply(seq(nrow(sth_data[which(sth_data$corporate == \"i\"),])), \n           function(x) sample(tickets, 1, replace = TRUE, \n             prob = c(0,0,.10,.05,.40,.05,.30,.10))) \n#-----------------------------------------------------------------\n# 8a. Assign years the account holder has had tickets\n#-----------------------------------------------------------------\n  if(renew == T){\n  avgDist <- mean(sth_data$distance)\n  set.seed(seed)\n  tenures <- with(sth_data,mapply(f_calculate_tenure,\n                                  corporate,\n                                  planType,\n                                  distance,\n                                  avgDist))\n  sth_data$tenure <- as.vector(tenures)\n  }else{sth_data$tenure = 0}\n#----------------------------------------------------------------- \n# 9b. Function to calculate tenure\n#-----------------------------------------------------------------\nf_calculate_tenure<-function(corporate,planType,distance,avgDist){\nif(corporate == \"c\" & planType == \"f\" & distance <= avgDist){\n    ten <-round(abs(rnorm(1,mean = 14,sd = 6)),0)}\nelse if(corporate == \"i\" & planType == \"f\" & distance <= avgDist){\n    ten <-round(abs(rnorm(1,mean = 10,sd = 6)),0)}\nelse if(corporate == \"c\" & planType == \"p\" & distance <= avgDist){\n    ten <-round(abs(rnorm(1,mean = 3,sd = 2)),0)}\nelse if(corporate == \"i\" & planType == \"p\" & distance <= avgDist){\n    ten <-round(abs(rnorm(1,mean = 3,sd = 2)),0)}\nelse if(corporate == \"c\" & planType == \"f\" & distance >= avgDist){\n    ten <-round(abs(rnorm(1,mean = 9,sd = 3)),0)}\nelse if(corporate == \"i\" & planType == \"f\" & distance >= avgDist){\n    ten <-round(abs(rnorm(1,mean = 7,sd = 3)),0)}  \nelse if(corporate == \"c\" & planType == \"p\" & distance >= avgDist){\n    ten <-round(abs(rnorm(1,mean = 2,sd = 1)),0)}\nelse if(corporate == \"i\" & planType == \"p\" & distance >= avgDist){\n    ten <-round(abs(rnorm(1,mean = 2,sd = 1)),0)}\nelse{ten <-round(abs(rnorm(1,mean = 8,sd = 3)),0)}\n  return(ten) \n}\n#----------------------------------------------------------------- \n# 9a. SPEND\n#-----------------------------------------------------------------  \n  avgTenure <- mean(sth_data$tenure)\n  set.seed(seed)\n  spend <- with(sth_data,mapply(f_calculate_spend,\n                                corporate,\n                                planType,\n                                tenure,\n                                avgTenure))\n  sth_data$spend <- as.vector(spend) * sth_data$tickets\n#-----------------------------------------------------------------\n# 9b. Function to calculate spend\n#-----------------------------------------------------------------\nf_calculate_spend<- function(corporate,planType,tenure,avgTenure){\nif(corporate == \"c\" & planType == \"f\" & tenure >= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 7500,sd = 800)),0)}\nelse if(corporate == \"i\" & planType == \"f\" & tenure >= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 2100,sd = 500)),0)}\nelse if(corporate == \"c\" & planType == \"p\" & tenure >= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 2000,sd = 300)),0)}\nelse if(corporate == \"i\" & planType == \"p\" & tenure >= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 1200,sd = 200)),0)}\nelse if(corporate == \"c\" & planType == \"f\" & tenure <= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 5000,sd = 500)),0)}\nelse if(corporate == \"i\" & planType == \"f\" & tenure <= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 2000,sd = 300)),0)}  \nelse if(corporate == \"c\" & planType == \"p\" & tenure <= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 2000,sd = 400)),0)}\nelse if(corporate == \"i\" & planType == \"p\" & tenure <= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 800,sd = 75)),0)}\nelse{spend <-round(abs(rnorm(1,mean = 2500,sd = 300)),0)}\n  return(spend) \n}\n#-----------------------------------------------------------------\n# 10a. Calculate the percentage of tickets used\n#-----------------------------------------------------------------\n  avgDist <- mean(sth_data$distance)\n  set.seed(seed)\n  ticket_use <- with(sth_data,mapply(f_calculate_ticket_use,\n                                     corporate,\n                                     distance,\n                                     avgDist))\n  sth_data$ticketUsage <- as.vector(ticket_use)\n#-----------------------------------------------------------------\n# 10b. Function to return ticket usage\n#-----------------------------------------------------------------\nf_calculate_ticket_use <- function(corporate,distance,avgDist){\nif(corporate == \"c\" & distance <= avgDist){\n  tu <- runif(1,min = .89, max = 1)}\n    else if(corporate == \"i\" & distance <= avgDist){\n      tu <- runif(1,min = .82, max = .94)}\n        else if(corporate == \"c\" & distance >= avgDist){\n          tu <- runif(1,min = .65, max = .9)}\n            else if(corporate == \"i\" & distance >= avgDist){\n              tu <- runif(1,min = .55, max = .85)}\n                else{tu <- runif(1,min = .65, max = .95)}\n  return(tu) \n}\n#-----------------------------------------------------------------\n# 11a.Return proper data frame\n#-----------------------------------------------------------------\n  if(renew == T){\n   sth_data_renew <-  f_renewal_assignment(seed,sth_data,\n                                           f_assign_renewal)\n   return(sth_data_renew)\n  }else{ return(sth_data)}\n#-----------------------------------------------------------------\n# 11b.Calculate renewal percentage\n#----------------------------------------------------------------- \nf_assign_renewal <- function(x,renew){\n  \n if(x == 10){sample(renew,1,prob = c(.99,.01))}\n  else if(x == 9){sample(renew,1,prob = c(.98,.02))}\n   else if(x == 8){sample(renew,1,prob = c(.95,.05))}\n    else if(x == 7){sample(renew,1,prob = c(.95,.05))}\n     else if(x == 6){sample(renew,1,prob = c(.92,.08))}\n      else if(x == 5){sample(renew,1,prob = c(.90,.10))}\n       else if(x == 4){sample(renew,1,prob = c(.85,.15))}\n        else if(x == 3){sample(renew,1,prob = c(.80,.20))}\n         else if(x == 2){sample(renew,1,prob = c(.30,.70))}\n          else if(x == 1){sample(renew,1,prob = c(.25,.75))}\n           else{sample(renew,1,prob = c(.5,.5))}\n}\n#-----------------------------------------------------------------\n# 11c.Calculate renewal percentage\n#-----------------------------------------------------------------\nf_renewal_assignment <- function(seed,sth_data,f_assign_renewal){\n\n  require(dplyr)\n\n  ids <- as.data.frame(sth_data$accountID)\n  names(ids) <- \"accountID\"\n  \n  set.seed(seed)\n  centers1 <- kmeans(sth_data$ticketUsage, centers = 5)$centers\n  centers1 <- sort(centers1)\n  ids$clusterTU <- \n    kmeans(sth_data$ticketUsage, centers = centers1)$cluster\n  \n  set.seed(seed)\n  centers2 <- kmeans(sth_data$distance, centers = 5)$centers\n  centers2 <- rev(sort(centers2))\n  ids$clusterDI <- \n    kmeans(sth_data$distance, centers = centers2)$cluster\n  \n  ids$clustSum   <- ids$clusterTU + ids$clusterDI\n  sth_data_renew <- dplyr::left_join(ids,sth_data, \n                                     by = \"accountID\")\n  \n  x <- 1\n  renew <- c(\"r\",\"nr\")\n  a_renew <- list()\n  while(x <= nrow(sth_data_renew)){\n    clust <- sth_data_renew[x,3]\n    a_renew[x] <- f_assign_renewal(clust,renew)\n    x <- x + 1\n  }\n  \n  sth_data_renew$renewed <- unlist(a_renew)\n  sth_data_renew <- dplyr::select(sth_data_renew,accountID,\n                                  corporate,season,planType,\n                                  ticketUsage,tenure,\n                                  spend,tickets,distance,\n                                  renewed)\n  return(sth_data_renew)\n\n} # End\n#-----------------------------------------------------------------\n# Function to build a parabola\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\nnew_data <- f_create_lead_scoring_data(434, \n                                       100,\n                                       \"2023\",\n                                       f_calculate_tenure,\n                                       f_calculate_spend,\n                                       f_calculate_ticket_use,\n                                       f_renewal_assignment,\n                                       f_assign_renewal,\n                                       renew = F)"},{"path":"chapter2.html","id":"simulating-operations-data","chapter":"2 Constructing our data sets","heading":"2.3 Simulating Operations data","text":"Operations data might include number data sets. ’ll cover building ballpark ingress scans table . However, many others line length concessions stand number transactions F&B.","code":""},{"path":"chapter2.html","id":"ticket-scans","chapter":"2 Constructing our data sets","heading":"2.3.1 Ticket scans","text":"First can build simple function help us build parabola. function builds parabola x intercept x = 1 x = 300. points refer number observations make Chapter 10. building quadratic function spits y value value x. function takes familiar form quadratic function:\\[\\begin{equation}\n\\ {f(x)} = {ax^2} + {bx} + c\n\\end{equation}\\]can build function create many different data sets. function calculate number scans per increment normal distribution.function produce data set looks like :tells number scans happened one minute increments particular event.","code":"\n#--------------------------------------------------------------------\n# Function to build a parabola\n#--------------------------------------------------------------------\nf_calc_scans <- function(x,y,j){\n  a <- y/(x^2 - 300*x + 300)\n  z <- a*(j^2 - 301*j + 300)\n  return(z)\n}\n#-----------------------------------------------------------------\n# Function to return a scans data frame\n#-----------------------------------------------------------------\nf_get_scan_data <- function(x_value,y_value,seed,sd_mod){\n  require(FOSBAAS)\n  x_val <- x_value\n  y_val <- y_value\n  obs   <- seq(1,300, by = 1)\n  set.seed(seed)\n  scans           <- mapply(f_calc_scans,x_val,y_val,obs)\n  scan_data       <- data.frame(observations = obs,\n                                scans        = scans)\n  scan_data$scans <- round(sapply(scan_data$scans,\n                          function(x) abs(rnorm(1,x,x/sd_mod))),0)\n  return(scan_data)\n}\nscan_data <- FOSBAAS::scan_data"},{"path":"chapter2.html","id":"simulating-and-understanding-ticketing-data-sets","chapter":"2 Constructing our data sets","heading":"2.4 Simulating and understanding ticketing data sets","text":"Ticketing data seems like relatively straight-forward. isn’t. lot complexity. environment dynamic, cases can get big-data territory. context book, stay away big-data problems. Many big-data problems nothing small-data problems disguise. nature sports, three years data likely enough everything need.’ll begin simulating three seasons worth data. sake simplicity, ’ll transform data format useful applications ’ll demonstrating throughout book. four features data important:Customer details demographicsTicket purchases (including secondary market purchases)Plan purchasesQualitative data obtained surveysFrom macro-level, elements season important. analysis cover top-approach forecasting sales revenue.","code":""},{"path":"chapter2.html","id":"simulating-season-data","chapter":"2 Constructing our data sets","heading":"2.5 Simulating season data","text":"Professional baseball teams typically play 81 games. ’ll simulating three seasons data. building schedule complex process, aren’t hindered myriad constraints therefore make easy . schedule provide framework customer ticketing data build subsequent sections chapter. also important note top-approach opposed bottom-approach consisting sales granular level.","code":""},{"path":"chapter2.html","id":"function-to-manually-bias-our-season-data","chapter":"2 Constructing our data sets","heading":"2.5.1 Function to manually bias our season data","text":"function f_simulate_sales modifies certain characteristics ’ll use forecast sales later book. creating distributions numbers used place variables defined range possibilities. Instead walking code time, ’ll just tell :Creates base attendance team. instance, play BOS, CHC, NYY, LAD, STL get highest sales base.day week falls weekend sales base higher weekday.playing summer months, sales higher.school , sales higher.last game year, sales higher.opening day, sales higher.bobblehead, sales higherThis function creates pattern ticket sales data based averages normal distributions. can access function FOSBAAS package.function accepts several arguments. One function f_simulate_sales. asks three random numbers, three modifiers simply coefficients bias results , season.resulting data set looks like :Table 2.3: Sample season purchase dataThis data set simulated schedule several fields:game numberopposing teamdateday weekthe monthif weekendif schools outthe days since last gameif opening dayif promotionthe number ticket salesthe season yearThis fine base package. modeling can obviously go much deeper include statistics Las Vegas odds playoffs, granular promotional data, many others. ignoring current divisional structure sake simplicity. Structuring calendar complex task involves many factors aren’t considering :Travel timeRestrictions based collective bargaining agreementTeam level requestsThis data looks similar data actually beginning season. ’ll use forecast ticket sales chapter six. Another important piece data manifest. manifest represents seating inventory. interesting represents upper bound available sell. typical manifest look like :Table 2.4: Sample manifest data","code":"\n#-----------------------------------------------------------------\n# Function to build season data\n#-----------------------------------------------------------------\nseed1      <- 309\nseed2      <- 755\nseed3      <- 512\nmodifier   <- 1.00\ndayMod     <- 1.10\nmonthMod   <- 1.15\nseasonYear <- '2023'\n\nseason23   <- f_build_season(seed1, \n                             seed2, \n                             seed3, \n                             seasonYear,\n                             f_simulate_sales, \n                             modifier, \n                             dayMod, \n                             monthMod)\nseason_data <- FOSBAAS::season_data\nmanifest_data <- FOSBAAS::manifest_data"},{"path":"chapter2.html","id":"simulating-customer-data","chapter":"2 Constructing our data sets","heading":"2.5.2 Simulating customer data","text":"data set simply customer id name. ’ll make names id. built list names couple government websites (ssa 2020) (Census 2020).’ll use data frame simulate transformed data set pulled ticketing system. Now can use data create data set purchases secondary market.Table 2.5: Secondary market purchasesThis data set contains:seat id corresponding manifestA customer id corresponding customer listThe ticket type (single game si, season se)game id scheduleThe number tickets soldA key fieldThe original price ticketA modeled cluster fieldThe price sold secondary marketA completely clean data set like unlikely encountered outside lab. reality, customer data fraught duplication problems. data already transformed. practice join tables produce data set looks like .","code":"\ncustomer_data <- FOSBAAS::customer_data\nsecondary_data <- head(FOSBAAS::secondary_data)"},{"path":"chapter2.html","id":"simulating-demographic-data","chapter":"2 Constructing our data sets","heading":"2.6 Simulating demographic data","text":"Demographic data can used variety tasks segmentation targeted marketing efforts.Demographic data typically purchased one several vendors may contain hundreds columns. data contains:customer IDThe first name customerThe last name customerThe full name customerGenderAgeLatitudeLongitudeDistance ballparkmarital statusethnicitychildren living householdThe county customer lives","code":"\ndemo_data <- head(FOSBAAS::demographic_data)"},{"path":"chapter2.html","id":"simulating-survey-data","chapter":"2 Constructing our data sets","heading":"2.7 Simulating survey data","text":"’ll use example survey data construct specific segmentation scheme based factor analysis. ’ll base analysis example Chapman Feit’s excellent “R Marketing Research Analytics” (Chris Chapman 2015). ’ll cover survey construction later chapter.","code":""},{"path":"chapter2.html","id":"perceptual-data","chapter":"2 Constructing our data sets","heading":"2.7.1 Perceptual data","text":"initial data set take form multi-select table:data takes form aggregated survey response specific answers counted aggregated team questions. question read like :feel following sports properties? Please check apply teams listed.question allow us create perceptual map. ’ll send 5,000 customers created Section 2.5.2.produces following table:Table 2.6: Perceptual data","code":"\n#-----------------------------------------------------------------\n#-----------------------------------------------------------------\nperceptual_data             <- as.data.frame(matrix(nrow=3,ncol=10))\nnames(perceptual_data)      <- c('Friendly','Exciting','Fresh',\n                                 'Inovative','Fun','Old','Historic',\n                                 'Winners','Great','Expensive')\nrow.names(perceptual_data)  <- c('Game Hens','Grizzlies',\n                                 'Predators')\n\nset.seed(2632)\nperceptual_data <- apply(perceptual_data,1:2,\n                         function(x) round(rnorm(1,3000,1000),0))"},{"path":"chapter2.html","id":"pricing-survey-data","chapter":"2 Constructing our data sets","heading":"2.7.2 Pricing survey data","text":"data used perform van Westendorp analysis demonstrate basics qualitative pricing analytics. ’ll want data specific format.Van Westendorp26 analysis asks respondent answer series questions product related perception price. questions take following form:price consider product expensive consider buying ? (expensive)price consider product priced low feel quality couldn’t good? (cheap)price consider product starting get expensive, question, give thought buying ? (Expensive/High Side)price consider product bargain—great buy money? (Cheap/Good Value)questions taken directly Wikipedia article. resulting data set resemble following:Table 2.7: Van Westendorp survey dataThese data sets represent main top-line data tend use working club.","code":"\n#-----------------------------------------------------------------\n#-----------------------------------------------------------------\nvw_data <- data.frame(matrix(nrow = 1000, ncol = 6))\nnames(vw_data) <- c('DugoutSeats', 'PriceExpectation', \n                    'TooExpensive', 'TooCheap', \n                    'WayTooCheap', 'WayTooExpensive')\nset.seed(715)\nvw_data[,1] <- 'DugoutSeats'\nvw_data[,2] <- round(rnorm(1000,100,10),0)\nvw_data[,3] <- round(rnorm(1000,130,20),0)\nvw_data[,4] <- round(rnorm(1000,60,15),0)\nvw_data[,5] <- round(rnorm(1000,50,10),0)\nvw_data[,6] <- round(rnorm(1000,160,20),0)"},{"path":"chapter2.html","id":"housing-your-data-and-functions-in-a-package","chapter":"2 Constructing our data sets","heading":"2.8 Housing your data and functions in a package","text":"book isn’t meant expose use R language, R great features analytic work worth exploring. make lives easier move subsequent chapters. Analyitical work tends repetitive housing functions package make life easier worth living. many resources available make building package easy ’ll cover basics .two packages make building packages really simple: devtools (Wickham, Hester, et al. 2022) roxygen (Wickham, Danenberg, et al. 2022). also numerous resources build package. Rstudio even built--tools makes process even easier. want demonstrate process really easy incredibly useful.","code":""},{"path":"chapter2.html","id":"building-a-simple-package","chapter":"2 Constructing our data sets","heading":"2.8.1 Building a simple package","text":"’ll want open RStudio create new R file. Creating basic package takes steps:Now installed packages need can use create package:can create data set want include package.place data package.Finally, install package.can access data using packages namespace.lot , especially around documentation. However, now housed data package can easily access. begin using R heavily highly recommend putting time understanding create use packages. little feature save lot time. also allows share work service Github working remotely team.","code":"\n#-----------------------------------------------------------------\n# Step 1: Download the utility packages\n#-----------------------------------------------------------------\ninstall.packages(\"devtools\")\ninstall.packages(\"roxygen2\")\n##browseVignettes(\"devtools\")\n##browseVignettes(\"roxygen2\")\n#-----------------------------------------------------------------\n# Step 2: Create a shell for your package\n#-----------------------------------------------------------------\ndevtools::create(\"UselessRPackage\")\n#-----------------------------------------------------------------\n# Step 3: Build a data set\n#-----------------------------------------------------------------\ndevtools::document()\nuselessData <- seq(1:755)\n#-----------------------------------------------------------------\n# Step 4: Place data set in package\n#-----------------------------------------------------------------\nusethis::use_data(uselessData, overwrite = T)\n#-----------------------------------------------------------------\n# Step 5: Install the package\n#-----------------------------------------------------------------\ndevtools::install()\n#-----------------------------------------------------------------\n# Step 5: Access your data\n#-----------------------------------------------------------------\nUselessRPackage::uselessData"},{"path":"chapter2.html","id":"key-concepts-and-chapter-summary-1","chapter":"2 Constructing our data sets","heading":"2.9 Key concepts and chapter summary","text":"chapter written introduce little code R explain data data sets. also demonstrates concepts cover ensuing chapters. also covered R may may good choice use analysis.Additionally, covered basics build package R. Analysis tends repetitive. Housing specific functions package simple way document functions easily access . R’s extensibility one best features makes incredibly flexible tool (especially coupled RStudio).Chapter 3 delve data creating graphics summarizing data. Subsequent chapters explore additional functionality delve solving specific problems. also spend significant amount time explaining think problems solve .","code":""},{"path":"chapter3.html","id":"chapter3","chapter":"3 Exploring your data","heading":"3 Exploring your data","text":"R makes easy exploratory work data. numerous packages functions make simple visualize tabulate data. ’ll leverage many packages outlined book “R Data Science” (Wickham 2017) graphics created excellent graphics package called ggplot2 (Wickham, Chang, et al. 2022). ggplot2 makes easy look data many different ways.’ll also recommend several methods analyzing data. “R Action” Robert Kabacoff (Kabacoff 2011) outstanding job demonstrating cursory data analysis variety data sets. ’ll reference book several times chapter. innumerable statistical methods rubrics analyzing data, graphs best job conveying information encounter book. always important don’t like building analytics may .","code":""},{"path":"chapter3.html","id":"building-a-consistent-design-language","chapter":"3 Exploring your data","heading":"3.1 Building a consistent design language","text":"several graphics paradigms available R elsewhere. D3 (d3 2020) outstanding javascript library building dynamic graphics. Python also contains several libraries seaborn (Waskom 2021) help construct statistical graphics. ’ll use ggplot2 (Wickham, Chang, et al. 2022) paradigm graphics book. ggplot2 make easy build consistent visually appealing graphics R.Let’s start constructing graphics theme use throughout examples. begin example creating color palette can use modify. Afterward, simply set variable equal set parameters corresponding components images creating.object graphics_theme_1 used subsequent graphs. keep looking consistent. can easily adjust element simply adding plot theme. override graphics theme illustrates elegance paradigm. Let’s pull data FOSBAAS package take look .Table 3.1: Example schedule dataWe’ve already talked data, next sections demonstrate sorts plots exploratory analysis typically perform help understand patterns data. want review data simply type ?FOSBAAS::season_data R console. part analysis critical. helps understand data can perform sophisticated modeling later steps. also best way communicate patterns data less experience . ’ll also offer word warning. people think distributions. Simple bar line plots get traction folks. Don’t attempt explain box plot someone meeting. lose room.","code":"\nlibrary(ggplot2)\nlibrary(dplyr)\n#-----------------------------------------------------------------\n# creating a color palette\n#-----------------------------------------------------------------\npalette <- c('dodgerblue','grey25','coral','mediumseagreen',\n             'orchid','firebrick','goldenrod','cyan',\n             'brown','steelblue','magenta')\n#-----------------------------------------------------------------\n# Creating a custom theme\n#-----------------------------------------------------------------\ngraphics_theme_1 <- ggplot2::theme() + \n  theme(axis.text.x  = element_text(angle  = 0, size  = 14, \n                                    vjust  = 0, color = \"grey10\"),  \n        axis.text.y  = element_text(angle  = 0, size  = 14, \n                                    vjust  = 0, color = \"grey10\"),  \n        axis.title.x = element_text(size   = 16, face = \"plain\", \n                                    colour = \"grey10\"), \n        axis.title.y = element_text(size   = 16, face = \"plain\", \n                                    color  = \"grey10\"), \n        legend.title = element_text(size   = 14, face = \"plain\", \n                                    color  = \"grey10\"), \n        legend.text  = element_text(size   = 11, \n                                    color  = \"grey10\"), \n        plot.title   = element_text(colour = \"grey10\", \n                                    size   = 14, angle = 0, \n                                    hjust  = .5, vjust = .5, \n                                    face   = \"bold\"), \n        legend.position   = \"right\", \n        legend.background = element_rect(fill     = \"grey99\", \n                                         size     = 3,  \n                                         linetype = \"solid\", \n                                         colour   = \"grey99\"), \n        legend.key        = element_rect(fill     = \"grey99\", \n                                         color    = \"grey99\"), \n        strip.background  = element_rect(fill     =  \"grey99\", \n                                         colour   = \"grey99\"), \n        strip.text        = element_text(size     = 14, \n                                         face     = \"plain\", \n                                         color    = \"grey10\"), \n        panel.grid.major  = element_line(colour   = \"grey80\"),  \n        panel.grid.minor  = element_line(colour   = \"grey80\"), \n        panel.background  = element_rect(fill     = \"grey99\", \n                                         colour   = \"grey99\"), \n        plot.background   = element_rect(fill     = \"grey99\", \n                                         colour   = \"grey99\"))\n#-----------------------------------------------------------------\n# High level schedule data\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\nseason_data <- FOSBAAS::season_data"},{"path":"chapter3.html","id":"histograms-and-density-plots","chapter":"3 Exploring your data","heading":"3.2 Histograms and density plots","text":"Histograms density plots critical understanding underlying structure data. Using point estimate average good way misinterpret data. Learn think distributions. DMV look line customers think height distribution. watch Olympics think distribution times athletes ran swam. everywhere else . following two graphs called simple commands: geom_histogram geom_density.code creates graph figure 3.1. Histograms incredibly useful understanding data set’s structure. unfamiliar , demonstrate count specific instance something (case, ticket sales). also included rug bottom graph. rug demonstrates data points actually lie graph. may unnecessary, find helps interpret graph little easily dealing smaller data sets. especially true density plots.following diagram also layers different seasons diagram using colors. colors use important. aren’t going cover design , basic understanding color wheel.27 Google . also several R packages deal color RColorBrewer (Neuwirth 2022). package designed maps demonstrates point trying make. Put thought colors use use . ’ll give graphics polished professional look.\nFigure 3.1: Histogram Ticket Sales\ncan interpret plot? Honestly, difficult see much difference. looks like 2024 higher percentage sellouts. also looks like huge spike around 31,000 tickets. Perhaps better ways help us understand data.small change, can create density plot. Density plots convey similar information histograms, can confusing individuals. Know audience choosing use one interpretable histogram. can see plot figure 3.2.kernel density plot represents density particular point. Think much area lives curve. plot looks like :\nFigure 3.2: Density Plot Ticket Sales\ngraph helps us visualize skew little better histogram. used alpha argument make graphs transparent. much easier see 2024 skews right 2022 2023.can interpret data curve certain point? can approximate fairly easily. multiple methods don’t necessarily integrate data. However, use integral R can help . Keep mind actually calculus working club business side probably something wrong. fact, using calculus probably something wrong. Take step back.can use density function help understand going curves.part intuitive. can now use data make observations specific points x axis.thirteen percent area curve points 40,000 tickets. included calculations demonstrative purposes. make density plots easier understand. perform calculations density curve help us understand data little granular fashion.Histograms density plots particular use performing certain statistical analyses data. ’ll use frequently. Familiarize learn love .","code":"\n#-----------------------------------------------------------------\n# Histograms\n#-----------------------------------------------------------------\nseason_data <- FOSBAAS::season_data\nx_label  <- ('\\n Ticket Sales')\ny_label  <- ('Count \\n')\ntitle    <- ('Distribution of Seasonal Ticket Sales')\nlegend   <- ('Season')\nhist_sales <- \n  ggplot2::ggplot(data  = season_data,\n                  aes(x = ticketSales,\n                      fill  = factor(season)))         +\n  geom_histogram(binwidth = 1000)                      +\n  scale_fill_manual(legend, values = palette)          +\n  geom_rug(color = 'coral')                            +\n  scale_x_continuous(label = scales::comma)            +\n  scale_y_continuous(label = scales::comma)            +\n  xlab(x_label)                                        + \n  ylab(y_label)                                        + \n  ggtitle(title)                                       +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Kernel density plot\n#-----------------------------------------------------------------\nx_label  <- ('\\n Ticket Sales')\ny_label  <- ('Density \\n')\ntitle    <- ('Distribution of Seasonal Ticket Sales')\nlegend   <- ('Season')\ndensity_sales <- \n  ggplot2::ggplot(data = season_data, \n                  aes(x    = ticketSales, \n                      fill = factor(season)))                +\n  geom_density(alpha = .5)                                   +\n  scale_fill_manual(legend,values = palette)                 +\n  geom_rug(color = 'coral')                                  +\n  scale_x_continuous(label = scales::comma)                  +\n  scale_y_continuous(label = scales::percent)                +\n  xlab(x_label)                                              + \n  ylab(y_label)                                              + \n  ggtitle(title)                                             +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Demonstrate AUC\n#-----------------------------------------------------------------\nden      <- density(season_data$ticketSales)\nbin_size <- (den$x[2] - den$x[1])\n\nround(sum(den$y) * bin_size,2) # Approximates to 1\n#> [1] 1\n#-----------------------------------------------------------------\n# Demonstrate AUC at 40,000 Tickets\n#-----------------------------------------------------------------\nsum(den$y[den$x >= 40000]) * bin_size\n#> [1] 0.1337782"},{"path":"chapter3.html","id":"box-faceted-and-scatter-plots","chapter":"3 Exploring your data","heading":"3.3 Box, faceted, and scatter plots","text":"can also easily split data season analyze distributions slightly differently. case, ’ve used facet_grid(season ~ .) argument split graph season. really isn’t anything different . function facet_wrap() something similar. Try see difference. faceted histogram can seen figure 3.3.\nFigure 3.3: Faceted histogram\nNotice restricted graph one color. Avoid multiple colors graphs split data. don’t need multiple ways distinguish individual subsets data . One way suffice.get interpretation graph preceeding one. 2024 skewed right likely indicating higher average ticket sales. 2023 appears clusters attendance, 2022 fairly evenly distributed. Perhaps differences schedule.Splitting graphs various features useful ’ll see frequently. can create box-plots changing one argument. following code produces box-plot figure 3.4.Boxplots preferred method looking distributions. case, blended boxplot scatter plot. interpret boxplot? black line middle colored section represents median. box represents 50th percentile values. interquartile (IQR) range representing values 25% 50%. whiskers represent calculation based range dots represent outliers.\nFigure 3.4: Segmented histogram box-whisker plots\ndata easier interpret fashion. IQR skewed much higher clearly larger median value. use median instead mean? median safer interpretation looking non-normal distributions (distributions skew). actually best look , get .’ll explore one type basic plot. violin plot blend density plot boxplot. following code produces plot figure 3.5.\nFigure 3.5: Segmented violin plot\nthink violin plots interesting, typically don’t use . prefer split density plots boxplots separate graphs. may feel differently. recommend showing one plots someone limited statistical experience. Perhaps shouldn’t use .","code":"\n#-----------------------------------------------------------------\n# Faceting a plot\n#-----------------------------------------------------------------\nx_label  <- ('\\n Ticket Sales')\ny_label  <- ('Count \\n')\ntitle    <- ('Distribution of Seasonal Ticket Sales')\nhistogram_sales_facet <- \n  ggplot2::ggplot(data = season_data, \n                  aes(x = ticketSales))                        +\n  facet_grid(season ~ .)                                       +\n  geom_histogram(binwidth = 1000, fill = palette[1])           +\n  geom_rug(color = 'coral')                                    +\n  scale_x_continuous(label = scales::comma)                    +\n  scale_y_continuous(label = scales::comma)                    +\n  xlab(x_label)                                                + \n  ylab(y_label)                                                + \n  ggtitle(title)                                               +\n  graphics_theme_1 \n#-----------------------------------------------------------------\n# Box plots\n#-----------------------------------------------------------------\nx_label  <- ('\\n Season')\ny_label  <- ('Ticket Sales \\n')\ntitle    <- ('Distribution of Seasonal Ticket Sales')\nbox_sales <- \n  ggplot2::ggplot(data  = season_data, \n                  aes(x = factor(season), \n                      y = ticketSales))               +\n  geom_boxplot(fill = 'dodgerblue')                   +\n  geom_jitter(alpha = .2,  height = 0, \n              width = .25, color  = 'coral')          +\n  geom_rug(color = 'coral')                           +\n  scale_y_continuous(label = scales::comma)           +\n  xlab(x_label)                                       + \n  ylab(y_label)                                       + \n  ggtitle(title)                                      +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# violin plot\n#-----------------------------------------------------------------\nx_label  <- ('\\n Season')\ny_label  <- ('Ticket Sales \\n')\ntitle    <- ('Distribution of Seasonal Ticket Sales')\nviolin_sales <-\n  ggplot2::ggplot(data = season_data, \n                  aes(x = factor(season), \n                      y = ticketSales))             +\n  geom_violin(fill = 'dodgerblue')                  +\n  geom_jitter(alpha = .35, height = 0, \n              width = .25, color = 'coral')         +\n  geom_rug(color = 'coral')                         +\n  scale_y_continuous(label = scales::comma)         +\n  xlab(x_label)                                     + \n  ylab(y_label)                                     + \n  ggtitle(title)                                    +\n  graphics_theme_1"},{"path":"chapter3.html","id":"line-and-tile-plots","chapter":"3 Exploring your data","heading":"3.4 Line and tile plots","text":"Line plots tile plots can used various ways. Line plots tend used demonstrate sort longitudinal trend. Tile plots can help visualize data multidimensional correlates features displayed. Line plots need sort grouping variable can occasionally frustrating work . general, put together way plots seen. following code produces plot figure 3.6.Line plots mainstay analysis. typically leveraged demonstrate change time series data points. pretty familiar . word warning can become busy. Perhaps plot better lines placed seperate graph.\nFigure 3.6: Line Tile Plots\nappears degree seasonality data. can see games relatively low attendance middle season. Perhaps school summer months helps bolster attendance. Baseball also doesn’t major sports competing months.look past sixtieth game notice 2024 surge attendance. Maybe playoff race. Line graphs also interesting notice trend seasonality might able decompose components using exponential smoothing even moving average. practice, found limited use techniques sports, something think aware .Tile plots (heatmaps) add level dimensionality data can extremely useful. next example little different. need aggregation data visualizing . ’ll use dplyr transformations found book. just makes life easier. different tend see languages python (unless ported). uses pipe %>% say words . ’ll seeing lot try data. simply selecting columns, grouping , creating aggregation. feels similar SQL. wonder ?… plot can seen figure 3.7.\nFigure 3.7: Tile Plots\ninterpret data? can clearly see Friday Saturday tend higher average ticket sales. shouldn’t surprise. also looks like Sundays March well. Beware! likely artifact reduced sample size. Overall, Heatmaps good job visualizing correlations, can misleading. careful always consider sampling issues.can also pump dplyr data directly ggplot. See .Let’s also take look variation heatmap called hex plot. can used slightly different ways. can see example hex plot figure 3.8.\nFigure 3.8: Hex Plots\ngraphic little different heatmap. demonstrates number games fall bin. tend use plots continuous variables x y axis interested density point. pay attention can perceive seasonality evident around game forty.","code":"\n#-----------------------------------------------------------------\n# Line plot\n#-----------------------------------------------------------------\nx_label <- ('\\n Game Number')\ny_label <- ('Ticket Sales \\n')\ntitle   <- ('Ticket Sales by Game Number')\nlegend  <- 'Season'\nline_sales <- \n  ggplot2::ggplot(data      = season_data, \n                  aes(x     = gameNumber,\n                      y     = ticketSales,\n                      color = factor(season)))             +\n  geom_line(size = .9)                                     +\n  scale_color_manual(legend, values = palette)             +\n  scale_y_continuous(label = scales::comma)                +\n  xlab(x_label)                                            + \n  ylab(y_label)                                            + \n  ggtitle(title)                                           +\n  graphics_theme_1 + theme(legend.position   = \"bottom\")\n#-----------------------------------------------------------------\n# Tile plot or heat map\n#-----------------------------------------------------------------\nx_label <- ('\\n Day of Week')\ny_label <- ('Month \\n')\ntitle   <- ('Ticket Sales by Day and Month')\n# compress data into an easier format\nsd_comp <- season_data                    %>% \n  select(dayOfWeek,month,ticketSales)     %>%\n  group_by(dayOfWeek,month)               %>%\n  summarise(avgSales = mean(ticketSales))\n\ntile_sales <- \n  ggplot2::ggplot(data     = sd_comp, \n                  aes(x    = dayOfWeek,\n                      y    = month,\n                      fill = avgSales))                       +\n  geom_tile()                                                 +\n  scale_fill_gradient(low = \"white\", high = \"dodgerblue\",\n                      name = 'Tickets',label = scales::comma) +\n  xlab(x_label)                                               + \n  ylab(y_label)                                               + \n  ggtitle(title)                                              +\n  graphics_theme_1\n\n\n  season_data                             %>% \n  select(dayOfWeek,month,ticketSales)     %>%\n  group_by(dayOfWeek,month)               %>%\n  summarise(avgSales = mean(ticketSales)) %>%\n\n  ggplot2::ggplot(aes(x    = dayOfWeek,\n                      y    = month,\n                      fill = avgSales))   +\n  geom_tile() \n#-----------------------------------------------------------------\n# Hexplots\n#-----------------------------------------------------------------\nx_label  <- ('\\n Game Number')\ny_label  <- ('Ticket Sales \\n')\ntitle   <- ('Ticket Sales by game')\n\nhex_sales <- \n  ggplot2::ggplot(data     = season_data, \n                  aes(x    = gameNumber,\n                      y    = ticketSales))                    +\n  geom_hex()                                                  +\n  scale_fill_gradient(low = \"dodgerblue\", high = \"coral\",\n                      name = 'Count',label = scales::comma)   +\n    scale_y_continuous(label = scales::comma)                 +\n  xlab(x_label)                                               + \n  ylab(y_label)                                               + \n  ggtitle(title)                                              "},{"path":"chapter3.html","id":"bar-plots","chapter":"3 Exploring your data","heading":"3.5 Bar plots","text":"want use pie chart, don’t. Bar plots almost always better choice. Visualizing relationships area simply easier bar plot. Bar plos probably popular way display data, find lacking. , really need train think distributions. Bar Plots counts. basic bar plot produced following code can seen figure 3.9.\nFigure 3.9: Proportion sales year weekend\nexample looking percentages tickets sold weekend vs. weekdays season. graph great job . can clearly see weekends make higher proportion sales 2024. also know weekdays make majority sales.can build variation plot position argument. Now can take look overall numbers. second bar plot can viewed figure 3.10.\nFigure 3.10: Barplot sales\ncan see 2022 2023 almost identical 2024 significantly ticket sales. type plot might use descriptive reporting. something accounting want see. don’t need know something happened, typically just need . true cases face. Make sure clearly explain .","code":"\n#-----------------------------------------------------------------\n# Bar plot version one\n#-----------------------------------------------------------------\nx_label  <- ('\\n Season')\ny_label  <- ('Proportion of Ticket Sales \\n')\ntitle    <- ('Proportion of Ticket Sales by DOW')\nbar_sales_pro <- \n  ggplot2::ggplot(data     = season_data, \n                  aes(y    = ticketSales,\n                      x    = season,\n                      fill = weekEnd))                   +\n  geom_bar(stat = 'identity',position = 'fill')          +\n  scale_fill_manual(values = palette, name = 'Weekend')  +\n  scale_y_continuous(label = scales::percent)            +\n  xlab(x_label)                                          + \n  ylab(y_label)                                          + \n  ggtitle(title)                                         +\n  graphics_theme_1 + theme(legend.position   = \"bottom\")\n#-----------------------------------------------------------------\n# Bar plot version two\n#-----------------------------------------------------------------\nx_label  <- ('\\n Season')\ny_label  <- ('Ticket Sales \\n')\ntitle    <- ('Ticket Sales by DOW')\nbar_sales <- \n  ggplot2::ggplot(data     = season_data, \n                  aes(y    = ticketSales,\n                      x    = season,\n                      fill = weekEnd))                  +\n  geom_bar(stat = 'identity', position = 'stack')       +\n  scale_fill_manual(values = palette, name = 'Weekend') +\n  scale_y_continuous(label = scales::comma)             +\n  xlab(x_label)                                         + \n  ylab(y_label)                                         + \n  ggtitle(title)                                        +\n  graphics_theme_1 + theme(legend.position   = \"bottom\")"},{"path":"chapter3.html","id":"a-final-word-on-graphics","chapter":"3 Exploring your data","heading":"3.6 A final word on graphics","text":"cautious represent data. just covered absolute rudiments creating graphs demonstrated commonly used instruments. Always think simplest way illustrate point trying make. Bar plots usually fine. like follow rules building graphs, don’t take word . Many people tell exactly things.Never use pie chart. difficult interpret bar plotDon’t use two Y axes. almost always better use two graphs differences scale units can confusing.Stick consistent color scheme.Stick consistent font schemeDon’t put much one graph. many colors shapes make graphs confusing. Always simplify.Additionally, didn’t cover dynamic plots. R provides interesting capabilities front. Shiny (Chang et al. 2022) allows build cool interactive web apps. also innumerable tools Tableau 28 Looker provide good business intelligence capability. Often times tools easier use quick ad hoc data exploration, lot utility putting everything code. automatically documents work can read . also gives readily available reference. find work can tend repetitive. use lot variations theme. code already written. just need adapt .","code":""},{"path":"chapter3.html","id":"summarizing-the-data","chapter":"3 Exploring your data","heading":"3.7 Summarizing the data","text":"section make heavy use R package already seen named dplyr (Wickham, François, et al. 2022). ’ll use dplyr easy read performant data sets ’ll encounter. ’ll also reference couple packages useful summarizing data psych (Revelle 2022). Summarizing data often accouterment graphs. don’t always , find often.Let’s create simple summary ticket sales day week using dplyr.Table 3.2: Mean ticket sales day weekdplyr wide assortment useful tools manipulating data. highly recommend leveraging whenever possible. edification, identical table can produced base R using function:many data manipulation packages data.table (Dowle Srinivasan 2021) plyr (Wickham 2022b). Let’s take look examples. frequently find looking quantiles. can use data set simple fences segmentation.case know 25% observations 26,126.5 tickets sold. can access components object brackets index. Many object R can accessed indexes. comes handy scripting.Turning data long wide format vise-versa something also common task. Perhaps like see Giants Baltimore need reformat data. little complex, demonstrates logically system operates. ’ll use dplyr tidyr (Wickham Girlich 2022) libraries achieve result. can perform tasks SQL, won’t intuitive user friendly . Learn use SQL vs. R vice-versa.Table 3.3: Median ticket sales day weekWe now table tells us exactly want know. practice probably need data long format often wide format. Let’s pretend want graph data. way ggplot works probably want convert data long format like example .Table 3.4: Median ticket sales day weekThis data much easier use ggplot. part, main operations performing. can much complex, just went main building blocks. understand just covered, surprised far can take .","code":"\n#-----------------------------------------------------------------\n# Creating a summary statistics table\n#-----------------------------------------------------------------\nlibrary(dplyr)\naverage_by_dow <- \nFOSBAAS::season_data                          %>% \n  group_by(dayOfWeek)                         %>% \n  summarise(AverageSales = mean(ticketSales))\n#-----------------------------------------------------------------\n# Creating a summary statistics table using 'by'\n#-----------------------------------------------------------------\nby(FOSBAAS::season_data$ticketSales,\n   FOSBAAS::season_data$dayOfWeek,function(x) mean(x))\n#> FOSBAAS::season_data$dayOfWeek: Fri\n#> [1] 38794.03\n#> --------------------------------------------- \n#> FOSBAAS::season_data$dayOfWeek: Mon\n#> [1] 28243.46\n#> --------------------------------------------- \n#> FOSBAAS::season_data$dayOfWeek: Sat\n#> [1] 38146.29\n#> --------------------------------------------- \n#> FOSBAAS::season_data$dayOfWeek: Sun\n#> [1] 29690.38\n#> --------------------------------------------- \n#> FOSBAAS::season_data$dayOfWeek: Thu\n#> [1] 28560.53\n#> --------------------------------------------- \n#> FOSBAAS::season_data$dayOfWeek: Tue\n#> [1] 28217.72\n#> --------------------------------------------- \n#> FOSBAAS::season_data$dayOfWeek: Wed\n#> [1] 28893.47\n#-----------------------------------------------------------------\n# Getting quantiles\n#-----------------------------------------------------------------\nquants <- quantile(FOSBAAS::season_data$ticketSales, \n                   probs = c(0,.10,.25,.5,.75,.9,1))\nquants\n#>      0%     10%     25%     50%     75%     90%    100% \n#> 19920.0 22797.6 26126.5 30956.0 35664.0 40816.6 45000.0\n#-----------------------------------------------------------------\n# Getting quantiles\n#-----------------------------------------------------------------\nquants[3]\n#>     25% \n#> 26126.5\n#-----------------------------------------------------------------\n# Converting to wide format\n#-----------------------------------------------------------------\nlibrary(dplyr)\nlibrary(tidyr)\nteam_dow <- \n  FOSBAAS::season_data                                   %>%\n  select(team,dayOfWeek,ticketSales)                     %>%\n  filter(team %in% c('SF','BAL'))                        %>%\n  group_by(team,dayOfWeek)                               %>%\n  summarise(medianSales = median(ticketSales),\n            games       = n())                           %>%\n  tidyr::pivot_wider(names_from  = team,\n                     values_from = c(medianSales,games)) %>%\n  mutate(difference = medianSales_BAL - medianSales_SF)  %>%\n  arrange(difference)                          \n#-----------------------------------------------------------------\n# Converting to long format\n#-----------------------------------------------------------------\nlibrary(dplyr)\nlibrary(tidyr)\nteam_dow_long <- \n  team_dow                                          %>%\n  select(dayOfWeek, medianSales_BAL,medianSales_SF) %>%\n  tidyr::pivot_longer(!dayOfWeek, \n                      names_to  = \"club\", \n                      values_to = \"medianSales\")\n                         "},{"path":"chapter3.html","id":"getting-statistical-information","chapter":"3 Exploring your data","heading":"3.7.1 Getting statistical information","text":"also thought might useful mention ways get statistical data without calculate . R excels several people extended ’s capabilities. ’ll show couple ones used . psych (Revelle 2022) library contains many useful functions manipulating summarizing data. ’ll use describe get basic statistical features data.Table 3.5: Summary StatisticsThis gives good overview structure ticket sales data looks like. ’s also good opportunity point one R’s quirks. psych package Hmisc (Harrell 2022) package function name (describe). even similar things. cautious dependencies can cause problems. load packages can overwrite . get around issue can always leverage full namespace: psych::describe instead describe.following example uses Hmisc package display summary statistics.can remove libraries environment using couple different commands: (unloadNamespace(\"Hmisc\") detach(\"package:Hmisc\")) Read documentation understand differences.Table 3.6: Summary Statistics using HmiscAs can see, somebody else done hard work . just need know look. Keep mind R packages high quality. software user-generated provided free. lots reputable packages, also lots poorly written. sometimes useful look provenance.","code":"\n#-----------------------------------------------------------------\n# Summary stats psych\n#-----------------------------------------------------------------\nlibrary(psych)\npsy_desc <- \n  t(data.frame(psych::describe(FOSBAAS::season_data$ticketSales)))\n#-----------------------------------------------------------------\n# Summary stats Hmisc\n#-----------------------------------------------------------------\nhmisc_desc <- (Hmisc::describe(FOSBAAS::season_data$ticketSales))\nhmisc_desc <- unlist(hmisc_desc$counts)\nhmisc_desc <- as.data.frame(hmisc_desc)"},{"path":"chapter3.html","id":"building-models-and-basic-statistics","chapter":"3 Exploring your data","heading":"3.7.2 Building models and basic statistics","text":"’ll apply rigor models progress ensuing chapters. However, want introduce couple concepts chapter. add complexity move actual projects.","code":""},{"path":"chapter3.html","id":"anova","chapter":"3 Exploring your data","heading":"3.7.2.1 ANOVA","text":"analysis variance commonly used tool gauge differences groups. going take cursory look one version ANOVA. lots different types ANOVA ’ll want put time understanding use . Additionally, ANOVA output equivalent regression output. ’ll use want understand differences groups. ways analyze differences groups students T test non-parametric methods. .following example one-way ANOVA covariate. want look differences ticket sales promotion day week. can begin building fequency table.group completely different representations indicate balanced design. matter? certainly . Test group differences using aov function.Table 3.7: ANOVA resultsWe’ll discuss designing experiments (DOE) later chapter. now, ’ll simply walk output. aren’t rigorous . Basically, P Value looks good F statistic isn’t super small. can move forward. Let’s take look statistics graph.\nFigure 3.11: Group means standard error promotion dow\nmodel output isn’t incredibly informative, graph gives lot information. model constructed can access parts dollar sign: mod$terms. lot information explore models begin making complex. case, can see sample issues.can also see certain promotions may effective others day week may impact effective promotion might absolute terms. Let’s apply Tukey test29 data. test help us compare pairs means based students t test. aren’t familiar tests, little research . can useful.Table 3.8: Tukey ComparisonsIn case, mean ticket sales bobbleheads concerts aren’t significantly different . mean? KPI ticket sales different marginal costs associated concerts bobbleheads less expensive option might preferable. Let’s visualize results.\nFigure 3.12: Group means standard error promotion dow\ninterval crosses zero, isn’t significant can potentially disregard information inconclusive.","code":"\n#-----------------------------------------------------------------\n# Build a frequency table\n#-----------------------------------------------------------------\ntable(FOSBAAS::season_data$promotion)\n#> \n#> bobblehead    concert       none      other \n#>         16          8        212          7\n#-----------------------------------------------------------------\n# One Way ANOVA\n#-----------------------------------------------------------------\nmod <- aov(ticketSales ~ promotion + dayOfWeek,\n           data = FOSBAAS::season_data)\n#summary(mod)\n#-----------------------------------------------------------------\n# Viewing group means\n#-----------------------------------------------------------------\nlibrary(dplyr)\ngraph_table <- FOSBAAS::season_data %>%\n               \n    select(promotion,ticketSales,dayOfWeek,daysSinceLastGame)  %>%\n    group_by(promotion,dayOfWeek)                              %>%\n    summarise(sales = mean(ticketSales),\n              daysSinceLastGame = mean(daysSinceLastGame),\n              N = n(),\n              sd = sd(ticketSales),\n              se = sd/sqrt(N))         \n\nx_label <- 'Day of Week'                                             \ny_label <- 'Mean ticket sales'                                            \ntitle   <- 'Group means and standard error: promos and sales'\nse <- \nggplot(graph_table, aes(y=sales, \n                        x=reorder(dayOfWeek,sales,mean), \n                        color=promotion))                     + \n    geom_errorbar(aes(ymin = sales-se, ymax = sales+se), \n                  width =.3,size = 1,\n                  position = position_dodge(0.25))            +\n    geom_point()                                              +\n    scale_color_manual(values = palette)                      +\n    scale_y_continuous(label = scales::comma)                 +\n    xlab(x_label)                                             + \n    ylab(y_label)                                             + \n    ggtitle(title)                                            +\n    graphics_theme_1 \n#-----------------------------------------------------------------\n# Tukey test\n#-----------------------------------------------------------------\ntu_test <- TukeyHSD(mod)\n#-----------------------------------------------------------------\n# Viewing group means\n#-----------------------------------------------------------------\nx_label <- 'Value'                                             \ny_label <- 'Promotion Comps'                                            \ntitle   <- '95% CI comps by promotion '\ntu_comp_graph <- \nggplot(tu_comp, aes(x=diff, \n                    y=promotion))                             + \n    geom_errorbar(aes(xmin = lwr, xmax = upr), \n                  width =.3,size = 1)                         +\n    geom_point()                                              +\n    scale_x_continuous(label = scales::comma)                 +\n    xlab(x_label)                                             + \n    ylab(y_label)                                             + \n    ggtitle(title)                                            +\n    geom_vline(xintercept = 0,color = 'red',lty = 2)          +\n    graphics_theme_1 "},{"path":"chapter3.html","id":"linear-regression","chapter":"3 Exploring your data","heading":"3.7.2.2 Linear Regression","text":"’ll go regression detail several later chapter.. Functionally, output ANOVA.Table 3.9: Summary statisticsYou can use coefficients explain impact specific variables. many tickets month July worth? coefficient (5264.39) represents impact. July gives five-thousand ticket bump. can also use model make predictions. model completely overfit, doesn’t matter point. ’ll look ways make sure don’t overfit models subsequent chapters. Let’s put predictions graph.\nFigure 3.13: Prediction vs. acutal values\ncan see model pretty good job approximating sales. ’ll go complete examples later.","code":"\n#-----------------------------------------------------------------\n# Creating a simple linear model\n#-----------------------------------------------------------------\nln_mod <- lm(ticketSales ~ team+dayOfWeek+month+\n                           daysSinceLastGame+openingDay+promotion,\n                           data = FOSBAAS::season_data)\n\nln_mod \n#> \n#> Call:\n#> lm(formula = ticketSales ~ team + dayOfWeek + month + daysSinceLastGame + \n#>     openingDay + promotion, data = FOSBAAS::season_data)\n#> \n#> Coefficients:\n#>       (Intercept)            teamATL            teamBAL  \n#>          37993.73            -579.17            -612.86  \n#>           teamBOS            teamCHC            teamCIN  \n#>          10479.61            8873.01            -196.89  \n#>           teamCLE            teamCOL            teamCWS  \n#>          -1047.45            -488.43            1914.73  \n#>           teamDET            teamFLA            teamHOU  \n#>           -445.81           -1755.96            5301.38  \n#>           teamKAN            teamLAA            teamLAD  \n#>          -1729.86            4876.19           11798.00  \n#>           teamMIL            teamMIN            teamNYM  \n#>          -3877.18             330.59            4384.53  \n#>           teamNYY            teamOAK            teamPHI  \n#>           7362.71            1123.20            3590.36  \n#>            teamSD             teamSF             teamTB  \n#>          -1422.63            3995.62           -1543.36  \n#>           teamTEX            teamWAS       dayOfWeekMon  \n#>          -2309.79           -1107.37           -8998.56  \n#>      dayOfWeekSat       dayOfWeekSun       dayOfWeekThu  \n#>           -247.95           -8519.78           -8806.70  \n#>      dayOfWeekTue       dayOfWeekWed           monthAug  \n#>          -8724.49           -8947.95             -79.88  \n#>          monthJul           monthJun           monthMar  \n#>           5264.39            2986.79           -1522.76  \n#>          monthMay           monthOct           monthSep  \n#>            184.91           -1699.57           -1255.60  \n#> daysSinceLastGame     openingDayTRUE   promotionconcert  \n#>            279.20            3986.05            -929.53  \n#>     promotionnone     promotionother  \n#>          -4598.88           -2877.06\n#-----------------------------------------------------------------\n# Using the regression output\n#-----------------------------------------------------------------\nseasons <- FOSBAAS::season_data\nseasons$pred <- predict(ln_mod)\n\nx_label <- 'Actual Sales'                                             \ny_label <- 'Predicted Sales'                                            \ntitle   <- 'Actual Sales vs. Predictions'\nlegend   <- ('Season')\nsales_mod <- \nggplot(seasons, aes(x = ticketSales, \n                    y = pred,\n                    color = factor(season)))                  + \n    geom_point()                                              +\n   stat_smooth(method = 'lm', se = T)                      +\n    scale_color_manual(legend,values = palette)               +\n    scale_x_continuous(label = scales::comma)                 +\n    scale_y_continuous(label = scales::comma)                 +\n    xlab(x_label)                                             + \n    ylab(y_label)                                             + \n    ggtitle(title)                                            +\n    graphics_theme_1 "},{"path":"chapter3.html","id":"key-concepts-and-chapter-summary-2","chapter":"3 Exploring your data","heading":"3.8 Key concepts and chapter summary","text":"Exploring data fundamental component analysis. looking useful patterns might make sophisticated analysis possible. also looking things like missing data sparsity. Graphs typically easiest way accomplish goal. also best method communicating results.covered several key concepts chapter:important create consistent design language graphs. Pay attention colors.Certain graphs better representing specific data. often don’t need exotic plots convey information. ’ll use variations different themes.Summarizing data can done several ways. multiple tools can use reshape summarize data. can even implement methods relatively easily.R makes easy explore deploy wide variety statistical methods.interpreting communicating findings simplest explanation usually best. people care something. Let ask context.Think chapter reference. just went basic Business Intelligence analytics tools use data exploration. can also use Business Intelligence tools like Tableau add additional dimensions illustrations. want build custom websites R provides tools accomplish well. even use javascript python accomplish thing.","code":""},{"path":"chapter4.html","id":"chapter4","chapter":"4 Structuring analytics projects","heading":"4 Structuring analytics projects","text":"Framing projects tends overlooked component analytics exercises. projects often begin medias res. effort get result simply cobble data together begin throwing models . recommend creating basic wireframe project begin. ’ll help keep scope check force project-manage work. can loosley break analytics projects six parts may sub-components:Defining measurable goal hypothesisData collection managementModeling dataEvaluating resultsCommunicating resultsDeploying resultsThis can often iterative process. numerous questions answer tools need, whether internal resources, return--investment, long take, anybody use , done , results, etc. process may also useful just beginning work. seasoned analyst good idea begin project experience.basic premise always working one projects. create objectives based environmental considerations, build programs implement objectives, monitor performance. Honestly, isn’t difficult process tends overlooked. Larger projects can smother die weight considerations (including political considerations) aren’t considered. good plan can help keep breathing.","code":""},{"path":"chapter4.html","id":"defining-goals","chapter":"4 Structuring analytics projects","heading":"4.1 Defining goals","text":"Defining goals can easy difficult. ’ll likely need able translate product operational tactic can understood leveraged functional unit business. Sales team may say want sell tickets need leads. marketing team may say want understand return marketing investment. translate statements practical solution? request contains number questions.several management techniques look root causes. One aptly named “5 Whys” (Serrat 2017). subjects outside scope book. point seek understand something order devise plan. sounds obvious consider critical component defining goal part discovery process. Let’s explore example:ticket sales manager says “need leads can sell season tickets.” person asking? one several things things may evolved person incentivized. fact, always interesting think incentives guide behavior (see Freakonomics: rogue economist explores hidden side everything (Levitt 2005). sales manager really saying?want make phone calls. numbers calls causes us sell tickets.current leads closing rate need order hit goals.don’t enough leads keep guys working.put thoughts context motivates sales manager, simple statement actually insidious:ideas going make guys work harder.just want execute job. Just give want.shouldn’t viewed negative lens. jobs executing can get tedious. Look task way escape tedium help sales team think strategically. Help escape mire.Let’s assume sales manager believes number calls directly correlates number tickets sold. believe something can investigate? simplistic method investigate conclusion might look see number phone calls correlated sales particular way. formal informal methods use, easiest thing simply put data couple tables graphs.","code":""},{"path":"chapter4.html","id":"identifying-goals","chapter":"4 Structuring analytics projects","heading":"4.1.1 Identifying goals","text":"data set FOSBAAS::aggregated_crm_data three fields row represents interactions one customer. See ?FOSBAAS::aggregated_crm_data:repID corresponding specific repcalls represents number calls rep made specific personrevenue represents amount revenue generated specific customer.Table 4.1: Calls revenue repWhy salespeople effective? Perhaps someone called bought six expensive seats skews results. call numbers correlated revenue? cor function give us correlation coefficient.Overall, calls weakly correlated revenue (higher number better). ?\nFigure 4.1: Boxplot revenue rep\ncan see boxplots figure 4.1 average sales customer fairly equivalent. However, reps far fewer outliers. supposition differences seeing reps driven relatively small number sales. can get summary stats rep using psych::describe.(ag_sales_data\\$revenue,ag_sales_data\\$repID).\nFigure 4.2: Cumulative revenue rep\ncan verify without looking actual stats looking jumps line graphs. also want take look statistics see anything else can understand. Let’s answer couple questions:certain reps efficient phone calls?certain reps efficient customers?\nFigure 4.3: Call failures\nOverall, rep stands less efficient, although differences top bottom. Let’s take closer look distribution sales revenue. Instead density plot ’ll use summary statistics make couple comparisons highest lowest performers.Table 4.2: Aggregated sales data quantilesAt least 50% customers interacted resulted revenue. top 1% sales resulted $10,000.00 revenue 25% customers spend less $2,924.00. Let’s use describe.function isolate top performer bottom performer.average sale top rep 50% higher lowest performer. Additionally, median sale greater zero, means success often. Let’s take look much generated reps upper end spectrum.top performer generated nine times much revenue top 1% spenders. couple tendency successful, answer reason reps better. now answered one . established?number phone calls per rep weakly correlated revenue.reps marginally effective others terms efficiency.reps sell many high-end deals others.else look ?experienced reps better closing sales?origin sales? sales called-?Perhaps females tend better luck given demographic distribution purchasers.seasonality impact figures? Perhaps reps began working different times?professional resellers hidden sales?number calls rep makes weakly correlated sales, goal shouldn’t reps make phone calls. also shouldn’t add sales reps unless perhaps sales rep’s experience another factor impacting sales. two tactics tend common solutions hear, however technology rapidly changing mindset. Artificial Intelligence automating lead-warming sales might -sells gauging establishing interest.point section establish goals, need sort objective justification goals . Determine driving desired outcomes settling tactic. sounds obvious, isn’t. reasons political. Look might incentivizing behavior.","code":"\n#-----------------------------------------------------------------\n# Aggregated CRM data\n#-----------------------------------------------------------------\nag_sales_data <- FOSBAAS::aggregated_crm_data\n\nlibrary(dplyr)\nagg_calls <- \n  ag_sales_data                     %>% \n  group_by(repID)                   %>%\n  summarise(calls   = sum(call),\n            revenue = sum(revenue)) %>%\n  mutate(revByCall = revenue/calls) %>%\n  arrange(desc(revByCall))\n#-----------------------------------------------------------------\n# Correlation coefficient\n#-----------------------------------------------------------------\ncor(agg_calls$calls,agg_calls$revenue)\n#> [1] 0.2589937\n#-----------------------------------------------------------------\n# Box plots of revenue by sales rep\n#-----------------------------------------------------------------\nx_label <- 'Rep ID'                                             \ny_label <- 'Revenue by sale'                                            \ntitle   <- 'Revenue by sale by rep'\nsales_box <- \nggplot(ag_sales_data, aes(y=revenue, \n                          x=factor(repID)))                   +\n    geom_boxplot(fill = palette[1])                           +\n    scale_color_manual(values = palette)                      +\n    scale_y_continuous(label = scales::dollar)                +\n    xlab(x_label)                                             + \n    ylab(y_label)                                             + \n    ggtitle(title)                                            +\n    graphics_theme_1                                          +\n    theme(axis.text.x  = element_text(angle = 90, size = 8, \n                                      vjust = 0, \n                                      color = \"grey10\"))\n#-----------------------------------------------------------------\n# Cumulative revenue by rep by customer\n#-----------------------------------------------------------------\nag_sales_line <- \nag_sales_data %>% group_by(repID) %>%\n                  mutate(cumSum = cumsum(revenue),\n                         observation = seq(1:500))\n\nx_label <- 'Customer'                                             \ny_label <- 'Revenue'                                            \ntitle   <- 'Revenue generated per rep by customer'\n\nsales_line <- \nggplot(ag_sales_line, aes(y     = cumSum, \n                          x     = factor(observation),\n                          group = repID,\n                          color = repID))                     +\n    geom_line()                                               +\n    scale_color_manual(values = palette,guide = FALSE)        +\n    scale_y_continuous(label = scales::dollar)                +\n    xlab(x_label)                                             + \n    ylab(y_label)                                             + \n    ggtitle(title)                                            +\n    graphics_theme_1                                          +\n    theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        panel.grid.major  = element_line(colour = \"white\"),  \n        panel.grid.minor  = element_line(colour = \"grey80\"))\n#-----------------------------------------------------------------\n# Failed calls and customers by rep\n#-----------------------------------------------------------------\nfailures <- \n  ag_sales_data                      %>% \n  group_by(repID)                    %>%\n  filter(revenue == 0)               %>%\n  summarise(failedCalls = sum(call),\n            failedCusts = n())       %>%\n  tidyr::pivot_longer(!repID, \n                      names_to  = \"failure\", \n                      values_to = \"value\")\nx_label  <- ('\\n Rep')\ny_label  <- ('Count \\n')\ntitle    <- ('Failures by rep')\nbar_sales <- \n  ggplot2::ggplot(data     = failures, \n                  aes(y    = value, \n                      x    = reorder(repID,value,sum),\n                      fill = failure))                  +\n  geom_bar(stat = 'identity', position = 'dodge')       +\n  scale_fill_manual(values = palette, name = 'failure') +\n  scale_y_continuous(label = scales::comma)             +\n  xlab(x_label)                                         + \n  ylab(y_label)                                         + \n  ggtitle(title)                                        +\n  coord_flip()                                          +\n  graphics_theme_1                                      + \n  theme(legend.position   = \"bottom\")\n#-----------------------------------------------------------------\n# quantiles of sales\n#-----------------------------------------------------------------\nquants <- quantile(ag_sales_data$revenue,\n                   probs = c(.5,.75,.9,.95,.975,.99,1))\n#-----------------------------------------------------------------\n# description of sales\n#-----------------------------------------------------------------\nids <- c(\"0LK62LATB8E3\",\"AFA0Z9M2M4LQ\")\ndescriptives <- \npsych::describe.by(\n  ag_sales_data[which(ag_sales_data$repID %in% ids),]$revenue,\n  ag_sales_data[which(ag_sales_data$repID %in% ids),]$repID)\n#-----------------------------------------------------------------\n# description of sales\n#-----------------------------------------------------------------\ndescriptives$`0LK62LATB8E3`[c(3,4,5,6,9)]\n#>      mean      sd median trimmed      max\n#> X1 1400.9 2966.49      0 1036.41 54039.92\n#-----------------------------------------------------------------\n# description of sales\n#-----------------------------------------------------------------\ndescriptives$AFA0Z9M2M4LQ[c(3,4,5,6,9)]\n#>       mean     sd median trimmed      max\n#> X1 2404.23 6747.1 963.96 1417.14 58428.16\n#-----------------------------------------------------------------\n# quantiles of sales\n#-----------------------------------------------------------------\n\nag_sales_data                                          %>% \n  filter(repID %in% c('0LK62LATB8E3','AFA0Z9M2M4LQ'),\n         revenue >= quants[6])                         %>%\n  group_by(repID)                                      %>%\n  summarise(highRevenue  = sum(revenue),\n            countRevenue = n())\n#> # A tibble: 2 × 3\n#>   repID        highRevenue countRevenue\n#>   <chr>              <dbl>        <int>\n#> 1 0LK62LATB8E3      54040.            1\n#> 2 AFA0Z9M2M4LQ     453204.            9"},{"path":"chapter4.html","id":"collecting-data","chapter":"4 Structuring analytics projects","heading":"4.2 Collecting data","text":"lot understand data collection always forced confront question data need acquire . data much easier acquire. instance, transactional data likely abound. hopefully access several years ticketing CRM data. However, may problems formatting consistency. place data collection headings:Transaction data ticketing, CRM, internal systemsTransaction data external systems (perhaps agency agreement something like concessions)Third party data. might include data vendors Acxiom.Public data. includes sources Census data information legal records.Internal research data. category includes surveys initiatives competitive intelligence.also good time talk much data may actually lacking. professional sports team isn’t Google, Facebook, Amazon. companies able build analytics capabilities around data far sophisticated way possible along vertical. partner companies another third party like take advantage capabilities.Furthermore, tech companies walled-gardens. don’t like share. ’ll contend fact. also need think critically using capabilities. spend money SEM30 ’ll want ask . market work? multiple channels consumers can exploit purchase tickets? easy skew attribution. knew going type “Game Hens tickets” search bar makes easy attribute sale activity. question ask attribute sale search-engine spend.Internal sources survey results, transaction data, interaction data phone-calls, loyalty programs offer degree reliability. However, depending geography legal environment may allow leverage data. fact, numerous restrictions leveraging, storing, sharing data :California Consumer Privacy Act 31The Illinois biometric information privacy Act 32The Can-Spam Act 33The -Call list 34As long live United States, laws might impact able leverage first third party data. list even comprehensive. GDRP35 (European privacy law) far reaching influence able United States. future commerce undoubtedly make arena challenging navigate use biometrics becomes prolific.Additionally, data often accurate. third party data-brokers get data? Sometimes simply use bank names determine person male female, African American vs. Asian: Washington = African American, Wang = Asian, Lopez = LatinX. LatinX represents special problem culture notion race. Third party brokers (Axiom) source data multiple sources model components . Unless dealing massive datasets (common sports), working third party data can difficult frustrating.Internal research data may required specific research instruments :IDI (-depth interviews)Satisfaction surveysConjoint studiesFocus groupsBrand-tracking studiesSocial listeningThese sources can become stale rather quickly. can also expensive conduct often require specialized software skill sets.Operational data may ready-made method data collection. Let’s say Executive wants understand relationship line-length concession stands relative attendance. get ? cameras installed .. tracks line length? ? ’ll collect manually leveraging rubric can easily explained executed front-line staff. might data collection plan look like? determining collect data useful think problem inside-. ’ll discuss concept depth chapter 9.main gist need focus couple things:questions really trying answer?data need answer questions?process can sprawl concerned consistency. Public sources data also abound, don’t typically offer granularity needed useful anything besides supplementary reporting. example census data. extensive APIs access data, results may practically useful outside long-term planning exercises.Data collection might also involve competitive intelligence. example going venues monitoring prices operational tactics. Competitive intelligence may also take form monitoring industries operate similar spaces. instance, can learn loyalty programs company like Starbucks? might learn aren’t appropriate .touched high-level concepts . Just understand data collection require combination .T. skill sets, research knowledge, critical thinking exercises. Getting data right spot take majority time. won’t able overlook .","code":""},{"path":"chapter4.html","id":"modeling-the-data","chapter":"4 Structuring analytics projects","heading":"4.3 Modeling the data","text":"Modeling data fun part working analytics. aren’t talking modeling data database sense, although structuring data component process. also aren’t going actually modeling data . Instead going speak tools need. lower levels leveraging least two languages (SQL programming language). Increasingly modeling component data becoming easiest part tech giants seek space. don’t know large advantages using one tool . instance, advantage using Google’s tools Microsoft? advantage using python R? answers probably vary. cases yes cases . time believe ’ll see reliance code wane.Differences R Python may make distinction tool plan using. especially true look deploy results. R Python highly extensible. libraries almost everything need write implementations likely minimal. need, probably better use another language C++. However, C++ significantly difficult many languages might encounter. (Practically speaking, R C++ since written )Many analytics projects Python use relatively small sets libraries. nice feature code one model look almost exactly like code another. won’t always true R consider advantage Python users. Let’s look example.code hierarchically cluster data set python might look something like following:Every method available module sklearn (Pedregosa et al. 2011) look like example. makes easy intuitive run several algorithms. demonstration, wanted cluster Kmeans algorithm python, code might look like .almost identical. Let’s compare code R code work tool. code hierarchically cluster data R follows.languages begin importing capabilities libraries. next code chunk uses kmeans algorithm. housed stats library works slightly differently hierarchical algorithm. find R tends feel procedural python.code chunk looks similar, isn’t ussually case. R, like Python, thousands packages. However, python tends one correct way everything. ’s Pythonic way. R wild west. Different algorithms often different authors approach writing code dealing objects differently. Luckily, several developers R attempted solve problem wrapper packages work API libraries add additional functionality.can even run python R using reticulate (Ushey, Allaire, Tang 2022) package. Packages caret (Kuhn 2022), mlr3 (Lang et al. 2022), tidymodels (Kuhn Wickham 2022) attempted create standard API many R functions. means similar approach can taken terms writing understanding code many functions. trade-often speed. Despite tradeoff, think incredibly useful take advantage one frameworks. make much easier cover steps modeling process. R Python learned one another. end day, pick tool get good . prefer R, admire python. place really isn’t difficult switch two data analysis.end section, modeling process follows four discrete (often iterative) steps. ’ll refer parts process chapter 5.Evaluating dataPreparing dataProcessing dataValidating outputYou’ll get better process gain experience become intuitive. reality, projects tend repetitive can encounter several problems modeling data. Every problem doesn’t always manifest, problems always arise. ’ll see solve problems forthcoming chapters.","code":"#-----------------------------------------------------------------\n# Clustering algorithm applied in python\n#-----------------------------------------------------------------\nfrom sklearn.cluster import AgglomerativeClustering\ndata = data.sample(n=1500)\ncluster = AgglomerativeClustering(n_clusters=6, \naffinity='euclidean', linkage='ward')  \ncl = py.DataFrame(cluster.fit_predict(data))#-----------------------------------------------------------------\n# kmeans algorithm applied in python\n#-----------------------------------------------------------------\nfrom sklearn.cluster import KMeans\ndata = data.sample(n=1500)\ncluster = KMeans(n_clusters=6, random_state=0)\ncl = py.DataFrame(cluster.fit_predict(data))\n#-----------------------------------------------------------------\n# Hierarchical clustering algorithm applied in R\n#-----------------------------------------------------------------\nlibrary(stats)\nlibrary(cluster)\ndata         <- sample(data, 1500)\nmod_data     <- cluster::daisy(data)\ncl           <- stats::hclust(mod_data, method = \"ward.D\")\ncuts         <- cutree(cl, k = 6)\ndata$cluster <- cuts\n#-----------------------------------------------------------------\n# kmeans algorithm applied in R\n#-----------------------------------------------------------------\nlibrary(stats)\ndata         <- sample(data, 1500)\ncl           <- stats::kmeans(data, centers = 6)\ndata$cluster <- cl$cluster"},{"path":"chapter4.html","id":"evaldata","chapter":"4 Structuring analytics projects","heading":"4.3.1 Evaluating your data","text":"refers understanding looking . data structured trying ? can occasionally one several built-features R. times, ’ll put thinking cap. Typical questions might include:questions trying answer data?attempted solve problem ? results?deeply held beliefs solution might ?data formatted?sparse data?current data? matter stale?data’s provenance? Can trust ?much missing missing? can important.Can use data?need data?data categorical, ordinal, numerical, mixed?dealing differences units scale?plan don’t find anything likely possibility?Taking time get familiar data structure critical getting accurate results. Focus question hand. example might current sales best job predicting future sales times year? might work better? Frame problem statement carefully.Furthermore, deeply held beliefs answer might may issues answer antithetical belief. probably reasons belief held. Honestly, time people formed belief correct. Just use extra discretion determine another solution merit.","code":""},{"path":"chapter4.html","id":"preparing-your-data","chapter":"4 Structuring analytics projects","heading":"4.3.2 Preparing your data","text":"Preparing data generally take time. ’ll go process detail chapter 5. missing data might consider imputation. dealing mixed data sets ’ll consider process . Ordinal data might require approach problem specific way. example might Holt-Winters 36 method estimating time-series data. particular method may also applicable underling structure within data. preparing data also critical think two common questions face.deal missing data?methods plan applying data?Missingness huge problem. NAs, NANs, inf? sparse OK? answers may vary. Additionally, systematic reason data missing results may invalid. Take warning, dealing missing data frustrating work.prepare data always ask something repeated? recommend prepping data code. reason, always need repeat projects. Documenting ETL work crucial. tend start SQL push point doesn’t make sense. don’t duplicate information SQL. instance, won’t dummy-code data set SQL. leave analysis phase. judgement call. think can efficiently. Certain methods Latent Class Regression accept discrete data transform numerical data discrete data can impact results.","code":""},{"path":"chapter4.html","id":"modelingdata","chapter":"4 Structuring analytics projects","heading":"4.3.2.1 Selecting the appropriate technique for analyzing your data","text":"Gone days expert use sound judgment select appropriate method processing data. Hardware software advances removed resource constraint delayed analytics revolution began early 2000s. future now. past researcher may purchase time main-frame37 (may even know term means) therefore needed good idea technique want leverage solve problem. Analysts modern times can afford lazy. modern analyst really considerations terms need approach problem. See figure 4.4.\nFigure 4.4: Choosing right technique\nexaggerating little . Many techniques require high degree rigor validate results. However, much easier analysts twenty years ago. surplus methods become problem . Additionally, easy get rut. instance, like deep learning going use every problem face. like use hammer use fix everything. ’s variation something already discussed thought might warrant repeated. Sometimes wrench make life easier even hammer work.","code":""},{"path":"chapter4.html","id":"processing-your-data","chapter":"4 Structuring analytics projects","heading":"4.3.3 Processing your data","text":"Begin looking simplest solution first. Stingy, parsimonious models tend forgiving friendly. Always look simplest solution. especially important interpretability, also durability. Simpler solutions tend elegant easier deal . Always approach processing intelligent, procedural way. Let’s illustrate brief example:“ticket sales manager wants understand likely candidates system purchase season tickets. also want understand likely spend upgrade seats.”approach problem gathered cleaned data? sounds like classification problem related lead scoring. OLS regression tends best place start gold standard estimating numerical values, Logistic regression tends best place begin trying estimate classes. even special forms “multinominal logistic regression” (Ripley 2022) can estimate several classes, just binary classes.time ’ll learn specific techniques tend work better specific problems. Techniques may also provide almost identical results. ’ve found random forests gradient boosting tend provide similar results. less interpretable, also don’t require much rigor regression. Regression forces think kinds things like Heteroskedacity, Multicolinearity, Autocorrelation, etc. always like use multiple techniques compare results. Getting support conclusion always good thing. Unless dealing huge amounts data, likely won’t much penalty taking route. tools make easy.One thing keep mind results deployed. Regression represented mathmatical formula. means results can computed almost instantly model won’t change. Deep learning algorithms can adaptive can produce unexpected results aren’t monitored. lots examples chat-bots developing racist 38 behaviors. Stick simplest solution unless need higher degrees precision deployment requires specific approach. Don’t fall rut “problem, let’s bludgeon Deep Learning.”","code":""},{"path":"chapter4.html","id":"evaluating-your-results","chapter":"4 Structuring analytics projects","heading":"4.4 Evaluating your results","text":"Evaluating results can best worst part project. discover solution? Regardless whether solved problem, stage process give great information. finding solution may tell isn’t solution data. may also indicate problem nuanced needs sophisticated thought. Let’s examine example.marketing manager wants understand outreach efforts impact season ticket holder retention.collect data CRM system number calls emails, attendance regarding non-sport events, etc. use data attempt estimate probability season ticket holders renew tickets. Unfortunately, find outreach appears irrelevant (results insignificant model). also discover ticket usage tenure variables appear significant (ignoring macro factors economy team performance). mean shouldn’t call email clients? course , mean may need closely examine incentive client representatives. also need careful communicate type result. ’ll discuss little next section.also technical component evaluation. Different models can compared one another ANOVA 39 BIC 40, AIC 41. estimate efficacy call campaign lift chart represented ROC 42 curve. OLS regression makes use F statistic, P-values, R-squared values. Results can cross-validated compared control group holdout sample. Logistic regression models concerned Accuracy, precision, sensitivity, specificity, overdispersion. many diagnostic features need evaluated ensure model want . Ultimately, ’ll always want compare results control group make iterative improvements. /B testing often best way accomplish goal something put production.Evaluating results exercise requires technical rigor domain knowledge. don’t find solution, always learn something. Think carefully approach component modeling data. second time-consuming component project biggest influence whether project success failure.","code":""},{"path":"chapter4.html","id":"communicating-the-results","chapter":"4 Structuring analytics projects","heading":"4.5 Communicating the results","text":"Getting research hands Executives Managers need understand problem potential solution difficult sounds. Communicating results project may require esoteric techniques art. Every large management consulting firm frameworks around problem solving extension framing results. can often borrow folks. Keep mind results can complex involve difficult concepts isn’t always easy ELI5 43. credibility grows organization matures, process becomes easier. colleagues suffer confirmation bias, exorcising bias may impossible. incredibly important understand.“often use reasoning find truth invent arguments support deep intuitive beliefs.”— Johnathan Haidt, “Happiness Hypothesis”quote Haidt (Haidt 2006) hammers home concept confirmation bias. results case may may winning argument. can frustrating phenomenon. also merit. much experience problem? strong beliefs solution? Understanding someone might interpret solution (especially solution challenges preconceived notions) must given thought. someone threatened solution, put thought present . Even think words use perceived recipients message. someone perceive results threatening insulting? happens gas-lighted? Always take step back examine analysis fair. really made difference another way? something doesn’t really matter, don’t worry communicating .Additionally, best practices can follow. , higher someones title , words bullet points include, lower title technical nature audience , graphs explanations include:Executives: Bullet points refer solution. Keep terse. Get point.Managers: Include specific explanations pertaining domain knowledge.Resist urge demonstrate domain knowledge. Typically, nobody cares something. don’t care Support Vectors, Eigenvalues, Deep Learning. care results cases, got (especially contradicts beliefs). Take collaborative approach demonstrating findings ask questions. ’ll gain credibility long term better success solutions implemented.R also provides great features helping communicate results findings. R Markdown (Allaire et al. 2020) makes easy create publishable documents include code, graphics, mathematical formulas. book written using . discussed Shiny (Chang et al. 2022) earlier. tools make R great choice building documents dynamic web pages.","code":""},{"path":"chapter4.html","id":"deploying-the-results","chapter":"4 Structuring analytics projects","heading":"4.6 Deploying the results","text":"Deployment mean different things. One refers people one refers systems.Communicating results (subject last section)Operationalizing output (automation component discussed figure 1.1.second part deploying results refers automating process putting solutions production. multiple tools make possible. Although tends languages advantage R. Compiled languages C++ can much faster R full-featured sense. Although exceptions. SQL Server supported integration R 44 Python since 2016, although isn’t clear widespread use become. Google, Amazon, Microsoft building analytics frameworks cloud-based DBMSs. allow turn-computing power make models run much quickly assuming can take advantage technique multi-threading (R natively support). Thinking make something efficient probably already done . stage, R may used protoyping something else actual deployment.valuable?Forecasts ticket sales made daily minute update report.Customer’s lead scores dynamically updated throughout year based certain conditions.predicted vs. actual vs. expected results marketing campaign constantly evaluated ROI.Ticket Prices automatically updated tight interval optimizing sales revenue.many applications. Additionally, always write wrapper (windows) within system runs script using simple batch script (notepad file saved .bat). following example runs R script called YourRScript.R.can write similar bash script linux box. dealing complicated systems, may need rely developers automate process. context available skill sets tell direction take.","code":"\n#-----------------------------------------------------------------\n# Batch script for automation\n#-----------------------------------------------------------------\n# REM This command will call an R Script from a program or \n# scheduling tool\n\n# \"C:\\Program Files\\R\\R-Version\\bin\\x64\\R.exe\" CMD BATCH \n#  --vanilla --slave \"\"C:\\locationOfScript\\YourRScript.R\""},{"path":"chapter4.html","id":"key-concepts-and-chapter-summary-3","chapter":"4 Structuring analytics projects","heading":"4.7 Key concepts and chapter summary","text":"Putting thought structure problem key component process project management. becomes important projects grow complexity require larger numbers people managed. project can benefit systematic process consisting steps. typically break steps six components:Defining measurable goal hypothesisData collection managementModeling dataEvaluating resultsCommunicating resultsDeploying resultsYou’ll work steps whether mean . simply natural order projects tend take.Defining goal tends hardest part analytics project. also important. Even aren’t putting together formal hypothesis test, still concerned scope creep. Spend time step get right.Data collection management typically time-consuming component one projects. Getting quality data can difficult even impossible. collect data , systematic . Document reproducibility.Modeling data fun part analytics project. lots options, start simplest method first. deep-learning prominent recent years, isn’t always best tool use. Interpretibility can important . Put thought using one technique another.Evaluating results quantitative qualitative exercise. never just data. Think output context exercise give good smell-test. results logical? Can take action ? relevant goal?“Simplicity ultimate sophistication.” quote attributed Leonardo Da Vinci. God knows actually came . Keep mind. Use bullet points. Keep results simple understand.Deploying results can mean multiple things. lots tools available put results action.","code":""},{"path":"chapter5.html","id":"chapter5","chapter":"5 Segmentation","heading":"5 Segmentation","text":"Appropriately segmenting data difficult. difficult. usually trivial find patterns business data, patterns may useful. Often times patterns may represent market structure exploitable feature. Additionally, unlikely access rich data massive data platforms Facebook, Google, even league . means limited respects. Without partnerships can’t even approach concept developing insight data way many companies. ’ll cover concepts chapter two parts:Preparing data analysisAnalyzing interpreting dataThis spot book shifts tacit examples. going walk fairly complete examples include code necessary, explanations output, interpretation deployment. ’ll cover techniques chapter based simulated data chapter 2. several things consider starting project like segmenting data set. ’ll want ask questions:trying accomplish?output consumed?consume ?incremental value project?Number four important limited time. another area focusing? Segmentation may leverage many different machine-learning techniques. chapter cover techniques found useful. Keep mind, aren’t guaranteed find anything useful. ? reasons:data badYou don’t enough dataYour technique inappropriateYou framed problem incorrectlyThere isn’t anything thereIf working consultant, going find something. find something. working club keep simple sentence mind:worst thing can happen data-science project think found something deploy solution isn’t valid.case may actively harming organization, also reducing credibility analytics. ’s OK can’t find something. First, harm. Scale can also problem. much think incremental value work worth? models built around sort consumer behavior going maximize ticket revenue potential?chapter cover building segmentation scheme raw data. projects can become complex. ’ll also look different ways accomplishing goal. Perhaps just need simple way differentiate two groups. Maybe need full-scale psychographic profiles. ’ll cover basics products delivered. ’ll come away understanding root segmentation scheme produced practice. cover portion variety, ’ll able identify consultant put together one frameworks build .","code":""},{"path":"chapter5.html","id":"building-a-segmentation-scheme","chapter":"5 Segmentation","heading":"5.1 Building a segmentation scheme","text":"example going based demographic data qualitative data taken survey. goal leverage simple scheme reasonable job explaining purchases tickets purchase . groups based demographics (discoverability), actually looking exploitable differences behavior. project successful, need follow four rules:Understand difference segments market structure. Groups differ behavior may segments.must able distinguish segments appeal selectively product channel.Validating segments require constant experimentation. going willing capable experimenting data.Sales Marketing leadership must support remain involved segmentation.Sometimes segmentation schemes can diffuse. instance people--necks. covers pretty large swath humans. segmentation schemes can irrelevant problem. selling dog-food, women red hair may relevant segment. can also diffuse irrelevant. example might people brown eyes.basic goal find groups fans potential customers differ observable ways associated communicate, content preference, marketing response. Generally, looking answer question: consumer want?. understanding differences, hope make better strategic marketing decisions regarding pricing, market positioning,\nsales sponsorship opportunities, brand development. analysis two components:demographic segmentation derived data collected third party.set behavioral archetypes derived survey responses.demographic segments overlaid customer’s behavioral archetypes paint picture demographic broadly behaves. segments , specific . Abstracting fewer segments means edges segments tend fuzzy. Additionally, durability segments tested ensure validity.segmentation project successful able increase probability making correct decisions. isn’t magic bullet, answer questions certainty. sports club special considerations.don’t unlimited scale potentialYour sample sizes going relatively smallYou can’t continuously deploy tests way TicTok . projects tend ad hoc nature. believe useful marketers help understand market structure within fan base. allows small tweaks help compound effectiveness multiple concurrent campaigns.Ultimately, success judged basis creating testable hypotheses using information alter behavior one segments. also define success narrowly:create capability accurately measure return marketing spend?able increase sales revenue increasing conversion rates?able better determine product offerings?able get relevent results surveys research projects?Regardless define success, segmentation always considered work progress. Segmentation remain static successful. maximize opportunity, must understand change behavior best suit customers expectations, channel capability, purchase behavior.","code":""},{"path":"chapter5.html","id":"demographic-segmentation","chapter":"5 Segmentation","heading":"5.2 Demographic segmentation","text":"Demographic segmentation attractive many reasons. However, discoverability main reason. can useful certain circumstances, use also fraught peril. Steve Koonin (CEO Atlanta Hawks) famously stated Alpharetta Unicorn couldn’t save franchise (referring Atlanta Hawks). “Alpharetta Unicorn” :“55-year-old guy ’s going drive hour Alpharetta city three buddies go Hawks game. doesn’t exist. music, kiss cam, cheerleaders, shooting free car, bobbleheads … nothing going change .”— Steve KooninAlpharetta suburb Atlanta contrasts much Fulton County GA citizens (2021) tend less ethnically diverse (mostly white), right-leaning politically (mostly Republican), tend homogeneously affluent. outlying suburb, also geographically isolated.45 Steve’s response loaded several reasons. actually saying Hawks brand congruent marketing tactics product. may also taking shot another -market team. Either way, segment may may exist. Perhaps just irrelevant specific situation statement political stunt justify direction decided take sales marketing positions.Let’s begin taking look relatively simple data set. undoubtedly confronted data much complex career, context sports data likely fairly straightforward simple. data sample can found FOSBAAS package entitled demographic_data.Let’s begin taking look column names.Table 5.1: structure demographic data setThis table 200,000 rows 12 columns corresponding standard demographic features. Latitude longitude already processed distance metric. ’ll pare data information think might useful processing.","code":"\n#-----------------------------------------------------------------\n# Read data to use for segmentation\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\ndemo_data  <- FOSBAAS::demographic_data\n#-----------------------------------------------------------------\n# Take a look at the structure of the data\n#-----------------------------------------------------------------\nstr(demo_data)"},{"path":"chapter5.html","id":"exploring-the-demographic-data-set","chapter":"5 Segmentation","heading":"5.2.1 Exploring the demographic data set","text":"can sample variables think might useful segmentation using select function dplyr.Taking look first lines data can see mixture discrete data ethnicity continuous data age. relevant process data algorithm might choose deploy.(#tab:seg_dataset_str2)Demograhic datahhIncome modeled indicator. common run type variable purchased data sets. indexed value discrete beginning end. ’d need data dictionary explain numbers actually mean. case, larger number, higher income. Let’s start taking look numerical data sets. ’ll use graphics paradigm illustrated chapter 3.","code":"\n#-----------------------------------------------------------------\n# subset the data\n#-----------------------------------------------------------------\nlibrary(dplyr)\ndemo_data <- demo_data %>% select(custID,ethnicity,age,\n                                  maritalStatus,children,\n                                  hhIncome,distance,gender)"},{"path":"chapter5.html","id":"age-of-our-customers","chapter":"5 Segmentation","heading":"5.2.1.1 Age of our customers","text":"’ll begin exploratory data analysis. Let’s build histogram categorical variable gender differentiate two groups.distribution clearly bimodal46. think describing data already know average age doesn’t make much sense. included gender see obvious differences men women sample. Let’s take closer look using describe function psych package(Revelle 2022).:Table 5.2: Males vs. FemalesWe can tell distribution skews older (mean 42.82 vs. median 46). Let’s also take look see significant differences women sample.Table 5.3: Descriptive statisticsAs can see, numbers almost identical. Men women tend age.","code":"\n#-----------------------------------------------------------------\n# Histogram of the age of fans\n#-----------------------------------------------------------------\nx_label  <- ('\\n Age')\ny_label  <- ('Count \\n')\ntitle    <- ('Distribution of age (demographic)')\nhist_age <- \n  ggplot2::ggplot(data=demo_data,aes(x=age,fill=gender))  +\n  geom_histogram(binwidth = 2)                            +\n  scale_fill_manual(values = palette)                     +\n  geom_rug(color = 'coral')                               +\n  scale_x_continuous(label = scales::comma)               +\n  scale_y_continuous(label = scales::comma)               +\n  xlab(x_label)                                           + \n  ylab(y_label)                                           + \n  ggtitle(title)                                          +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Get summary statistics\n#-----------------------------------------------------------------\nlibrary(psych)\ndescript   <- psych::describe(demo_data$age)\ndescriptMF <- psych::describeBy(demo_data$age,demo_data$gender)"},{"path":"chapter5.html","id":"distance-from-facility","chapter":"5 Segmentation","heading":"5.2.1.2 Distance from facility","text":"Distance another interesting numerical data set. Distance especially important depending thinking product. likely purchase season ticket? Someone five miles venue five hundred? answer depends, general rule, season ticket holders tend live closer venues.distribution also bimodal. Can speculate ? Perhaps sample season ticket holders percentage group. Perhaps another major city around two-hundred miles away? Patterns like show looking tickets sales. simply easier people close-purchase utilize tickets. never imagine ballpark stadium placed rural area. Distance simply going significant variable models. careful treat . measuring distance purchasers? individual ticket? differentiating fans companies?Taking look data can see skew numbers. median distance 66.47 miles mean distance 110 miles. sorts datasets can problematic regression, may work favor terms identifying market structure. can also useful thinking different marketing tactics OOH47.Table 5.4: Descriptive statistics distanceThe distribution distance much different age. median significantly smaller mean indicates left skew distribution.","code":"\n#-----------------------------------------------------------------\n# Distance from our facility\n#-----------------------------------------------------------------\nlibrary(ggplot2)\nx_label   <- ('\\n Distance')\ny_label   <- ('Count \\n')\ntitle     <- ('Distribution of distance (demographic)')\nhist_dist <- \n  ggplot2::ggplot(data=demo_data,aes(x=distance))            +\n  geom_histogram(binwidth = 2,fill='dodgerblue')             +\n  geom_rug(color = 'coral')                                  +\n  scale_x_continuous(label = scales::comma)                  +\n  scale_y_continuous(label = scales::comma)                  +\n  xlab(x_label)                                              + \n  ylab(y_label)                                              + \n  ggtitle(title)                                             +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Use the psych package to generate summary statistics\n#-----------------------------------------------------------------\ndescript <- psych::describe(demo_data$distance)"},{"path":"chapter5.html","id":"household-income","chapter":"5 Segmentation","heading":"5.2.1.3 Household income","text":"Household income another continuous data set. also modeled, means likely scaled fashion. distribution multimodal several peaks.analysis important. Ultimately, looking differences behavior can attribute different features related discoverable group individuals. ’ll touch begin process data quantitative analysis. don’t currently reason believe difference person behaves based income. However, can probably make guesses may may merit relation data.Table 5.5: Descriptive statistics incomeThere clearly clusters groups data. Think critically one. higher household income mean disposable income? cases yes, cases, . implicit signs disposable income use? Perhaps luxury cars pool ownership. point, people may tend spend income. someone makes money tend buy nicer cars bigger houses. asset investments houses may increase wealth, may indicate disposable income. ever someone’s fabulous home noticed cheap furniture ?","code":"\n#-----------------------------------------------------------------\n# Histogram of income\n#-----------------------------------------------------------------\nx_label  <- ('\\n Household Income')\ny_label  <- ('Count \\n')\ntitle    <- ('Distribution of Income (demographic)')\nhist_income <- \n  ggplot2::ggplot(data=demo_data,aes(x=hhIncome)) +\n  geom_histogram(binwidth = 2,fill='dodgerblue')  +\n  geom_rug(color = 'coral')                       +\n  scale_x_continuous(label = scales::comma)       +\n  scale_y_continuous(label = scales::comma)       +\n  xlab(x_label)                                   + \n  ylab(y_label)                                   + \n  ggtitle(title)                                  +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# summary statistics of hhincome\n#-----------------------------------------------------------------\ndescription <- psych::describe(demo_data$hhIncome)"},{"path":"chapter5.html","id":"consololidating-the-numerical-analysis","chapter":"5 Segmentation","heading":"5.2.1.4 Consololidating the numerical analysis","text":"can get numbers little manipulation. case aren’t looking anomalous data NAs missing data.Table 5.6: Consolidated statisticsAs best practice, don’t things multiple times. can think way like view data, somebody else probably thought solved problem.","code":"\n#-----------------------------------------------------------------\n# Generate summary stats of multiple values\n#-----------------------------------------------------------------\nnumerical_cols <- names(dplyr::select_if(demo_data, is.numeric))\ndescription    <- psych::describe(demo_data[numerical_cols])"},{"path":"chapter5.html","id":"categorical-variables","chapter":"5 Segmentation","heading":"5.2.1.5 Categorical variables","text":"myriad ways analyze categorical data. ’ll go methods describe seeing. can create proportion table prop.table function take look large discrete groups .case, white fans represent 80% sample, African Americans represent 10%, Hispanics Asians representing another 10%.Table 5.7: Ethnicity proportionsAnalyzing ethnicity fraught difficulty. mean? identify differentiate Hispanics? seems nonsensical. Folks can get wrapped-around--axle . Sponsors, media, internal HR staffs, local governments, everyone gets concerned ethnicity means doesn’t mean terms attendance. loaded word. analyst can make worse. Let’s consider joke heard. don’t know attribute :algorithm walks bar bartender says “?” algorithm looks around says “’ll everyone else .”likely going find exactly expect find . , ethnicity matter. Perhaps doesn’t point estimate. Longitudinally can think applications. meaningful ticket buyers tend live nearby ticket buyers identify certain ethnicity? mean five ten years area grows?also typically useful look crosstabs data. Let’s compare African Americans white fans.Table 5.8: Ethnicity proportionsWhen comparing groups can see numbers fairly close. can also make generalizations.Many fans tend white. congruent population patterns?Wealth factors seem similarWhites tend slightly younger live closer parkWe can leverage sort analysis see -indexing within populations (assuming merit). factors explain purchases? Now done cursory analysis data can begin processing modeling. Never overlook analyzing factor data set individually. Getting understanding data structure extremely useful work rest analysis.","code":"\n#-----------------------------------------------------------------\n# build proportion table for categorical data\n#-----------------------------------------------------------------\ndescription      <- table(demo_data$ethnicity)\ndescription_prop <- prop.table(description)\n#-----------------------------------------------------------------\n# use describeBy to generate statistics tabulated by a factor\n#-----------------------------------------------------------------\nnumerical_cols <- names(dplyr::select_if(demo_data, is.numeric))\ndescription    <- psych::describeBy(demo_data[numerical_cols], \n                                    demo_data$ethnicity)\ndeth <- rbind(description$aa[,c(2,3,4,5,6)], \n              description$w[,c(2,3,4,5,6)])"},{"path":"chapter5.html","id":"preparing-the-data-for-modeling","chapter":"5 Segmentation","heading":"5.2.2 Preparing the data for modeling","text":"Unfortunately, ’ll spend lot time prepping data can operate . ’ll talk major problems face can . several confronted frequently:Missing data (NA,NAN,Inf)Sparse dataWe’ll briefly touch imputation give thoughts ’s appropriate use. enormous subject far outside scope book.","code":""},{"path":"chapter5.html","id":"dealing-with-missing-data","chapter":"5 Segmentation","heading":"5.2.3 Dealing with missing data","text":"Missing data almost always problem different ways can handled. One consideration data missing. reasons data missing can bias results analysis. frequent problem surveys. instance, perhaps survey long people tend finish portion . data “missing completely random.” technical term ways deal , don’t worry . Just aware issues.Imputing data (dealing missing data) can extremely complex task. First, need understand much data missing. several packages deal missing data. can also write simple function help identify common culprits. Let’s take look data see missing anything.Table 5.9: Missing valuesNothing missing! isn’t surprising since created data set. Let’s introduce missing data can explore important subject. randomly assign cells NA based random number generated cell dataframe.function randomly makes 2% fields NA.Now can apply simple function determine missing values. looks like successful.Table 5.10: New missing valuesNow need something missing values. small amount data missing may OK simply delete rows. much small amount? don’t know. Let’s look rows determine rows missing values. ’ll use simple loop count missing fields row column. different ways .looks like four-thousand entries missing attribute. Since know overlap may great idea simply delete rows. many rows missing data? can use complete cases function determine many rows complete.Table 5.11: Complete casesAbout thirty thousand rows missing data. enough build suitable model. know? hard fast rules. sampled 10,000 rows data find distributions field tend look full data set. really looking . case reach arbitrarily small number (perhaps 400-700 people). people call law large numbers. really just expression. law. However, might think data missing. data typically better, get penalized process time.let’s try couple different techniques impute missing data. ethnicity know vast majority (80%) known values “w.” can replace NAs “w” assume close 80% accurate. easy accomplish following code.Age little trickier. know distribution multi-modal. Taking common value might reasonable. However, also simply take mean median. Additionally, factors within data set children income may useful predicting age degree accuracy. technique best? don’t know. However, represents two percent entries. choose suboptimal path, damage minimal. case, going lazy simply use mean age.using averages interesting looked entire data set find know individuals average age. known issue designing everything airplane cockpits gloves. also simulate distribution randomly assign values distribution. However, technique also issues. , just aware think critically . can reasonably justify approach, probably good enough.know 58% fans married. approach one? pattern marriage random? randomly pick one assuming individual married 58% time accurate simply saying everyone married? can’t really tell . pull Bayes’ theorem come kinds ways estimate likelihood person married. Since dealing 2% data, going go simple imputation random guesses discrete probability.now imputed three variables can see tedious exercise can . several packages can help automate task.Table 5.12: Customer data setWe can approach children gender much way categorical data.Let’s take slightly different approach rest data. Customer Ids missing, going delete rows. aren’t many, without able link segment constituents individual less utility. want able compare demographic attributes actual behaviors.","code":"\n#-----------------------------------------------------------------\n# Write a function to generate stats on missing data\n#-----------------------------------------------------------------\nf_missing_data <- function(dF){\n\nna  <- sum(apply(dF,2,function(x) is.na(x)))\nnan <- sum(apply(dF,2,function(x) is.nan(x)))\ninf <- sum(apply(dF,2,function(x) is.infinite(x)))\n\nmissing        <- as.data.frame(rbind(na,nan,inf))\nnames(missing) <- 'Values'\nreturn(missing)\n}\n\nmissing <- f_missing_data(demo_data)\n#-----------------------------------------------------------------\n# Randomly make some values in dataframe NA\n#-----------------------------------------------------------------\nset.seed(755)\ndemo_data[] <- \napply(demo_data,1:2, function(x) ifelse(runif(1,0,1) > .98,NA,x))\n#-----------------------------------------------------------------\n# Print the number of missing values for each column\n#-----------------------------------------------------------------\nfor(i in names(demo_data)){\n  print(paste(sum(is.na(demo_data[,i])),names(demo_data[i])))\n}\n#> [1] \"3973 custID\"\n#> [1] \"3997 ethnicity\"\n#> [1] \"4042 age\"\n#> [1] \"4111 maritalStatus\"\n#> [1] \"4021 children\"\n#> [1] \"3993 hhIncome\"\n#> [1] \"4044 distance\"\n#> [1] \"4030 gender\"\n# You can get the same data with this statement:\n\n# sapply(1:length(names(demo_data)), function(x)\n#        paste(sum(is.na(demo_data[,x])),names(demo_data[x])))\n\n# A best practice is to use apply functions in R when possible\n#-----------------------------------------------------------------\n# Only keep rows with no NAs in any column\n#-----------------------------------------------------------------\ncomplete_cases <- nrow(demo_data[complete.cases(demo_data),])\nknitr::kable(nrow(demo_data[complete.cases(demo_data),]),caption = 'Complete cases',\n             align = 'c',format = \"markdown\",padding = 0)\n#-----------------------------------------------------------------\n# Simple imputation. Make any NAs 'w'\n#-----------------------------------------------------------------\ndemo_data$ethnicity[is.na(demo_data$ethnicity)] <- 'w'\n#-----------------------------------------------------------------\n# Make an y missing age values the mean of overall age\n#-----------------------------------------------------------------\nmA <- \n  round(mean(as.numeric(as.character(na.omit(demo_data$age)))),0)\ndemo_data$age[is.na(demo_data$age)] <- mA\n#-----------------------------------------------------------------\n# Impute values based on a proportion\n#-----------------------------------------------------------------\nl <- \n  length(demo_data$maritalStatus[is.na(demo_data$maritalStatus)])\ndemo_data$maritalStatus[is.na(demo_data$maritalStatus)] <- \n  sample(c('m','s'), size=l, prob=c(.6,.4), replace=TRUE)\n\nknitr::kable(head(as.data.frame(demo_data[,1:6])),caption = 'Customer data set',\n             align = 'c',format = \"markdown\",padding = 0)\n#-----------------------------------------------------------------\n# Impute remaining values based on a proportion\n#-----------------------------------------------------------------\n#prop.table(table(demo_data$children))\n#prop.table(table(demo_data$gender))\n\nl <- length(demo_data$children[is.na(demo_data$children)])\ndemo_data$children[is.na(demo_data$children)] <- \nsample(c('n','y'), size=l, prob=c(.52,.48), replace=TRUE)\n\nl <- length(demo_data$gender[is.na(demo_data$gender)])\ndemo_data$gender[is.na(demo_data$gender)] <- \nsample(c('m','f'), size=l, prob=c(.5,.5), replace=TRUE)"},{"path":"chapter5.html","id":"imputating-with-a-package","chapter":"5 Segmentation","heading":"5.2.3.0.1 Imputating with a package","text":"several packages imputing numerical data. One commonly used ones mice (van Buuren Groothuis-Oudshoorn 2021). easy use works relatively quickly smaller datasets. several parameters can adjusted mice function. See documentation explain indicate. instance, used predictive mean matching method. However, dozen options . may appropriate others specific conditions.Let’s go ahead delete rows customerid missing. Use na.omit greedily remove rows column NA. can subtle, case na.omit get job done.Now can run function look missing values one time.Table 5.13: New missing valuesPerfect. expert imputation, now get gist. Correcting data missing values tedious critical component exercises. doesn’t tend one best way methods often judgment calls. 40% data missing go similar exercise just saw? Probably , may depend data missing. point think context trying accomplish.","code":"\n#-----------------------------------------------------------------\n# Impute numerical values using the mice package\n#-----------------------------------------------------------------\nlibrary(mice)\ndemo_data$distance <- as.numeric(demo_data$distance)\nimp_frame          <- as.data.frame(cbind(demo_data$hhIncome,\n                           demo_data$distance))\nimp_frame[]        <- apply(imp_frame,2,function(x) as.numeric(x))\n\nimputed_Data       <- mice(imp_frame, m=5, maxit = 10,\n                           method = 'pmm', seed = 755,\n                           printFlag = F)\nnew_Data           <- complete(imputed_Data,2)\ndemo_data$hhIncome <- new_Data$V1\ndemo_data$distance <- new_Data$V2"},{"path":"chapter5.html","id":"discretizing-our-numerical-variables","chapter":"5 Segmentation","heading":"5.2.3.1 Discretizing our numerical variables","text":"Many operations (latent class regression) require discreet data. art part exercise. However, several ways approach . ’ll couple different ways. ’ll start creating couple different data structures appropriate segmentation methods.Let’s begin creating new data frame work . ’ll use data frames tibbles interchangeably. haven’t spoken tibbles. Tibbles Wickham construct working tidyverse. functionally act like data frames, features generally make little friendlier work . However certain legacy packages require data frames. one peculiarities R won’t find Python. can also frustrating.","code":"\n#-----------------------------------------------------------------\n# Create a new dataframe to work with\n#-----------------------------------------------------------------\ndemo_data_discrete <- tibble(custID = demo_data$custID)"},{"path":"chapter5.html","id":"dummy-coding-our-categorical-variables","chapter":"5 Segmentation","heading":"5.2.3.2 Dummy coding our categorical variables","text":"Categorical variables treated particular way useful many segmentation exercises. begin, need discretize numerical variables. ’ll function apply numerical data sets. following function represents choice ’ll make. determine break continuous data discrete groups. case, use familiar generational marketing groups. package lubridate (Spinu, Grolemund, Wickham 2021) used make working dates easier. Dates can incredibly frustrating along regular expressions48 one reviled parts working data.Programmers thrive criticizing sort function. often algorithmic methods simplifying sort operation. switch function R languages can sometimes used. long aren’t dealing massive data sets, cares? readable get job done. SQL joins usually efficient -statement usually good direction begin larger data sets.tend find prefer individual’s birth year age. helps keep data evergreen. Keep mind build surveys. Now discrete value representing specific generations.Distance represents another nuanced problem. select create breaks? Distance can often approximated using zipcode centroids Haversine 49 formula.can also use Google’s APIs calculate latitude longitude based address estimate drive time. important certain urban suburban areas.case, already calculated distance. ’ll use quantiles place 60% customers one four buckets.can discretize household income way. case, ’ll use three buckets. Keep mind categories , columns final data set. can problematic get many.Finally, ’ll add variables dataframe.Table 5.14: discretized dataNow discrete variables, can take preparation step dummy-code variables. , several ways . psych package function tend use. another frustrating thing R. Python, unless implement encoder, likely always exact way every time.Table 5.15: Dummy coded dataNow dataframe consists categorical data, ’ll create one mixed numerical dummy-coded categorical data.Table 5.16: Combined data frameOur new dataframe now mixes discrete continuous variables format consumable wide array machine learning tools. column numeric. take even , example work now.","code":"\n#-----------------------------------------------------------------\n# Calculate birth year and assign to a generation\n#-----------------------------------------------------------------\nlibrary(lubridate)\nbirthYear <- lubridate::year(Sys.Date()) - \n             as.numeric(demo_data$age)\n\nf_seg_generation_def <- function(birth_year){\n  \n  if(birth_year >= 1910 & birth_year <= 1924){'gen_Greatest'}\n   else if (birth_year >= 1925 & birth_year <= 1945){'gen_Silent'}\n    else if (birth_year >= 1946 & birth_year <= 1964){'gen_Boomer'}    \n     else if (birth_year >= 1965 & birth_year <= 1979){'gen_X'}     \n      else if (birth_year >= 1980 & birth_year <= 1994){'gen_Mill'}       \n       else if (birth_year >= 1995 & birth_year <= 2012){'gen_Y'}    \n        else if (birth_year >= 2013 & birth_year <= 2020){'gen_Alpha'}      \n         else{'Unknown'}\n}\ndemo_data_discrete$generation <- \n  sapply(birthYear,f_seg_generation_def)\n#-----------------------------------------------------------------\n# Assign a discrete value to distance\n#-----------------------------------------------------------------\nquantile_list <- quantile(demo_data$distance,\n                          probs = c(.1,.2,.3,.4,.5,.6,.7,.8,.9))\n\nf_seg_dist_def <- function(dist,q){\n  \n  if(dist <= 30.67){'dist_Primary'}\n    else if (dist > 30.67 & dist <= 51.23){'dist_Secondary'}\n      else if (dist > 41.23 & dist <= 106.07){'dist_Tertiary'}    \n        else{'dist_Quaternary'}        \n}\n\ndemo_data_discrete$marketLoc <- \n  sapply(demo_data$distance,f_seg_dist_def)\n#-----------------------------------------------------------------\n# Assign a discrete value to household Income\n#-----------------------------------------------------------------\nhhIncomeQuantiles <- quantile(demo_data$hhIncome,\n                              probs = c(.25,.75))\n\nf_seg_hhInc_def <- function(hhInc, quant = hhIncomeQuantiles){\n  \n  if(hhInc <= hhIncomeQuantiles[[1]]){\"income_low\"}\n    else if(hhInc >= hhIncomeQuantiles[[2]]){\"income_high\"}\n      else{\"income_med\"}\n}\n\ndemo_data_discrete$income <-\n  sapply(demo_data$hhIncome,f_seg_hhInc_def)\n#-----------------------------------------------------------------\n# Add columns to demo_data\n#-----------------------------------------------------------------\ndemo_data_discrete <- demo_data_discrete    %>% \n  mutate(ethnicity = demo_data$ethnicity,\n         married   = demo_data$maritalStatus,\n         gender    = demo_data$gender)\n#-----------------------------------------------------------------\n# Dummy code (One Hot encoding) for model consumption\n#-----------------------------------------------------------------\ndummy_coded_vars <- \n  apply(demo_data_discrete[,2:7],2, \n        function(x) psych::dummy.code(factor(x)))\n\nmod_data_discrete <- \n  cbind.data.frame(dummy_coded_vars[1:length(dummy_coded_vars)])\n\nrow.names(mod_data_discrete) <- demo_data_discrete$custID\n#-----------------------------------------------------------------\n# Combine data frames\n#-----------------------------------------------------------------\nmod_data_numeric <- demo_data[,c('age','distance','hhIncome')]\nmod_data_numeric <- cbind.data.frame(mod_data_numeric,\n                                     mod_data_discrete[,13:20]) "},{"path":"chapter5.html","id":"hiearchical-clustering","chapter":"5 Segmentation","heading":"5.2.4 Hiearchical clustering","text":"Hierarchical clustering seductive results easily illustrated. However, drawback simple models, technique may tell already know. precise explanation “constraint structure hierarchical corresponds fact , although subsets can include one another, intersect.” (Ian H. Witten 2011) technique also problematic larger data sets.Let’s take look example. ’ll start creating dissimilarity matrix.can now use dissimilarity matrix cluster data. Keep mind dissimilarity matrices get large fast. Take look understand . many cases, sampling data approximate large data sets key. big data problems simply small data problems disguise. case sampled twenty-five individuals.can see, several groups fans tend cluster together. However, cuts can tend happen predictable ways using technique. byproduct categorical data.","code":"\n#-----------------------------------------------------------------\n# Prepared data for hierarchical clustering\n#-----------------------------------------------------------------\nlibrary(dplyr)\nlibrary(cluster)\nset.seed(755)\nmod_data_samp     <- mod_data_numeric %>% dplyr::sample_n(25)\nmod_data_samp$age <- as.numeric(mod_data_samp$age)\n# Create dissimilarity matrix\nmod_dist <- cluster::daisy(mod_data_samp)\n#-----------------------------------------------------------------\n# Apply heirarchical clustering method\n#-----------------------------------------------------------------\nmod_HC <- stats::hclust(mod_dist, method = \"centroid\")\n#-----------------------------------------------------------------\n# Apply hierarchical clustering method\n#-----------------------------------------------------------------\nplot(mod_HC)"},{"path":"chapter5.html","id":"latent-class-regression","chapter":"5 Segmentation","heading":"5.2.5 Latent class regression","text":"Latent class regression powerful technique produces durable clusters. leverages discrete data, continuous data discredited dummy-coded prior use. ’ve already dummy-coded data, ’ll process one time accepted LCR function. another one quirks R. allows happen. case ’ll simply make zeros = 2. can apply function simple ifelse function.Table 5.17: Data coded LCR functionA useful trick try simplify function feeds clustering function. make function easier read add parameters .going choose five classes include data. formal methods determining number classes include, keep mind classes useful understandable probably abstract relatively groups. can also problematic since behaviors exhibited group can become diffuse.can also save model import apply back new data future.Now can look see many groups created machine-learning algorithm.Table 5.18: Frequency table segmentsIt looks like groups fairly large range. smallest group contains 10,000 people largest one contains 56,000 individuals.can now create list ids class apply original data set.Table 5.19: LC model resultsOnce joined, can create interesting crosstabs look differences groups.general, distance fairly similar one group slightly away average.Table 5.20: Distance segmentAge much interesting. three distinct groups manifest arrange latent classes.Table 5.21: Age segmentHousehold income also well differentiated three distinct groups.Table 5.22: HHI segmentGender two groups male female three groups evenly split.Table 5.23: Gender segmentEthnicity appears fairly well distributed across three groups.one little complex. can use heat map visualize relationships. exponentially diminish differences colors using log function fill argument. useful technique. Keep mind. Logarithms great compressing data isn’t easy visualize distribution.One segment made entirely white individuals one segment white individuals. segments split proportionally across ethnic group. now? reason believe groups represent segments individuals? even represent market structure. far finished. groups spend money average? respond advertising different ways? ’ll want answer questions. Another way approach problem segment people measurable behaviors look look-alike groups. probably common way leverage sort analysis.demonstrate one segmentation approach combine previous analysis create psychograhic segments.","code":"\n#-----------------------------------------------------------------\n# Latent Class regression\n#-----------------------------------------------------------------\nmod_data_LC   <- mod_data_discrete\nmod_data_LC[] <- apply(mod_data_discrete, 2, \n                       function(x) ifelse(x == 1 ,1 ,2))\n#-----------------------------------------------------------------\n# Latent Class regression formula\n#-----------------------------------------------------------------\nformat  <- paste(names(mod_data_LC), collapse = \",\")\nformula <- \n  with(mod_data_LC,\n       cbind(generation.gen_X,generation.gen_Mill,\n             generation.gen_Boomer,generation.gen_Y,\n             generation.gen_Silent,marketLoc.dist_Quaternary,\n             marketLoc.dist_Tertiary,marketLoc.dist_Secondary,\n             marketLoc.dist_Primary,income.income_med,\n             income.income_low,income.income_high,\n             ethnicity.w,ethnicity.aa,ethnicity.h,\n             ethnicity.a,married.m,married.s,gender.f,\n             gender.m)~1)\n#-----------------------------------------------------------------\n# Latent Class regression\n#-----------------------------------------------------------------\nrequire(poLCA)\nset.seed(363)\nseg_LCR_5 <- poLCA::poLCA(formula, data = mod_data_LC, \n                          nclass = 5, verbose = F)\nsave(seg_LCR_5, file=\"CH5_LCR_5_Model.Rdata\")\n#-----------------------------------------------------------------\n# Evaluating the results\n#-----------------------------------------------------------------\ntable(seg_LCR_5$predclass)\n#-----------------------------------------------------------------\n# Attaching to the data frame\n#-----------------------------------------------------------------\nmod_results          <- \n  as.data.frame(matrix(nrow = nrow(mod_data_LC), ncol = 0))\nmod_results$custID   <- row.names(mod_data_LC)\nmod_results$lcrClass <- seg_LCR_5$predclass\n#-----------------------------------------------------------------\n# View segments\n#-----------------------------------------------------------------\nhead(mod_results)\n#-----------------------------------------------------------------\n# Attaching to the data frame\n#-----------------------------------------------------------------\ndemo_data_segments <- \n  dplyr::left_join(demo_data,mod_results, by = \"custID\")\n#-----------------------------------------------------------------\n# Distance cross tab\n#-----------------------------------------------------------------\nlibrary(dplyr)\ndemo_seg_distance <- demo_data_segments %>%\n  group_by(lcrClass)                    %>% \n  summarise(avgDist = mean(distance),\n            medDist = median(distance))\n                  \n#-----------------------------------------------------------------\n# Age cross tab\n#-----------------------------------------------------------------\ndemo_data_segments$age <- as.numeric(demo_data_segments$age)\n\ndemo_seg_age <- demo_data_segments             %>%\n                group_by(lcrClass)             %>% \n                summarise(avgAge = mean(age),\n                          medAge = median(age))\n                    \n#-----------------------------------------------------------------\n# Household income cross tab\n#-----------------------------------------------------------------\ndemo_seg_HHI <- demo_data_segments                        %>%\n                     group_by(lcrClass)                   %>% \n                     summarise(avgHHI = mean(hhIncome),\n                               medHHI = median(hhIncome))\n                    \n#-----------------------------------------------------------------\n# Gender cross tab\n#-----------------------------------------------------------------\ndemo_seg_gender <- \n  demo_data_segments         %>%\n  group_by(lcrClass)         %>% \n  count(gender)              %>%\n  tidyr::pivot_wider(names_from = gender, \n                     values_from = n)\n                     \n#-----------------------------------------------------------------\n# Ethnicity tab\n#-----------------------------------------------------------------\ndemo_seg_ethnicity <- \n  demo_data_segments    %>%\n  group_by(lcrClass)    %>% \n  count(ethnicity)      %>%\n  tidyr::pivot_wider(names_from  = ethnicity, \n                     values_from = n)\n                    \n#-----------------------------------------------------------------\n#  Visualize ethnicity\n#-----------------------------------------------------------------\nlibrary(ggplot2)\ndemo_seg_ethnicity_l <- \n  demo_data_segments %>%\n  group_by(lcrClass) %>% \n  count(ethnicity)      \n\ndemo_seg_ethnicity_l <- as.data.frame(demo_seg_ethnicity_l)\n\nx_label  <- ('\\n Ethnicity')\ny_label  <- ('Class \\n')\ntitle   <- ('Ethnicity by class')\ntile_segment <- ggplot2::ggplot(data   = demo_seg_ethnicity_l, \n                              aes(x    = ethnicity,\n                                  y    = lcrClass,\n                                  fill = log(n)))               +\n  geom_tile()                                                   +\n  scale_fill_gradient(low=\"white\", high=\"dodgerblue\",\n                      name = 'log(Fans)',label = scales::comma) +\n  xlab(x_label)                                                 + \n  ylab(y_label)                                                 + \n  ggtitle(title)                                                +\n  graphics_theme_1"},{"path":"chapter5.html","id":"a-benefits-sought-approach-to-segmentation","chapter":"5 Segmentation","heading":"5.3 A benefits sought approach to segmentation","text":"Behavioral segmentation can take many forms. ’ll focus activities fans may consider attending game. Taking benefits-sought approach segmentation interesting gives specific insight might position marketing offers specific group. downside discoverability reduced.’ll link forms build psychographic profiles. can go much deeper. instance, can ask many questions pertaining sorts activities. ’ll cover chapter research, chapter 9.","code":""},{"path":"chapter5.html","id":"factoranalysis","chapter":"5 Segmentation","heading":"5.4 Using a factor analysis to segment","text":"factor analysis data reduction technique similar principle components analysis. ’ll use slightly different way compress stated behaviors use patterns create specific archetypes.can load survey data using fa_survey_data data set FOSBAAS.Table 5.24: Survey dataThis data set based asking individuals like ballpark. list predefined activities fans asked rank 0 10. Fans also asked attended game.data wasn’t created person mind. can simply assign customer_ids data set simulate real people taken survey. ’ll also need process data perform factor analysis. use scale function normalize data. haven’t discussed yet, can important mixed data sets. Make sure data placed dataframe instead tibble.Now data prepared analysis, need consider many factors create. formulaic way determine number factors. ’ll use nScree function nFactors (Raiche Magis 2020) package get idea many factors appropriate. ways visualize data. Try eigen(cor(survey_sc)) function access eigenvalues.Table 5.25: Number FactorsNow idea many factors appropriate, can run factor analysis. ’ll use GPArotation package (Bernaards Jennrich 2022) add rotation argument factanal function stats (R-stats?) package. help us visualize data.’ll restructure data prepare visualization.Now data processed format can graph, can take look .can see certain activities tend group factors. data? First , need name groups approachable. art. Let’s give terrible try:Factor 1: Activities tend oriented toward game . Let’s call Avid fans. Creative.Factor 2: Tailgating, posting social media. Socializers. group also tends affinity children’s activities.Factor 3. Drink beer, sample food, Eat park food. Foodies.Factor 4: Seems interested park. ’ll need go deeper . Parkies.Factor 5: Doesn’t really distinct groupings. StrangersConsultants marketing backgrounds begin crafting stories groups. Supplemental analysis necessary get granular behavioral profiles. won’t take fake exercise far. get point.One problems using sort analysis impossible cast segments across groups taken survey. Additionally, someone belong different segments different days. Perhaps father brings children game one day goes friends another. Belonginess can mutable. However, richer data data sample might able predict segment. might also able create richer profiles utilizing demographic segments created earlier chapter. Let’s begin building simple model see factors can used predict reason someone came game. combine data see can find.’ll use function nnet (Ripley 2022) package -fit model. aren’t going rigorous really just looking curiosity. ’ll use multinom function try predict classes.surprisingly, can’t. However, reasons created completely randomly, wouldn’t really expect see good predictions . predict reasons, expect see nice 45 degree color band middle graph. terrible result like real life? go back evaluate data another way. dead-end. happens frequently.","code":"\n#-----------------------------------------------------------------\n# access the survey data\n#-----------------------------------------------------------------\nsurvey <- FOSBAAS::fa_survey_data\n#-----------------------------------------------------------------\n# access the survey data\n#-----------------------------------------------------------------\nnFacts <- nFactors::nScree(survey_sc)\n#-----------------------------------------------------------------\n# run a factor analysis\n#-----------------------------------------------------------------\nlibrary(GPArotation)\nsurvey_sc_ob <- factanal(survey_sc,\n                         factors  = 5,\n                         rotation = \"oblimin\",\n                         scores   = \"Bartlett\")\n#-----------------------------------------------------------------\n# Restructure the data\n#-----------------------------------------------------------------\nsurvey_sc_ob_scores <- as.data.frame(unlist(survey_sc_ob$scores))\nsurvey_sc_ob_scores$Reason <- survey$ReasonForAttending\n\nfa_data <- unclass(survey_sc_ob$loadings)\nfa_data  <- as.data.frame(fa_data)\nfa_data$Selection <- row.names(fa_data)\n\nlibrary(tidyr)\nfa_data_p <-  \n  fa_data %>% tidyr::pivot_longer(!Selection, \n                                   names_to  = \"Factor\", \n                                   values_to = \"Loading\")\n\nfa_data_p$Order <- \n  reorder(fa_data_p$Selection,fa_data_p$Loading,sum)\n#-----------------------------------------------------------------\n# Visualize the factors\n#-----------------------------------------------------------------\nrequire(ggplot2)\nfactor_table <- \nggplot(fa_data_p, aes(Factor, Order))                       +\n  geom_tile(aes(fill = Loading))                            + \n  geom_text(aes(label = round(Loading, 1)),color='grey40')  +\n  scale_fill_gradient(low      = \"white\", \n                      high     = \"dodgerblue\", \n                      space    = \"Lab\",\n                      na.value = \"grey50\", \n                      guide    = \"colourbar\")               +\n  xlab('\\n Factor')                                         + \n  ylab('Activity\\n')                                        + \n  ggtitle('What do fans do at games? \\n')                   +\n  graphics_theme_1                                          +\n  theme(legend.position=\"none\")                             +\n  theme(axis.text.y = element_text(angle = 0, \n                                   size = 8, vjust = 0, \n                                   color = \"grey10\"))\n#-----------------------------------------------------------------\n# predict classes\n#-----------------------------------------------------------------\nlibrary(nnet)\nmod_glm <-   nnet::multinom(Reason ~ .,\n                            data = survey_sc_ob_scores,\n                            linout = FALSE)\n#> # weights:  98 (78 variable)\n#> initial  value 26390.573296 \n#> iter  10 value 26368.581604\n#> iter  20 value 26367.453867\n#> iter  30 value 26362.202858\n#> iter  40 value 26361.592507\n#> iter  50 value 26360.892606\n#> final  value 26360.870907 \n#> converged\n\npred_survey_sc_ob_scores <- predict(mod_glm , \n                            newdata=survey_sc_ob_scores,\n                            type='class')\n\ntile_data <- \nas.data.frame(table(survey_sc_ob_scores$Reason,\n                    pred_survey_sc_ob_scores))\n#-----------------------------------------------------------------\n# Visualize the factors\n#-----------------------------------------------------------------\nrequire(ggplot2)\n\nx_label  <- ('\\n Actual response')\ny_label  <- ('Attend predict \\n')\ntitle   <- ('Prediction of attendance reason')\n\ntile_class <- ggplot2::ggplot(data = hex_data, \n                              aes(x    = Var1,\n                                  y    = pred_survey_sc_ob_scores,\n                                  fill = Freq))        +\n  geom_tile()                               +\n  scale_fill_gradient(low = \"dodgerblue\", high = \"coral\",\n                      name = 'Count',label = scales::comma)   +\n  xlab(x_label)                                               + \n  ylab(y_label)                                               + \n  ggtitle(title)                                              +\n  graphics_theme_1                                            +\n  theme(axis.text.x = element_text(angle = 90, \n                                   size = 8, vjust = 0, \n                                   color = \"grey10\"),\n        axis.text.y = element_text(angle = 0, \n                                   size = 8, vjust = 0, \n                                   color = \"grey10\"))"},{"path":"chapter5.html","id":"casting-segments-onto-other-groups","chapter":"5 Segmentation","heading":"5.4.1 Casting segments onto other groups","text":"Can use demographic factors estimate someone’s factor group? , easy cast segments onto broader groups people. Let’s assign survey respondents individuals customer tableNow combined analyses can look patterns within data. First, let’s recode data approachable. Let’s use switch function. switch function great, every language . Thanks R.Now ready take look profiles slightly sophisticated way.","code":"\n#-----------------------------------------------------------------\n# Apply classes to individuals\n#-----------------------------------------------------------------\nids <- sample(demo_data_segments$custID,10000)\nsurvey_sc_ob_scores$Class <- \n  colnames(survey_sc_ob_scores)[max.col(survey_sc_ob_scores[,1:5],\n                                        ties.method=\"first\")]\nsurvey_sc_ob_scores$custID <- ids\ncombined_classes <- dplyr::left_join(demo_data_segments,\n                                     survey_sc_ob_scores,\n                                     by=\"custID\")         %>%\n                                     na.omit()\ncombined_classes <- as.data.frame(combined_classes)\n#-----------------------------------------------------------------\n# recode the factors and classes\n#-----------------------------------------------------------------\ncombined_classes$f_seg_name <- sapply(combined_classes$Class, \n                 function(x) switch(x,\"Factor1\" = \"Avid fan\",\n                                      \"Factor2\" = \"Socializers\",\n                                      \"Factor3\" = \"Foodies\",\n                                      \"Factor4\" = \"Parkies\",\n                                      \"Factor5\" = \"Strangers\"))\n\ncombined_classes$d_seg_name <- sapply(combined_classes$lcrClass, \n                 function(x) switch(x,'1' = \"Young and Broke\",\n                                      '2' = \"Marty Male\",\n                                      '3' = \"Fionna Female\",\n                                      '4' = \"Diffuse Diane\",\n                                      '5' = \"Nearby Ned\"))"},{"path":"chapter5.html","id":"psychographic-segmentation","chapter":"5 Segmentation","heading":"5.5 Psychographic segmentation","text":"Psychographic augmentations tend abstract, although appear simplify combination behavioral demographic schemes. see schemes often. Segments alliterative names “Jets fan Jeff” “Torpid Tina.” can use behavioral demographic segmentation schemes build psychographic schemes.can see demographic segments factor segments cluster specific groups. can now analyze groups create specific psychographic profiles.Now can take look differences groups. data contrived, interesting insights can extract .factored segments don’t seem influenced age distance.Clusters well differentiated distance age.case, can say male female segments tend index older segments live average distance away park. Young Broke segment tends live nearby. gave Nearby Ned ironic name since tends live furthest park., just getting started. example produced demonstrate tedious segmentation can . full analysis much exhausting, repetitive. covered basic steps.","code":"\n#-----------------------------------------------------------------\n# Visualize the results\n#-----------------------------------------------------------------\ncc <- combined_classes                %>%\n      group_by(f_seg_name,d_seg_name) %>% \n      count()      \n\nx_label  <- ('\\n Factor Segments')\ny_label  <- ('Demographic Segment \\n')\ntitle   <- ('Segment Comparrison')\ntile_segment <- \n  ggplot2::ggplot(data = cc, \n                  aes(x = f_seg_name,\n                      y = d_seg_name,\n                      fill = n))                              +\n  geom_tile()                                                 +\n  scale_fill_gradient(low=\"white\", high=\"dodgerblue\",\n                      name = 'Count',label = scales::comma)   +\n  xlab(x_label)                                               + \n  ylab(y_label)                                               + \n  ggtitle(title)                                              +\n  graphics_theme_1                                            +\n    theme(axis.text.x = element_text(angle = 0, \n                                   size = 8, vjust = 0, \n                                   color = \"grey10\"),\n        axis.text.y = element_text(angle = 0, \n                                   size = 8, vjust = 0, \n                                   color = \"grey10\"))\n#-----------------------------------------------------------------\n# Scale the results and average\n#-----------------------------------------------------------------\ncombined_classes$age_s <- \n  scale(as.numeric(combined_classes$age))\ncombined_classes$distance_s <- \n  scale(as.numeric(combined_classes$distance))\n\ncc <- \n  combined_classes                %>%\n  group_by(f_seg_name,d_seg_name) %>% \n  summarise(age = mean(age_s), \n  distance = mean(distance_s)) \n#-----------------------------------------------------------------\n# Observe differences between the segments\n#-----------------------------------------------------------------\nx_label  <- ('\\n Scaled Avg Age')\ny_label  <- ('Scaled Avg Distance \\n')\ntitle    <- ('Segments by distance and age')\n\npoint_segment <- \n  ggplot2::ggplot(data      = cc, \n                  aes(x     = age,\n                      y     = distance,\n                      color = f_seg_name,\n                      shape = d_seg_name))                +\n  geom_point(size = 3)                                    +\n  scale_color_manual(values = palette)                    +\n  geom_hline(yintercept = median(cc$distance), lty = 2)   +\n  geom_vline(xintercept = median(cc$age), lty = 2)        +\n  xlab(x_label)                                           + \n  ylab(y_label)                                           + \n  ggtitle(title)                                          +\n  graphics_theme_1"},{"path":"chapter5.html","id":"other-segmentation-methods","chapter":"5 Segmentation","heading":"5.6 Other segmentation methods","text":"hundreds methods segment data. general rule, start simplest method work . means using one rule . Think Males Females. many tools use:K-means clusteringKNNExpectation maximizationsimple schemesNo one approach best. Nate Silver describes Hedgehog Fox book “Signal Noise.” (Silver 2012) case forecasting, want fox. ’ll need combine practical knowledge analytic process. purely numerical approach unlikely yield best results.","code":""},{"path":"chapter5.html","id":"key-concepts-and-chapter-summary-4","chapter":"5 Segmentation","heading":"5.7 Key concepts and chapter summary","text":"Segmenting data art science. also frustrating, repetitive, difficult. didn’t cover simple segmentation schemes, age ranges generations. However, good place typically start. covered lot material chapter tried hit many high marks tangible examples. covered:Data exploration demographic approach segmentationData preparation imputationAn hierarchical approach segmentationSegmentation schemes using latent class regression demographicsA benefits sought approach utilizing factor analysisCombining schemes build psychographic profilesThere lots ways segment customers potential customers can creative . key uncover something beyond market structure. looking exploitable differences groups help spend limited marketing dollars efficiently.demographic segmentation merits, may much use . simply easiest place typically start.Preparing data analysis tedious task better suited someone besides . part project time consuming. bears heavily outcome get correct.Hierarchical approaches segmentation utility cases, can often tell already know.Latent class regression tends produce durable segments good second-step looking simple segmentation schemes.Benefits-sought approaches segmentation often good utility, discoverable methods. factor analysis good method simplify complex data sets.can ensemble methods create richer profiles.","code":""},{"path":"chapter6.html","id":"chapter6","chapter":"6 Forcasting Sales and Setting Prices","heading":"6 Forcasting Sales and Setting Prices","text":"Like segmentation, easy write entire book forecast price tickets. innumerable considerations determining willingness pay, taking arbitrage account, cannibalizing sales, margins, marketing channel, product suite, brand considerations, competing internal goals. chapter also immediately follows examples might potentially segment market. purposeful. Pricing dynamic exercise certain circumstances products may worth less money. However, pricing isn’t always deliberate exercise. love quote found Robert Phillips’ book “Revenue Pricing Optimization” (Phillips 2005)“many cases, prices charged customers result large number uncoordinate arbitrary decisions.”— Robert L. PhillipsThis absolutely true. pricing fundamental component marketing mix, isn’t always well understood interacts promotions marketing efforts (’ll explore little chapter 9). Additionally, always brand considerations consider. Marketers often regress blunt instrument lower prices effort move tickets. can counterproductive long term. team isn’t good, lower prices less impact team performing well. However, less impetus lower prices team performing well. tremendous research area (Keller 2003).Wharton’s Peter Fader pooints number limitations valuation approaches: require much judgmental data thus contain much subjectivity; Intangible assets always synoymous brand equity.”brand impacted price. Since season ticket holders critical part risk mitigation, anything erodes value viewed scrutiny. chapter take simplified example can set prices discuss rationale behind thinking. walk real examples demonstrate way done practice. However, keep mind going knee deep ocean. Pricing can challenging exercise.","code":""},{"path":"chapter6.html","id":"understanding-your-inventory","chapter":"6 Forcasting Sales and Setting Prices","heading":"6.1 Understanding your inventory","text":"value tickets? Let’s think inventory might look. multiple ticket classes individuals may cross multiple classes. Additionally, certain conditions make sales inventory fluid. instance, goal maximize revenue selling many season tickets possible may actually reduce revenue potential sold discount. tipping point. pieces inventory may inefficient use certain games. instance, may want use group discounts opening day. Major ticketing inventory includes:Season ticketsGroup ticketsSmall plansSingle game ticketssubscription ticketsAdditionally, several channels tickets might obtained. Understanding channels critical pricing. Think airlines. exert significant control channels enables extremely fine-grain control pricing tactics. Typical ticketing channels include:Primary channels\nDirectly team league\nconsignment mechanism\nticketing system Ticketmaster\nDirectly team leagueThrough consignment mechanismThrough ticketing system TicketmasterSecondary channels\nSeatGeek\nStubhub\nSeatGeekStubhubSecondary channels even nuanced large number tickets may owned professional ticket brokers. tickets may may exist liquid market allows prices float levels. Ticket brokers represent special problem. Let’s discus exactly ticket brokers .Ticket brokers buy sell tickets events. sports environment, two primary strategies. However, can hedge bets purchasing multiple events teams sport different sports. arbitrage opportunities might also exist parking passes. two main strategies :Scale purchase cheap inventoryPurchase good inventory maintain strong marginsOption number one can little risky. However, varies market. purchase lots inexpensive inventory make money marquee events margins high demand significantly outstrips supply (Think big games weekend prime opponents playoff games). Option two little difficult. likely takes time makes visible team. Many clubs now employ revenue sharing opportunities brokers simply using way sell single game tickets secondary market. Clubs may also leverage secondary channels flatten markets.Selling brokers mitigates risk get money can used operations early. amount money dealing large 50. However, tradeoff. longer control sales channels. Imagine purchase airline tickets secondary channels. airlines able dynamically price effectively? Additionally, customer service issue. longer resides club. happens ticket doesn’t work? fix ? becomes brand problem. Recently, quick proliferation digital tickets made problem less relevant, persist. also technology component. ticketing platform allow disallow. many technology problems likely solved future, haven’t currently eliminated.Adding complexity sales channels inventory classes price levels associated specific locations:Premium areas might amenities free F&B, better seats, club access, -seat service etc.seats may shaded areasDistance field court may matter experiential perceptual perspectiveThe home field side park may attractive buyersThe setting sun may make seats less appealingSome people may want sit area near certain food offerings, near ingress point close preferred parking location.Scoreboard sightlines may issueAll factors may considered, certainly . Product stratification key maximizing revenue opportunity across spectrum customer expectations.determine willingness--pay? historical data based people paid tickets past events. also sales levels associated prices ticket-class specific location. data might much harder capture.stadium arena might dozens even hundreds different price levels. cannibalization must considered. may know price increase decrease one section might demand another. cross-price elasticity demand seating locations isn’t trivial question answer. can also deploy interesting techniques help mitigate cannibalization liner optimization51.Another confounding factor inventory levels. know much inventory available? might know sold seven seats dugout section sold Brewers Saturday night July. However, might know inventory hold available sale. Additionally, might visibility many tickets sale secondary market. aren’t even considering marketing efforts promote game.Inventory control critical part pricing exercise. shouldn’t overlooked. Incentives sell specific asset classes sales team can lead sub-optimal inventory control protocols. instance, sales team may inclined hold inventory specific games help hit group goals. conducting pricing exercise, make sure understand inventory may alloted.","code":""},{"path":"chapter6.html","id":"understanding-pricing-mechanisms","chapter":"6 Forcasting Sales and Setting Prices","heading":"6.2 Understanding pricing mechanisms","text":"pricing ticket may faced unresolvable goal.want sell available tickets highest price possible optimize revenueIs selling 10 tickets $100 $1,000 revenue equivalent selling 100 tickets $10 $1,000 revenue? answer may different depending ask. going look price-response functions get quantitative perspective. Price response functions fun work analytics perspective, get lot enjoyment deploying type analysis. Price response functions incredibly useful pointed “Pricing Segmentation Analytics.” (Tudor Bodea 2012)useful feature price-response functions , estimated, can used determine price sensitivity product demand change resposne change price. - Furguson BodeaWhat price-response function look like practice.? Let’s say sell tickets build chart results:table tells many tickets sold price level. figure 6.1 represents data:\nFigure 6.1: Linear price response function\ncan clearly see linear model probably won’t great job forecasting sales different price levels. might also deduce encounter diminishing returns point continue raise prices. graph also missing critical information. now, let’s assume prices represent one section stadium refer single game tickets. ’ll ignore fact don’t know tickets sold, channel sold , available inventory. Let’s also briefly talk fitting distributions price response functions. lot different ways going look relatively simple example. following examples taken “Pricing Segmentation Analytics.” (Tudor Bodea 2012)Typical examples price response functions include:Linear models:\\[\\begin{equation}\n\\ {d(p)} = {D} + {m}*{b}\n\\end{equation}\\]Exponential models:\\[\\begin{equation}\n\\ d(p) = C*p^\\epsilon\n\\end{equation}\\]Logit models:\\[\\begin{equation}\n\\ {d(p)} = \\frac{C*e^{+b*p}}{1 + e^{+b*p}}\n\\end{equation}\\]Let’s try build simple model approximates data little better straight line. can see linear model won’t fit data well curved one. also doesn’t look constant like logit curve (two models likely encounter). Let’s try simple polynomial.\nFigure 6.2: Polynomial price response function\ncurve approximates line relatively well. go exercise finding line produces best fit, isn’t necessary stage. Now happy simple polynomial reasonable job approximating curve, ? R provides several ways evaluate fits. following example feed polynomial function lm (linear model) function R. can write function f_get_Sales can use return values line figure 6.2. ’ll use coefficients complete function.now overfit price response function can use forecast sales different price levels. Since concept may unfamiliar ’ll take minute explain going . fit <- lm(sales$sales~poly(sales$price,2,raw=TRUE)) using lm function (linear model) fit line data points. aren’t necessarily looking best fit example. function f_get_sales() simply takes coefficients generated model stored fit builds equation can use plug price values equation give us sales values (y) terms price (x). might look little scary, really just middle-school algebra stage.equation function sales <- coef(fit)[1] + (coef(fit)[2]*new_price + (coef(fit)[3] * new_price^2)) nothing rearranged linear equation second degree exponent. linear equation equation fit line figure 6.1 take familiar form:\\[\\begin{equation}\n\\ {y} = {m}{x} + {b}\n\\end{equation}\\]done rearranged linear equation added second degree exponent. math now looks like :\\[\\begin{equation}\n\\ {f(x)} = b + {m_1}{x} + {m_2}{x^2}\n\\end{equation}\\]conceptualize linear regression simple linear equation additional feature manipulates slope line. think simplest way think .\\[\\begin{equation}\n\\ ({m_1}{x} + {m_2}{x^2}) = {m}{x}\n\\end{equation}\\]Now can take look estimates applying 52 function original values. , apply functions one areas R diverges languages. familiar Map functions, basically thing. case, replacing loop sapply.Table 6.1: Estimates applied old salesAdditionally, can feed numbers directly function. many sales expect $26 price level?Table 6.2: Estimates applied new salesThere tricks can use equation. highest point curve represents maximum number tickets sold particular price level. find price number sales point? raw equation:\\[\\begin{equation}\n\\ {f(x)} = -737.366 + {52.934}{x} + {-0.906}{x^2}\n\\end{equation}\\]deep get math book. ’ll see tricky stuff chapter operations, difficult get . Calculate derivative function. since ’ve taken calculus, simply use math engine Wolframalpha 53.\\[\\begin{equation}\n\\ \\frac{dy}{dx} ({-737.366} + {52.934}{x} - {0.906}{x^2}) = {52.934} - {1.812}{x}\n\\end{equation}\\]Setting derivative = 0 solving equation give price level highest point curve:\\[\\begin{equation}\n\\ f'(0) = {52.934} - {1.812}{x};\n\\end{equation}\\]Now simply solve equation:\\[\\begin{equation}\n{26467}/{906} = {29.21}\n\\end{equation}\\]highest number sales happen price $29.21. can plug number back original equation get y value.\\[\\begin{equation}\n\\ f(29.21) = -737.366 + {52.934}{29.21} + {-0.906}{29.21^2} = 35.8151054\n\\end{equation}\\]$29.21 model predicts 35.8151054 tickets sold. Let’s take look figure 6.3 try interpretation. side note, want jerk refuse use basic calculus write binary search function traverses function along range values outputs minimums maximums. Since typically dealing whole numbers limited range, really nothing wrong taking approach.\nFigure 6.3: Finding local maximum\ngreat. identified maximum number tickets sold price level range. revenue look like specific prices according model?Table 6.3: Forecasted revenue different price levelsNow imagine set prices product just modeled. . know revenue maximizing number lives. tell entire story? Let’s assume every price level sell tickets cost . cumulative revenue look like case?Let’s use loop time instead apply function defined function.Table 6.4: Revenue maximizing pricesThis interesting way look problem. make assumption sell every ticket costs price set, appears setting prices $25 make revenue. However, sold every ticket forecasted amounts actually earn revenue: 8495.9703213. simplistic example demonstrates power variable dynamic pricing. also considerations. set prices case might appear $25 price level best. earn ticket revenue sell tickets. However, simply due price elasticity assumptions made. season-ticket-holder price $29? now brand considerations. inventory limited makes decisions slightly discrete.","code":"\n#-----------------------------------------------------------------\n# The Price response function\n#-----------------------------------------------------------------\nlibrary(tidyverse)\n# Build a simple data set\nsales <- tibble::tibble(\n  sales = c(20,30,35,43,35,8,2,0),\n  price = c(25,28,29,30,31,34,35,36)\n)\n\nx_label  <- ('\\n Price')\ny_label  <- ('Ticket Sales \\n')\ntitle    <- ('Ticket Sales by Price')\nline_sales <- \n  ggplot2::ggplot(data  = sales, \n                  aes(x = price,\n                      y = sales))                 +\n  geom_point(size = 2.5,color = 'mediumseagreen') +\n  scale_x_continuous(label = scales::dollar)      +\n  xlab(x_label)                                   + \n  ylab(y_label)                                   + \n  ggtitle(title)                                  +\n  graphics_theme_1                                +\n  geom_line(color = \"mediumseagreen\")             +\n  geom_smooth(method = 'lm') \n#-----------------------------------------------------------------\n# The Price response function\n#-----------------------------------------------------------------\nx_label  <- ('\\n Price')\ny_label  <- ('Ticket Sales \\n')\ntitle    <- ('Ticket Sales by Price')\nline_sales_poly <- \n  ggplot2::ggplot(data = sales, \n                  aes(x = price,\n                      y = sales))                 +\n  geom_point(size = 2.5,color = 'mediumseagreen') +\n  scale_x_continuous(label = scales::dollar)      +\n  xlab(x_label)                                   + \n  ylab(y_label)                                   + \n  ggtitle(title)                                  +\n  graphics_theme_1                                +\n  geom_line(color = \"mediumseagreen\")             +          \n  stat_smooth(method = \"lm\",color = 'coral', \n              formula = y ~ x + poly(x, 2)-1) \n#-----------------------------------------------------------------\n# Function to return sales based on price\n#-----------------------------------------------------------------\nfit <- lm(sales$sales~poly(sales$price,2,raw=TRUE))\n\n f_get_sales <- function(new_price){\n sales <- coef(fit)[1] + \n         (coef(fit)[2]*new_price + (coef(fit)[3] * new_price^2))\n return(sales)\n }\n \n#-----------------------------------------------------------------\n# Use f_get_sales to get modeled demand at each price level\n#-----------------------------------------------------------------\nold_prices      <- c(25,28,29,30,31,34,35,36)\nestimated_sales <- sapply(old_prices, function(x) f_get_sales(x))\n#-----------------------------------------------------------------\n# Use f_get_sales to get modeled demand at each price level\n#-----------------------------------------------------------------\nestimated_sales_new <- f_get_sales(26)\n#-----------------------------------------------------------------\n# Demonstrate the highest point on the curve\n#-----------------------------------------------------------------\nx_label  <- ('\\n Price')\ny_label  <- ('Ticket Sales \\n')\ntitle   <- ('Ticket Sales by Price')\nline_sales_polyb <- \n  ggplot2::ggplot(data  = sales, \n                  aes(x = price,\n                      y = sales))                     +\n  geom_point(size = 2.5,color = 'mediumseagreen')     +\n  scale_x_continuous(label = scales::dollar)          +\n  xlab(x_label)                                       + \n  ylab(y_label)                                       + \n  ggtitle(title)                                      +\n  graphics_theme_1                                    +\n  geom_line(color = \"mediumseagreen\")                 +                                         \n  stat_smooth(method = \"lm\", color = 'coral',\n              formula = y ~ x + poly(x, 2)-1, se = F) +\n  geom_hline(yintercept = 35.8151054, lty = 2)        +\n  geom_vline(xintercept = 29.21, lty = 2)\n#-----------------------------------------------------------------\n# Look for optimum price levels\n#-----------------------------------------------------------------\nestimated_prices <- seq(from = 25, to = 35, by = 1)\nestimated_sales <- sapply(estimated_prices,\n                          function(x) f_get_sales(x))\n\nestimated_revenue <- tibble::tibble(\n  \n  sales      = estimated_sales,\n  price      = estimated_prices,\n  revenue    = estimated_sales * estimated_prices,\n  totalSales = rev(cumsum(rev(sales)))\n)\n#-----------------------------------------------------------------\n# Look for optimum price levels\n#-----------------------------------------------------------------\nx <- 1\nrevenue <- list()\nwhile(x <= nrow(estimated_revenue)){\n  \n revenue[x] <- estimated_revenue[x,2] * estimated_revenue[x,4]\n x <- x + 1\n}\nestimated_revenue$totalRevenue <- unlist(revenue)"},{"path":"chapter6.html","id":"basics-of-forecasting","chapter":"6 Forcasting Sales and Setting Prices","heading":"6.3 Basics of forecasting","text":"forecasting fundamental setting prices lots different ways . subject can also get sophisticated. ’ll focus couple basic approaches forecasting confronted working club. , demonstrate approach problems focus think . Let’s create sample data. can also find data FOSBAAS package. using FOSBAAS::past_season_data. can use functions build different conditions data set like simulated different situations.Now let’s take look data set created.Table 6.5: past seasons dataIn section, going dive regression little rigorous fashion. , highly recommend purchasing book regression current favorite “R Companion Applied Regression.” (John Fox 2019) John Fox Sanford Weisberg. practical doesn’t dwell theory. also lots statistical terms need familiar really gain proficiency regression. Often times, important understand limitations tool using. non-exhaustive list terms might run across might include:Hypothesis testing54Normality55Independence56linearity57Validation58Homoskedacity vs Heteroskedacity59Autocorrelation60Multicolinearity61Interaction terms62Omited Variable Bias63Transformations64Endogeneity65An unfortunate fact working sports data living non-parametric world rank order might important. Additionally, critical underlying assumptions data may met. specific ways deal problems. However, clinical study, need going going every test world make sure model durable. hit high points move . just need make sure aware deep ocean gets. won’t get heads.said, can approach forecasting several ways, two main types forecasting approaches typically deploy:Top-forecastsBottom-forecastsWhat call top-forecast typically consists macro-level factors team payroll projected wins. forecasts can useful comparing potential across entire league since can understand main components advance. Bottom-forecasts can vary granularity, consist specific information marketing schedule specifics around scheduling game attractiveness specific market. might mean rivalry games attractive can build model. Let’s start building simple bottom-forecast.First, let’s create treatment control group can evaluate efficacy model. ’ll perform tasks couple different packages future chapters go depth rigorously apply modeling techniques data. following lines code create training test data set. example uses standard R functions.earlier examples overfit models. didn’t split data testing training data. even partition data calibration set, ’ll stick simpler training test set now.Now can build simple linear model training data set. select following values? Selecting values model can involved. time , let’s just select values think look reasonable. Variable selection can involved creating parsimonious models (simplest model captures variance) best practice. Also keep mind dealing mixed data sets. regression examples find deal clean numerical data normally distributed. rarely case wild.following sections won’t exhaustive, give crash course things need look . also use realistic example find working club.standard summary model looks like :looking output. ’ll need hit books . aren’t familiar confusing statistics speak, ’ll need .Estimate: coefficients equation produced regressionStd. Error: standard error represents average distance estimates fall regression line.t value: coefficient divided standard errorPr(>|t|): p value. tell variable significant. sure estimate falls within confidence interval.Residual standard error: larger number, less likely model useful.Multiple R-squared: much variation explained modelF-statistic: model valid? Lower numbers badAbout 60% variance ticket sales explained simple model. good? Probably especially consider standard error. anything can improve ? Let’s take look regression diagnostics help us evaluate results. Additionally, can run plot(ln_mod_bu) get standard diagnostic plots.cut statistics brevity. summaries tend look like :Table 6.6: Summary stats modelThis OLS regression certain assumptions need made underlying data. aren’t satisfied, model may problems. car package (Fox, Weisberg, Price 2022) great tools help evaluate models. ’ll use evaluate model.","code":"\n#-----------------------------------------------------------------\n# Generate past seasons data\n#-----------------------------------------------------------------\nseason_2022 <- \n  FOSBAAS::f_build_season(seed1 = 3000, season_year = 2022,\n  seed2 = 714, num_games = 81, seed3 = 366, num_bbh = 5,\n  num_con = 3, num_oth = 5, seed4 = 309, seed5  = 25,\n  mean_sales = 29000, sd_sales = 3500\n )\n\nseason_2023 <- \n  FOSBAAS::f_build_season(seed1 = 755, season_year = 2023,\n  seed2 = 4256, num_games = 81, seed3 = 54, num_bbh = 6,\n  num_con = 4, num_oth = 7, seed4 = 309, seed5  = 25,\n  mean_sales = 30500, sd_sales = 3000\n )\n\nseason_2024 <- \n  FOSBAAS::f_build_season(seed1 = 2892, season_year = 2024,\n  seed2 = 714, num_games = 81, seed3 = 366, num_bbh = 9,\n  num_con = 2, num_oth = 6, seed4 = 6856, seed5  = 2892,\n  mean_sales = 32300, sd_sales = 2900\n )\n\npast_season <- rbind(season_2022,season_2023,season_2024)\n#-----------------------------------------------------------------\n# Build test and training set\n#-----------------------------------------------------------------\nsamp <- round(0.05 * nrow(past_season),0)\n\nset.seed(715)\nrows  <- sample(seq_len(nrow(past_season)), \n                        size = samp)\ntrain <- past_season[-rows, ]\ntest  <- past_season[rows, ]\n#-----------------------------------------------------------------\n# Linear model for ticket sales\n#-----------------------------------------------------------------\nln_mod_bu <- lm(ticketSales ~ promotion + daysSinceLastGame + \n                  schoolInOut + weekEnd + team , data = train)\nln_mod_bu_sum <- \ntibble::tibble(\n  st_error     = unlist(summary(ln_mod_bu)[6]),\n  r_square     = unlist(summary(ln_mod_bu)[8]),\n  adj_r_square = unlist(summary(ln_mod_bu)[9]),\n  f_stat       = unlist(summary(ln_mod_bu)$fstatistic[1]))#> \n#> Call:\n#> lm(formula = ticketSales ~ promotion + daysSinceLastGame + schoolInOut + \n#>     weekEnd + team, data = train)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -10994.7  -2468.7    126.9   2504.1   8969.3 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)       33357.39    2929.22  11.388  < 2e-16 ***\n#> promotionconcert    127.09    1657.31   0.077 0.938953    \n#> promotionnone     -5358.02     988.93  -5.418 1.71e-07 ***\n#> promotionother    -3721.18    1375.10  -2.706 0.007390 ** \n#> daysSinceLastGame   288.13      42.69   6.750 1.53e-10 ***\n#> schoolInOutTRUE    4608.16    1037.41   4.442 1.47e-05 ***\n#> weekEndTRUE        6397.82     610.61  10.478  < 2e-16 ***\n#> teamATL            3135.03    3000.93   1.045 0.297417    \n#> teamBAL            4382.63    2925.33   1.498 0.135651    \n#> teamBOS           12751.52    3594.69   3.547 0.000484 ***\n#> teamCHC           13193.89    3565.58   3.700 0.000278 ***\n#> teamCIN            3659.22    3226.57   1.134 0.258101    \n#> teamCLE            2201.94    2906.84   0.758 0.449632    \n#> teamCOL            3057.71    3285.54   0.931 0.353142    \n#> teamCWS            3906.19    2985.27   1.308 0.192195    \n#> teamDET            3202.46    3267.54   0.980 0.328217    \n#> teamFLA            2833.12    2924.04   0.969 0.333750    \n#> teamHOU           10209.56    2947.13   3.464 0.000649 ***\n#> teamKAN            5246.46    3732.75   1.406 0.161403    \n#> teamLAA            9102.40    2888.86   3.151 0.001875 ** \n#> teamLAD           12150.54    3198.52   3.799 0.000192 ***\n#> teamMIN            1625.41    3708.86   0.438 0.661672    \n#> teamNYM            9879.64    3030.31   3.260 0.001306 ** \n#> teamOAK            1877.06    3741.10   0.502 0.616397    \n#> teamPHI            9450.28    3949.78   2.393 0.017645 *  \n#> teamSF             8185.73    3741.10   2.188 0.029813 *  \n#> teamTB             3212.49    3191.25   1.007 0.315305    \n#> teamTEX            4595.50    3069.58   1.497 0.135925    \n#> teamWAS            1036.66    3036.91   0.341 0.733192    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3900 on 202 degrees of freedom\n#> Multiple R-squared:  0.6632, Adjusted R-squared:  0.6166 \n#> F-statistic: 14.21 on 28 and 202 DF,  p-value: < 2.2e-16"},{"path":"chapter6.html","id":"outliers-and-unusual-data-points","chapter":"6 Forcasting Sales and Setting Prices","heading":"6.3.1 Outliers and unusual data points","text":"Linear regression sensitive outliers. always best look data points fall well outside average range. instance, perhaps games rained . Maybe new promotion worked really well. Something create unusual data point two. Sometimes appropriate remove .appears one outlier:\nTable 6.7: Outliers\nQQ plot help check underlying assumptions around normality satisfied. demonstrates “95% confidence envelope.”part, points tend fall line. points stray can see outlier (48) bottom. Linear regression models can highly influenced extreme points. ’ll end removing point rerunning model.can run influence plot combine several graphs see can see plot(ln_mod_bu). bigger circle, larger Cooks Distance. Cooks distance mathematical definition, just think way identify influential points data.appears several points -sized influence model. evaluate individually. may OK remove model values appear influenced exogenous factor accounting . might points influential? Perhaps rained dates depressed ticket sales. might reasonable exclude dates analysis.Table 6.8: Influential ObservationsLet’s remove observations.Table 6.9: Summary stats clean modelWe get modest improvement removing influential values. else can ? can also look interaction terms transform response. multiple ways handle . Additionally may need transform data. case, don’t see obvious curvature data.can also transform variables regression directlyTable 6.10: Summary stats log modelThis makes model worse. also makes difficult interpret. isn’t uncommon data find working sports. aren’t dealing new medicines. dealing dollars cents. don’t precise. However, precision helps! Often business, simply best can. understand limitations done, ’ll still good shape comes time make decision.Let’s go ahead apply model testing data set. well appear approximate ticket sales practice? ’ll use predict function apply predictions test data observe close estimates fall reality.Table 6.11: Mean error percentageOn average, 2% mark. good? Maybe. depends trying accomplish. Let’s take look error graph.Overall, error relatively low. However, can see average error low, point estimates can vary considerably. feel comfortable applying model new data. believe . However, probably look predictors might improve model. Perhaps removing season tickets help. Unfortunately, away season, blunt estimates tend . won’t gambling odds, may promotional schedule. pull insignificant teams model won’t able use make forecast, ’ll guess. ’ll often settle “good enough.”top-approach forecasting sales analyzing prices useful certain circumstances many applications. next example cover bottom-approach.","code":"\n#-----------------------------------------------------------------\n# Outliers test\n#-----------------------------------------------------------------\nlibrary(car)\noutliers <- car::outlierTest(ln_mod_bu)\n#-----------------------------------------------------------------\n# QQPlot\n#-----------------------------------------------------------------\nqq <- car::qqPlot(ln_mod_bu, main=\"QQ Plot\")\n#-----------------------------------------------------------------\n# Influence plot\n#-----------------------------------------------------------------\nip <- car::influencePlot(ln_mod_bu, id.method=\"identify\", \n              main=\"Influence plot for our linear model \")\n#-----------------------------------------------------------------\n#  Remove outliers and view summary\n#-----------------------------------------------------------------\npast_season_clean <- past_season[-c(37,48,53,129,142,143),]\n\nln_mod_clean <- lm(ticketSales ~ promotion + daysSinceLastGame + \n                  schoolInOut + weekEnd + team , \n                  data = past_season_clean)\n#-----------------------------------------------------------------\n# Save our model\n# save(ln_mod_clean, file=\"ch6_ln_mod_clean.rda\")\n#-----------------------------------------------------------------\nln_mod_clean_sum <- \ntibble::tibble(\n  st_error     = unlist(summary(ln_mod_clean)[6]),\n  r_square     = unlist(summary(ln_mod_clean)[8]),\n  adj_r_square = unlist(summary(ln_mod_clean)[9]),\n  f_stat       = unlist(summary(ln_mod_clean)$fstatistic[1]))\n#-----------------------------------------------------------------\n# Transforming the data\n#-----------------------------------------------------------------\nsummary(pw_mod <- car::powerTransform(ln_mod_clean))\n#> bcPower Transformation to Normality \n#>    Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n#> Y1    3.0383        3.04       2.2665       3.8101\n#> \n#> Likelihood ratio test that transformation parameter is equal to 0\n#>  (log transformation)\n#>                            LRT df       pval\n#> LR test, lambda = (0) 71.19639  1 < 2.22e-16\n#> \n#> Likelihood ratio test that no transformation is needed\n#>                            LRT df       pval\n#> LR test, lambda = (1) 30.40168  1 3.5122e-08\n#-----------------------------------------------------------------\n# Directly transforming the data\n#-----------------------------------------------------------------\nln_mod_log <- \n  lm(log(ticketSales) ~ promotion + daysSinceLastGame + \n                        schoolInOut + weekEnd + team , \n                        data = past_season_clean)\nln_mod_log_sum <- \ntibble::tibble(\n  st_error     = unlist(summary(ln_mod_log)[6]),\n  r_square     = unlist(summary(ln_mod_log)[8]),\n  adj_r_square = unlist(summary(ln_mod_log)[9]),\n  f_stat       = unlist(summary(ln_mod_log)$fstatistic[1])\n)\n#-----------------------------------------------------------------\n# Apply predictions to our test data set. \n#-----------------------------------------------------------------\ntest$pred_tickets <- predict(ln_mod_clean,\n                             newdata = test)\ntest$percDiff <- \n  (test$ticketSales - test$pred_tickets)/test$ticketSales\nmean_test <- mean(test$percDiff)\n#-----------------------------------------------------------------\n# Apply predictions to our test data set. \n#-----------------------------------------------------------------\ntest_line <- test\ntest_line$order <- seq(1:nrow(test))\ntest_line <-   tidyr::pivot_longer(test_line,\n                      cols = c('ticketSales','pred_tickets'),\n                      values_transform = list(val = as.character))\nx_label  <- ('\\n Selected Game')\ny_label  <- ('Ticket Sales \\n')\ntitle   <- ('Ticket forecasts vs. Actuals')\nline_est <- \n  ggplot2::ggplot(data      = test_line, \n                  aes(x     = order,\n                      y     = value,\n                      color = name))             +\n  geom_point(size = 2.5)                         +\n  geom_line()                                    +\n  scale_color_manual(values = palette)           +\n  scale_y_continuous(label = scales::comma)      +\n  xlab(x_label)                                  + \n  ylab(y_label)                                  + \n  ggtitle(title)                                 +\n  graphics_theme_1"},{"path":"chapter6.html","id":"analyzing-a-schedule-and-ranking-games","chapter":"6 Forcasting Sales and Setting Prices","heading":"6.3.2 Analyzing a schedule and ranking games","text":"much impact schedule financial success team? factors considered? sport like professional baseball, ticket sales important component revenue generation. Almost every revenue stream derived sales. Let’s take look schedule data begin frame problem. like take ensemble approach problems. case, going thing two different ways. First, going evaluate games based attractive seem fans. bottom-approach use secondary market purchases estimate attractive game might .","code":""},{"path":"chapter6.html","id":"utilizing-transaction-data","chapter":"6 Forcasting Sales and Setting Prices","heading":"6.3.2.1 Utilizing transaction data","text":"always, let’s begin getting understanding underlying data structure.Table 6.12: Secondary market data structureTable 6.13: Secondary market data structureNow can look game see many transactions happened spent. can join table original season data table see can use regression accurately predict games commanded higher prices secondary market. data wasn’t created linked, going alter little bit. ’re going results actually look realistic.’ll simply create coefficient representing scaled ticket sales add mean secondary price.Now let’s build model data.Table 6.14: Linear model summary statisticsThis model appears work well. appears explain 90% variance secondary market prices. model overfit, get point. Let’s assume put rigor example get application.","code":"\n#-----------------------------------------------------------------\n# Secondary market data, manifest, and sales data\n#-----------------------------------------------------------------\nsm  <- FOSBAAS::secondary_data\nman <- FOSBAAS::manifest_data\nsea <- FOSBAAS::season_data\nsea$gameID <- seq(1:nrow(sea))\n#-----------------------------------------------------------------\n# Secondary market data, manifest, and sales data\n#-----------------------------------------------------------------\nhead(sm)[c(1,2,7,9)]\n#-----------------------------------------------------------------\n# Secondary sales by section\n#-----------------------------------------------------------------\nsm_man <- left_join(sm,man, by = 'seatID')\n\navg_price_comps <- \n  sm_man %>% dplyr::select(gameID,price,singlePrice,tickets) %>%\n             na.omit()                                       %>%\n             dplyr::group_by(gameID)                         %>%\n             dplyr::summarise(meanSec   = mean(price),\n                       meanPri   = mean(singlePrice),\n                       tickets   = sum(tickets))\n#-----------------------------------------------------------------\n# Adjust secondary to reflect primary\n#-----------------------------------------------------------------\nsea_adj  <- left_join(sea,avg_price_comps, by = \"gameID\") \nadj_coef <- scale(sea_adj$ticketSales)\nsea_adj$meanSecAdj <- sea_adj$meanSec + (adj_coef * 10)\n#-----------------------------------------------------------------\n# Adjust secondary to reflect primary\n#-----------------------------------------------------------------\nln_mod_sec <- \n  lm(meanSecAdj ~ promotion + daysSinceLastGame + \n                        schoolInOut + weekEnd + team , \n                        data = sea_adj)\nln_mod_sec_sum <- \ntibble::tibble(\n  st_error     = unlist(summary(ln_mod_sec)[6]),\n  r_square     = unlist(summary(ln_mod_sec)[8]),\n  adj_r_square = unlist(summary(ln_mod_sec)[9]),\n  f_stat       = unlist(summary(ln_mod_sec)$fstatistic[1])\n)"},{"path":"chapter6.html","id":"applying-our-models-to-a-new-data-set","chapter":"6 Forcasting Sales and Setting Prices","heading":"6.3.3 Applying our models to a new data set","text":"Now model constructed, apply new data? ? Let’s evaluate results model create event score can used think promotion schedule. can .new data uses seed2 = 714. type View(f_build_season) can see seed2 controls teams selected schedule. happens use different seed2? might get factor levels didn’t anticipate. model unable make prediction values. . easiest thing remove offending factor levels. also make analogs substitute like-factor.Table 6.15: 2025 season dataNow predictions around ticket sales, can build event scores . simple part.Table 6.16: 2025 season event scoresWe can see groupings games similar levels attractiveness. Although many games appear much attractive others, may also priced higher. Based demand attractiveness, can progressively price games ones highest anticipated demand receiving higher prices.Let’s cluster like games together based event scores. ’ll use kmeans function group games together. ’ll see kmeans getting used quite bit. relatively simple understand good job discriminating numerical data sets.Now let’s look summary stats groupsTable 6.17: Summary stats clusterWe now several groups games can evaluate. can use groups various ways. One way might look games might need help. Perhaps can see promotion impacted sales. ’ll chapter 8. now data set gives good idea attractive game every game. However, pricing much complex practice. Reference pricing plays part might price. main idea going price games based attractive . using attractiveness proxy demand.can improve bottom-forecast? can begin layer data onto . data looking ? several pieces data can use:number season tickets sold anticipate sellingAvailable inventory game allocatedprices paid volume ticket classYou unlikely perfect conditions setting prices. Additionally, model isn’t especially powerful. improve model? practice, prefer ensemble approach. access liquid market, like secondary market high number transactions different prices, can use data.","code":"\n#-----------------------------------------------------------------\n# Create data for a new season\n#-----------------------------------------------------------------\nseason_2025 <- \n  FOSBAAS::f_build_season(seed1 = 755, season_year = 2025,\n  seed2 = 714, num_games = 81, seed3 = 366, num_bbh = 5,\n  num_con = 3, num_oth = 7, seed4 = 366, seed5  = 1,\n  mean_sales = 0, sd_sales = 0\n )\n#-----------------------------------------------------------------\n# Apply model output to new data set\n#-----------------------------------------------------------------\nseason_2025$predTickets <- predict(ln_mod_clean,\n                             newdata = season_2025)\nseason_2025$predPrices  <- predict(ln_mod_sec,\n                             newdata = season_2025)\n#-----------------------------------------------------------------\n# Build event scores\n#-----------------------------------------------------------------\nseason_2025$eventScoreA <- \n  as.vector(scale(season_2025$predTickets) * 100)\nseason_2025$eventScoreB <- \n  as.vector(scale(season_2025$predPrices) * 100)\nseason_2025$eventScore  <- \n   season_2025$eventScoreA + season_2025$eventScoreB\n\nseason_2025 <- season_2025[order(-season_2025$eventScore),]\n#-----------------------------------------------------------------\n# Observe differences in event scores\n#-----------------------------------------------------------------\nlibrary(ggplot2)\nseason_2025$order <- seq(1:nrow(season_2025))\n\nx_label  <- ('\\n Game')\ny_label  <- ('Event Scores \\n')\ntitle   <- ('Ordered event scores')\nline_est_es <- \n  ggplot2::ggplot(data  = season_2025, \n                  aes(x = order,\n                      y = eventScore))             +\n  geom_point(size = 1.3,color = 'dodgerblue')      +\n  geom_line()                                      +\n  scale_color_manual(values = palette)             +\n  scale_y_continuous(label = scales::comma)        +\n  xlab(x_label)                                    + \n  ylab(y_label)                                    + \n  ggtitle(title)                                   +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Kmeans clustering on event scores\n#-----------------------------------------------------------------\nset.seed(715)\nclusters <- kmeans(season_2025$eventScore,6)\nseason_2025$cluster <- clusters$cluster\n#write.csv(season_2025,'season_2025.csv',row.names = F)\n#-----------------------------------------------------------------\n# Summary statistics\n#-----------------------------------------------------------------\nlibrary(dplyr)\nseason_summary <- season_2025                            %>% \n                  group_by(cluster)                      %>%\n                  summarise(mean   = mean(eventScore),\n                            median = median(eventScore),\n                            sd     = sd(eventScore),\n                            n      = n())                %>%\n                  arrange(desc(mean))\nknitr::kable(season_summary,caption = \"Summary stats by cluster\",\n             align = 'c',format = \"markdown\",padding = 0)"},{"path":"chapter6.html","id":"leveraging-qualitative-data","chapter":"6 Forcasting Sales and Setting Prices","heading":"6.4 Leveraging qualitative data","text":"Leveraging surveys pricing support may may utility. two main methodologies deployed practice:Van Westendorp analysisConjointBoth techniques supported survey tools Qualtrics. Conjoint analysis can complex understanding well outside scope book. Let’s take look Van Westendorp survey talk can useful.First, need create survey data.Table 6.18: Survey resultsLike many statistical tools find, package analyzing Van Westendorp data. Let’s start little manually. ’ll little prep work. Let’s build new dataframe slightly different terminology.Ecdf function Hmisc package give coordinates cumulative distribution. ’ll use reshape2 (Wickham 2020) package melt data frame. package helps pivot data results identical using tools tidyr package already seen.Now ready view graph.can also use pricesensitivitymeter package produce analysis little quickly.price_sensitivity object makes easy explore data.Table 6.19: Price sensitivity metricsYou can also produce plot already produced. plotting mechanism leverages ggplot2, can override look. now qualitative data price sensitivity. doesn’t represent willingness pay, help us conceptualize range prices might appropriate.","code":"\n#-----------------------------------------------------------------\n# Create Van Westendorp survey data\n#-----------------------------------------------------------------\nvw_data <- data.frame(matrix(nrow = 1000, ncol = 6))\nnames(vw_data) <- c('DugoutSeats', 'PriceExpectation', \n                    'TooExpensive', 'TooCheap', \n                    'WayTooCheap', 'WayTooExpensive')\nset.seed(715)\nvw_data[,1] <- 'DugoutSeats'\nvw_data[,2] <- round(rnorm(1000,100,10),0)\nvw_data[,3] <- round(rnorm(1000,130,20),0)\nvw_data[,4] <- round(rnorm(1000,60,15),0)\nvw_data[,5] <- round(rnorm(1000,50,10),0)\nvw_data[,6] <- round(rnorm(1000,160,20),0)\n#-----------------------------------------------------------------\n# Empirical cumulative distribution function\n#-----------------------------------------------------------------\nlibrary(Hmisc)\ndat <- data.frame(\n    \"toocheap\"     = vw_data$WayTooCheap,\n    \"notbargain\"   = vw_data$TooExpensive,\n    \"notexpensive\" = vw_data$TooCheap,\n    \"tooexpensive\" = vw_data$WayTooExpensive\n)\na <- Ecdf(dat$toocheap,what=\"1-F\",pl=F)$y[-1]\nb <- Ecdf(dat$notbargain, pl=F)$y[-1]\nc <- Ecdf(dat$notexpensive,what = \"1-F\", pl=F)$y[-1]\nd <- Ecdf(dat$tooexpensive,pl=F)$y[-1]\n#-----------------------------------------------------------------\n# Build data set for creating graphic\n#-----------------------------------------------------------------\nlibrary(reshape2)\necdf1 <- data.frame(\n  \"variable\"  = rep(\"toocheap\",length(a)),\n  \"ecdf\"      = a,\n   \"value\"    = sort(unique(dat$toocheap)))\necdf2 <- data.frame(\n  \"variable\"  = rep(\"notbargain\",length(b)),\n  \"ecdf\"      = b,\n  \"value\"     = sort(unique(dat$notbargain),decreasing = T))\necdf3 <- data.frame(\n  \"variable\"  = rep(\"notexpensive\",length(c)),\n   \"ecdf\"     = c,\n   \"value\"    = sort(unique(dat$notexpensive),decreasing = T))\necdf4 <- data.frame(\n  \"variable\"  = rep(\"tooexpensive\",length(d)),\n  \"ecdf\"      = d,\n  \"value\"     = sort(unique(dat$tooexpensive)))\ndat2 <- rbind(ecdf1,ecdf2,ecdf3,ecdf4)\ndat  <- melt(dat)\ndat  <- merge(dat,dat2,by=c(\"variable\",\"value\"))\n#-----------------------------------------------------------------\n# Graph the results\n#-----------------------------------------------------------------\nrequire(ggplot2)\nrequire(scales)\nrequire(RColorBrewer)\n\nPaired     <- RColorBrewer::brewer.pal(4,\"Paired\")\n\ng_xlab     <- '\\n prices'\ng_ylab     <- 'Responses  \\n'\ng_title    <- 'Dugout Price Value Perception\\n'\n\nvw_gaphic <- \nggplot(dat, aes(value, ecdf, color=variable))     + \n      annotate(\"rect\", xmin = 55, xmax = 146, \n                     ymin = 0,  ymax = 1,\n             alpha = .4, fill = 'coral')          +\n    geom_line(size = 1.2) +\n    scale_color_manual(values = Paired,\n                       name = 'Value Perception') + \n    xlab(g_xlab)                                  + \n    ylab(g_ylab)                                  + \n    ggtitle(g_title)                              + \n    scale_y_continuous(labels = percent)          +\n    scale_x_continuous(labels = dollar)           +\n    coord_cartesian(xlim = c(0,220),ylim= c(0,1)) +\n    geom_hline(yintercept = .5,\n               lty=4,\n               alpha = .5)                        +\n    graphics_theme_1\n#-----------------------------------------------------------------\n# Duplicate work with a library\n#-----------------------------------------------------------------\nlibrary(pricesensitivitymeter)\nprice_sensitivity <- psm_analysis(\n                           toocheap        = \"WayTooCheap\",\n                           cheap           = \"TooCheap\",\n                           expensive       = \"TooExpensive\",\n                           tooexpensive    = \"WayTooExpensive\",\n                           data            = vw_data,\n                           validate        = TRUE)\n#-----------------------------------------------------------------\n# Explore price sensitivity data\n#-----------------------------------------------------------------\nps <- \n  tibble::tibble(lower_price = price_sensitivity$pricerange_lower,\n                 upper_price = price_sensitivity$pricerange_upper)"},{"path":"chapter6.html","id":"implementing-revenue-management-strategies-in-sports","chapter":"6 Forcasting Sales and Setting Prices","heading":"6.5 Implementing revenue management strategies in sports","text":"techniques becoming increasingly commoditized. mechanisms forecasting pricing well understood widely deployed. Additionally, ticketing platforms markets already working together make deploying changes possible across multiple markets instantaneously. already covered main considerations, didn’t talk :Brand consumer equitySetting initial pricesAutomatically adjusting pricesControlling cannibalizationPublic relations perceptionThe perceived fairness raising pricesLowering prices can negatively impact perception brand equity long term. Concepts prospect theory explain changes utility asymmetric gains losses. (Tudor Bodea 2012) someone feels missed pricing policy negative impacts. Additionally, price changes may perceived fair. keep behavioral economics mind deploy pricing strategy.Initial prices difficult set reference prices powerful mechanisms. develop new product (premium seating area), ’ll think carefully value proposition. -market competitive intelligence may useful . Don’t rely pro forma.evaluate prices deploy . best set internal mechanism sales, marketing, operations. Pricing one single-important sales revenue influencers. ’ll want right people table. ’ll also want make sure right technology deployed. price get market? updated website? policies much little specific tickets may cost?Additionally, inventory control protocols may influence decisions. raise prices one section, impact demand another? fundamental component pricing. price levels linked. might able explain links systems equations. multiple ways approach problems.Finally, people respond prices? depend number factors demand, success, markets. Exploiting opportunities isn’t always appropriate. Qualitative research may help. can deploy conjoint surveys research tools help gauge fans respond products changes price. Taking ensemble approach pricing recommended approach.","code":""},{"path":"chapter6.html","id":"key-concepts-and-chapter-summary-5","chapter":"6 Forcasting Sales and Setting Prices","heading":"6.6 Key concepts and chapter summary","text":"Getting pricing right key component integrated marketing mix strategy. Pricing number quantitative qualitative components. covered several concepts:Inventory controlMechanical price controlsMore -depth regressionForecasting schedule analysisEvent scoringQualitative researchWe skimmed broad subject, know understand basics complex subject.Inventory can complex people may perceive inventory different ways. Understanding differences inventory important building discreet-choice model.mechanics determining prices tend rely mathematical models. models can adjusted promote sales maximize revenue generation. goals may inclusive exclusive.Regression can rigorous. want confident results, ’ll need spend time evaluating model methodical way. Get book regression make sure understand enough use correctly.Schedules can play -sized role revenue generation. walked evaluate schedule. useful marketing wants build promotions games less demand.covered use Van Westendorp analysis help us understand people perceive inventory prices.","code":""},{"path":"chapter7.html","id":"chapter7","chapter":"7 Lead Scoring","heading":"7 Lead Scoring","text":"told pay $50,000 sold four season tickets someone next twenty-four hours? ? first thing ask much tickets cost. less $50,000 know always just buy tickets collect surplus. ways increase likelihood sold four season tickets?call ticket broker ask buy tickets.look lapsed purchasersI look abandoned carts websiteI call individuals yet renew season ticketsI beg family friends purchaseThere lots tactics deploy make sure picking low-hanging fruit. unlikely ever get point simply pick telephone begin calling phone numbers. way us approach problem analytically?Lead scoring fundamental sales campaigns. Qualifying leads can done many ways, goal always . ordering leads efficient way sales efforts can maintain greater level efficiency. Warm leads critical someone hasn’t interacted brand, likely less efficient use time. Working marketing always reminds one-hundred thirty year old quote:“Half money spend advertising wasted; trouble don’t know half.”— John WannamkerLead scoring assist evaluating direct sales efforts help maximize ROI context. However, several considerations. evaluate lead?Customer lifetime Value?likelihood purchase sales cycleRecent, frequency, monetary valueThis might also judgment call predicated sales marketing teams incentivized. Always consider compensation packages. Compensation designed incentivize behavior. However, may optimized terms achieving organizational goals.","code":""},{"path":"chapter7.html","id":"recency-frequency-and-monetary-value","chapter":"7 Lead Scoring","heading":"7.1 Recency, frequency, and monetary value","text":"RFM scores described poor-man’s analytic technique. basic premise score sales candidates along three dimensions build lists comprising cohorts highest aggregate scores. might work practice?Let’s put together ad hoc data set demonstrate RFM scores might work.added three columns customer file:long last interaction happened daysHow many interaction happenedHow much spent customerTable 7.1: Prepped RFM dataThere many ways apply scores . take similar approach scoring events, add one extra step. RFM scores traditionally built one five five highest score.Now can apply function data set. generalize wanted. also use apply function. sort exercise, aren’t concerned extreme efficiency. just want demonstrate mechanics. ’ll use lists loops accomplish task. repeat exercise often, daisy chaining loops best practice. Try copy paste three times. ’ll make errors.Now customer RFM score can used build campaigns. Let’s subset group campaign. ’ll select customers interacted recently, frequently, tended spend .Table 7.2: Top prospects campaignRFM scores can useful segmentation schemes lead scoring. However, better ways accomplish goal. Lead scoring close get book discussing Customer Relationship Management (CRM). also fundamental component CRM. following examples take sophisticated example take advantage analytics framework make regression machine learning easier.","code":"\n#-----------------------------------------------------------------\n# RFM data\n#-----------------------------------------------------------------\nlibrary(dplyr)\ndemo_data <- FOSBAAS::demographic_data[,c(1,4)]\nset.seed(44)\ndemo_data <- demo_data %>%\nmutate(\nlastInteraction = abs(round(rnorm(nrow(demo_data),50,30),0)),\ninteractionsYTD = abs(round(rnorm(nrow(demo_data),10,5),0)),\nlifetimeSpend   = abs(round(rnorm(nrow(demo_data),10000,7000),0))\n)\n#-----------------------------------------------------------------\n# Function to calculate FRM scores\n#-----------------------------------------------------------------\ndemo_data$Recency       <- -scale(demo_data$lastInteraction)\ndemo_data$Frequency     <- scale(demo_data$interactionsYTD)\ndemo_data$MonetaryValue <- scale(demo_data$lifetimeSpend)\n# Produce quantiles for each scaled value\nr_quant <- unname(quantile(demo_data$Recency,\n                           probs = c(.2,.4,.6,.8)))\nf_quant <- unname(quantile(demo_data$Recency,\n                           probs = c(.2,.4,.6,.8)))\nm_quant <- unname(quantile(demo_data$Recency,\n                           probs = c(.2,.4,.6,.8)))\n# Function to evaluate RFM score\nf_create_rfm <- function(quantList,number){\n  \n  if(number <= quantList[[1]]){'1'}\n    else if(number <= quantList[[2]]){'2'}\n      else if(number <= quantList[[3]]){'3'}\n        else if(number <= quantList[[4]]){'4'}\n          else{'5'}\n}\n#-----------------------------------------------------------------\n# Create final RFM values\n#-----------------------------------------------------------------\nvalue <- list()\nj     <- 1\nfor(i in demo_data$Recency){\n  value[j] <- f_create_rfm(r_quant,i)\n  j <- j + 1\n}\ndemo_data$r_val <- unlist(value)\n#-----------------------------------------------------------------\nvalue <- list()\nj     <- 1\nfor(i in demo_data$Frequency){\n  value[j] <- f_create_rfm(f_quant,i)\n  j <- j + 1\n}\ndemo_data$f_val <- unlist(value)\n#-----------------------------------------------------------------\nvalue <- list()\nj     <- 1\nfor(i in demo_data$MonetaryValue){\n  value[j] <- f_create_rfm(m_quant,i)\n  j <- j + 1\n}\ndemo_data$m_val <- unlist(value)\n#-----------------------------------------------------------------\n\ndemo_data$RFM <- paste(demo_data$r_val,\n                       demo_data$f_val,\n                       demo_data$m_val, sep = '')\n#-----------------------------------------------------------------\n# Create final RFM values\n#-----------------------------------------------------------------\ntop_prospects <- subset(demo_data,demo_data$RFM == '555')"},{"path":"chapter7.html","id":"scoring-season-ticket-holders-on-their-liklihood-to-renew","chapter":"7 Lead Scoring","heading":"7.2 Scoring season ticket holders on their liklihood to renew","text":"advancements hardware software, art lead scoring become bit commodity. can leverage several techniques including: random forests, gradient boosting, logistic regression, even deep learning tool Tensorflow without getting penalized time costs. Take minute appreciate fact. amazing. Amazon, Google, others making process even better AWS GCP. example demonstrate machine-learning applied real world problem. ’ll frame problem around common question asked every year every club sports.ticket sales service manager like way understand accounts less likely renew season tickets.’ll use mlr3 library (Bischl et al. 2021) demonstrate couple different machine learning algorithms. package mlr3 similar caret (Kuhn 2022) refactored tidymodels (Kuhn Wickham 2022). Caret great library sought create unifying API scores R libraries. ’ll look tidymodels next chapter. , mlr3 reminiscent scikit-learn Python. like using python analysis, mlr3 feel familiar.can call functions directly, library makes process much easier. lack consistent frameworks R one languages biggest drawbacks. Additionally, mlr3 excellent documentation 66.Using framework also comes issues.’ll learn , adds complexity task.Errors (especially beta versions) can frustrating trouble-shootThey can work slowly libraries used taskHowever, believe ’ll better long run leverage framework. tend make basic tasks machine-learning much systematic repeatable, especially comes benchmarking.Let’s also discuss package data.table (Dowle Srinivasan 2021) . data.table powerful tool underlies many popular packages. many uses, can confusing work opposed dplyr (Wickham, François, et al. 2022). can softly considered one pillars holds much R universe. using python, datatable remind pandas (team 2020). working really large data sets, datatable useful learn.","code":""},{"path":"chapter7.html","id":"implementing-a-lead-scoring-project","chapter":"7 Lead Scoring","heading":"7.2.1 Implementing a lead scoring project","text":"random forest excellent tool classification can forecast two classes. Logistic regression typically used binary classes (renewed, renew) probably first step. forms logistic regression handle multi-class problems. ’ll take look tools. practice, random forest tends handle wide variety problems faced club well.Missing data orphan cases can make tasks extremely frustrating. guarantee model converge. stated earlier time spent putting data order. ’ll much happier put hours getting data proper spot. fun part exercises modeling. However, ends part spend smallest amount time working . hope high worth lows .Additionally, follow process outlined chapter 4. ’ll demonstrate isn’t managerial B.S. Structuring projects critical working teams working distributed fashion. Let’s remind basic steps want follow:Define measurable goal hypothesisData collectionModel dataEvaluate resultsCommunicate resultsDeploying results","code":""},{"path":"chapter7.html","id":"defining-our-goal","chapter":"7 Lead Scoring","heading":"7.2.1.1 Defining our goal","text":"going use multiple years season ticket holder renewal data. also problem statement:don’t understand identify season ticket accounts less likely renew.output score can used compare customers one another. ’ll need look features might predict whether someone likely renew tickets. might ticket usage tenure. don’t know.rub don’t know can help improve prospects someone might renew. levers can pull? one thing build useful model predicts renewals finance, trying impact renewals sales. ’ll need think carefully insight gather. Perhaps find something didn’t know looking . perspective, success can gauged several ways.","code":""},{"path":"chapter7.html","id":"understanding-the-data-set","chapter":"7 Lead Scoring","heading":"7.2.1.2 Understanding the data set","text":"data set includes several features including account, whether renewed, several features related season tickets. data located FOSBASS package. can see data created section 2.2.data set contains several years data. looks like something find club going behave appropriately. Let’s take look structure data:Table 7.3: Dataset evaluating likelihood renewThis data isn’t particularly complex. can see several columns appear useful. data set also already clean, let’s refer section 4.3.1 chapter 4 make sure covering bases. know data good shape, still deal major issues face running operation data.sort analysis mechanism useful solve problem?data structured formatted way can use ?missing values systematic issues cause problem?Question one easy. can refer chart section 4.4. clearly example calculating future, numerical values.question two, different analysis techniques require different formats. can see couple columns categorical. Let’s go ahead create two data sets. ’ll dummy code categorical data one data set contains numerical values.Table 7.4: Numeric data set regressionFor question three, can little preparation standard data set. ’ll get rid couple columns won’t use turn response variable factor. know isn’t missing data, skip part. already covered deal .terms prep work, ready go. need dwell . chapter designed cover data modeling detail, isn’t important anyway. also didn’t really worry data collection. data might interesting ?Additionally, like briefly cover special topic deserves lot attention give. Maps. Geography important sales, marketing, corporate sponsorship groups, others. context, ’ll see geography impact sales marketing strategy along chapter.","code":"\n#-----------------------------------------------------------------\n# access renewal data\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\nlibrary(dplyr)\nmod_data <- FOSBAAS::customer_renewals\n#-----------------------------------------------------------------\n# Dummy code and alter data frame for all numeric input\n#-----------------------------------------------------------------\nd1 <- as.data.frame(psych::dummy.code(mod_data$corporate))\nd2 <- as.data.frame(psych::dummy.code(mod_data$planType))\n\nmod_data_numeric <- dplyr::select(mod_data,ticketUsage,\n                                        tenure,spend,tickets,\n                                        distance,renewed) %>%\n                    dplyr::bind_cols(d1,d2)\n#-----------------------------------------------------------------\n# Prepare the data for analysis\n#-----------------------------------------------------------------\nmod_data$renewed   <- factor(mod_data$renewed)\nmod_data$accountID <- NULL\nmod_data$season    <- NULL"},{"path":"chapter7.html","id":"understanding-geography-and-building-maps","chapter":"7 Lead Scoring","heading":"7.2.1.2.1 Understanding geography and building maps","text":"Geography plays large role selling tickets sporting events. especially true sports games baseball. Coupling digital physical assets club may eventually make less important holistically. However, ticket sales geography remain critical point consideration.R also isn’t best way , easy way accomplish analysis platform. GIS large field unto outside scope book. However, R multiple libraries devoted maps good choice aren’t looking extremely high quality geographic graphics. map created using stamen maps67, open source tool can used wide variety projects.maps aren’t best, easy execute can give much insight get sophisticated product ArcGIS 68. R also makes easy access google APIs allow Geocode addresses perform many interesting tasks geographic data. can find multitude informative demos online.Using county names can create maps look like following. one took lot data wrangling uses maps (Brownrigg 2021) package, output looks nice. underlying data identical.","code":"\n#-----------------------------------------------------------------\n# Using R to visualize geographic data\n#-----------------------------------------------------------------\nlibrary(ggmap)\nlibrary(ggplot2)\n\ndemos <- FOSBAAS::demographic_data\ndemos <- demos[sample(nrow(demos), 5000), ]\n\nmap_data <- subset(demos,demos$longitude >= -125 & \n                   demos$longitude <= -67 &\n                   demos$latitude >= 25.75 & \n                   demos$latitude <= 49)\nus <- c(left = -91, bottom = 32, right = -80, top = 38)\nmap <- get_stamenmap(us, zoom = 6, maptype = \"toner-lite\") %>% \n       ggmap() \n\ngeographic_vis <- \n  map +\n  geom_point(data = map_data, \n             mapping = aes(x = longitude, y = latitude),\n             size = .2,alpha = .5, color= 'dodgerblue')\n#-----------------------------------------------------------------\n# Using R to visualize geographic data\n#-----------------------------------------------------------------\nlibrary(maps)\n\nfips <- \nmaps::county.fips %>%\n  as_tibble       %>% \n  extract(polyname, \n          c(\"region\", \"subregion\"), \"^([^,]+),([^,]+)$\") \n\ncounty_data <- \nggplot::map_data(\"county\") %>% \n  left_join(fips)          %>%\nmutate(county = paste(region,subregion, sep = ',')) \n\n\ncustomers <- left_join(customers,county_data, by = 'county')\n\ncounts    <- customers %>% group_by(county) %>%\n                           summarise(accounts = n())\n\ncounty_data <- county_data %>% left_join(counts,by='county') \n\nstate_data           <- map_data(\"state\") \nmain_color           <- 'steelblue4'\ncounty_data$accounts <- ifelse(is.na(county_data$accounts) == T,\n                               1,\n                               county_data$accounts)\n\n# adjust number of accounts per county\ncorrection <- county_data %>% select(county,accounts)                       %>%\n                              group_by(county)                              %>%\n                              mutate(adjustment = n())                      %>%\n                              unique()                                      %>%\n                              summarise(adj_accounts = accounts/adjustment) %>%\n                              select(county,adj_accounts)\n\ncorrection$adj_accounts <- ifelse(correction$adj_accounts <= 1,\n                                  1,\n                                  correction$adj_accounts)\n\ncounty_data <- county_data %>% left_join(correction, by = 'county')\n\nregion_map <- \n      \n  county_data %>% \n  ggplot(aes(long, lat, group = group))                                 +\n  geom_polygon(aes(fill=log10((adj_accounts))), \n               color=main_color,size = 0)                               +\n  geom_polygon(data = state_data ,color=\"white\",\n               size = .01,fill = 'transparent')                         +\n  coord_map(xlim = c(-91,-79), ylim = c(31,38.5))                       +\n #scale_fill_viridis(option=\"inferno\")                                  + \n  scale_fill_gradient(low = main_color ,high = 'white')                 +\n  labs( x = '', y ='',\n        title = \"Nashville ticket purchasers\",\n        subtitle = \"Total accounts by county\",\n        caption  = \"purchaser density\")                                 +\n   theme(plot.caption = element_text(hjust = 0, \n                                     face= \"italic\",color = \"grey90\"),\n         plot.title.position = \"plot\", \n         plot.caption.position =  \"plot\",\n         plot.subtitle = element_text(color = \"grey90\"))                +\n  graphics_theme_1                                                      +\n  theme(rect = element_rect(fill = main_color ),\n        axis.text.x  = element_blank(),  \n        axis.text.y  = element_blank(),\n        panel.grid.major  = element_line(colour = main_color ),  \n        panel.grid.minor  = element_line(colour = main_color ), \n        panel.background  = element_rect(fill = main_color , \n                                         colour = main_color ), \n        plot.background   = element_rect(fill = main_color , \n                                         colour = main_color ),\n        legend.position=\"none\")"},{"path":"chapter7.html","id":"model","chapter":"7 Lead Scoring","heading":"7.2.1.3 Model the data","text":"use mlr3 framework , important understand need use one frameworks. can call functions directly. mlr3 descended mlr, (Bischl et al. 2021) built modern framework. ’ll want begin downloading library poking around 69.downloading packages need can go ahead get started task modeling data. know want determine likely specific season ticket holder renew. Make sure data set doesn’t contain characters. ’ll need dummy code change factors.classification problem mlr3 follows specific pattern building model. data placed object called task:input straight forward. name object, tell data use, give target column, result. case, “r” indicates account renewed past.task, want select learner like use. MLR works several libraries. case like apply random forest data going use ranger (Wright, Wager, Probst 2022) package. Ranger powerful library includes lots tools can leverage build random forest models. can add parameters ranger function library.can now build test training data set. reminder, also use three-way data partition calibration data set. However, isn’t really necessary.Training model simple. ’ll use learner object point task rows identified training data set.learner contains several useful pieces data generated. can access different ways.Table 7.5: Learner outputThis model doesn’t appear great job judging prediction error. Let’s take look holdout sample:several ways consider accurate model might interpretation can confusing. first thing look confusion matrix. confusion matrix simple way gauge well model predictions performed known values.confusion matrix demonstrates often model correct incorrect response. case, model isn’t performing well. perform better random guess? many metrics can extract model.case, model 0.8208345 percent accurate. Let’s visualize results.simple models, mlr3::autoplot() function includes several graphs built ggplot2, means can apply themes . First, let’s explore data little . can access predictions command learner_ranger_rf\\(model\\)predictions. probabilities may come useful depending .can compare models one another couple different ways. compare random forest model model simply predicts classifier. doesn’t use parameters fed model.classification error better random forest, large amount. classification errors resampling rerunning models.can also see generate lift model looking ROC curve.data? can now use model predict likely person renew ticket. However, can see model doesn’t perform well like. options terms proceding:Tune model parameters try improve itTry different modelGet dataLet’s try couple different models attempt tune models improve accuracy.","code":"\n#-----------------------------------------------------------------\n# Download and install the mlr3 libraries\n#-----------------------------------------------------------------\nlibrary(\"mlr3\")         #  install.packages(\"mlr3viz\")\nlibrary(\"mlr3learners\") #  install.packages(\"mlr3learners\")\nlibrary(\"mlr3viz\")      #  install.packages(\"mlr3viz\")\nlibrary(\"mlr3tuning\")   #  install.packages(\"mlr3tuning\")\nlibrary(\"paradox\")      #  install.packages(\"paradox\")\n#-----------------------------------------------------------------\n# Recode response to a factor\n#-----------------------------------------------------------------\nmod_data_numeric$renewed <- factor(mod_data_numeric$renewed)\n#-----------------------------------------------------------------\n# Build task\n#-----------------------------------------------------------------\ntask_mod_data <- TaskClassif$new(id       = \"task_renew\", \n                                 backend  = mod_data_numeric, \n                                 target   = \"renewed\", \n                                 positive = \"r\")\n# Add task to the task dictionary\nmlr_tasks$add(\"task_renew\", task_mod_data)\n#-----------------------------------------------------------------\n# Define learner\n#-----------------------------------------------------------------\nlearner_ranger_rf <- lrn(\"classif.ranger\",\n                         predict_type = \"prob\",\n                         mtry         = 3,\n                         num.trees    = 500)\n# Check parameters with this: learner_ranger_rf$param_set$ids()\n# look at a list of learners: mlr3::mlr_learners\n#-----------------------------------------------------------------\n# Build test and training data set\n#-----------------------------------------------------------------\nset.seed(44)\ntrain_mod_data <- sample(task_mod_data$nrow, \n                         0.75 * task_mod_data$nrow)\ntest_mod_data  <- setdiff(seq_len(task_mod_data$nrow), \n                          train_mod_data)\n#-----------------------------------------------------------------\n# Train the model\n#-----------------------------------------------------------------\nlearner_ranger_rf$train(task    = task_mod_data, \n                        row_ids = train_mod_data)\n#-----------------------------------------------------------------\n# Inspect the results\n#-----------------------------------------------------------------\n# print(learner_ranger_rf$model)\nlearner_output <- \ntibble::tibble(\n    numTrees  = unname(learner_ranger_rf$model$num.trees),\n    trys      = unname(learner_ranger_rf$model$mtry),\n    samples   = unname(learner_ranger_rf$model$num.samples),\n    error     = unname(learner_ranger_rf$model$prediction.error)\n)\n#-----------------------------------------------------------------\n# Evaluate holdout sample\n#-----------------------------------------------------------------\nprediction <- learner_ranger_rf$predict(task_mod_data, \n                                        row_ids = test_mod_data)\n#-----------------------------------------------------------------\n# Confusion matrix\n#-----------------------------------------------------------------\nprediction$confusion\n#>         truth\n#> response    r   nr\n#>       r  2662  508\n#>       nr  106  151\n#-----------------------------------------------------------------\n# Evaluate holdout sample\n#-----------------------------------------------------------------\nmeasure = msr(\"classif.acc\")\nprediction$score(measure)\n#> classif.acc \n#>   0.8208345\n#-----------------------------------------------------------------\n# Access model predictions\n#-----------------------------------------------------------------\nprobs <- as.data.frame(learner_ranger_rf$model$predictions)\nhead(probs)\n#>           r         nr\n#> 1 0.5376683 0.46233166\n#> 2 0.9015481 0.09845190\n#> 3 0.9252880 0.07471201\n#> 4 0.9603436 0.03965638\n#> 5 0.9370835 0.06291653\n#> 6 0.9028345 0.09716551\n#-----------------------------------------------------------------\n# Evaluate holdout sample\n#-----------------------------------------------------------------\n\ntasks        <- tsks(c(\"task_renew\"))\nlearner      <- lrns(c(\"classif.featureless\",\"classif.rpart\"),\n                     predict_type = \"prob\")\nresampling   <- rsmps(\"cv\")\nobject       <- benchmark(benchmark_grid(tasks, \n                                         learner, \n                                         resampling))\n# Use head(fortify(object)) to see the ce for the resamples\n#-----------------------------------------------------------------\n# Boxplot of classification error\n#-----------------------------------------------------------------\nbplot <- \nautoplot(object)   +\n  graphics_theme_1 +\n  geom_boxplot(fill = 'dodgerblue') \n#-----------------------------------------------------------------\n# ROC curve for random forest model\n#-----------------------------------------------------------------\nroc_model <- \nautoplot(object$filter(task_ids = \"task_renew\"), \n         type = \"roc\") +\n  graphics_theme_1 +\n  scale_color_manual(values = palette)"},{"path":"chapter7.html","id":"resampling","chapter":"7 Lead Scoring","heading":"7.2.1.3.1 Resampling","text":"Conceptually, last model validated one training set data one test set data. Occasionally ’ll see three-way partition data train, test, calibration set, often times just train test set randomly sampled entire data set. many cases OK, lead confidence model warranted. Resampling strategies help calibrate model give confidence accuracy. can visualize mean resampling figure 7.1.\nFigure 7.1: Resampling concept\ncommon method find 10-fold cross validation learning procedure executed 10 times different training sets (overlap) 10 error estimates averaged yield overall error estimate (Ian H. Witten 2011). Leveraging framework mlr3 makes efforts (resampling tuning) much easier. Let’s look example cross validation regards model already constructed.task object learner objects identical saw previous sections.However, need add resampling object:’ll need instantiate resampling strategy. just means going create .can now call resample. take longer seen building model multiple data sets.now confident way evaluate model.classification error improved. great! improve .","code":"\n#-----------------------------------------------------------------\n# Rebuild model\n#-----------------------------------------------------------------\ntask_mod_data <- TaskClassif$new(id       = \"task_renew\", \n                                 backend  = mod_data_numeric, \n                                 target   = \"renewed\", \n                                 positive = \"r\")\n\nlearner_ranger_rf$train(task_mod_data, row_ids = train_mod_data)\n#-----------------------------------------------------------------\n# Add a resampliing parameter\n#-----------------------------------------------------------------\nresampling_mod_data  <- rsmp(\"cv\")\n#-----------------------------------------------------------------\n# Rebuild model\n#-----------------------------------------------------------------\nresampling_mod_data$instantiate(task_mod_data)\nresampling_mod_data$iters\n#> [1] 10\n#-----------------------------------------------------------------\n# We can now call the resample object\n#-----------------------------------------------------------------\nresamp <- resample(task_mod_data, \n                   learner_ranger_rf, \n                   resampling_mod_data, \n                   store_models = TRUE)\n#> INFO  [19:21:52.308] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/10)\n#> INFO  [19:21:56.179] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 2/10)\n#> INFO  [19:22:00.094] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 3/10)\n#> INFO  [19:22:04.577] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 4/10)\n#> INFO  [19:22:08.117] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 5/10)\n#> INFO  [19:22:11.883] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 6/10)\n#> INFO  [19:22:15.883] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 7/10)\n#> INFO  [19:22:19.594] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 8/10)\n#> INFO  [19:22:23.224] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 9/10)\n#> INFO  [19:22:27.054] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 10/10)\n#-----------------------------------------------------------------\n# Rebuild model\n#-----------------------------------------------------------------\nresamp$aggregate(msr(\"classif.ce\"))\n#> classif.ce \n#>  0.1794095\n# Look at scores from the models: resamp$score(msr(\"classif.ce\"))"},{"path":"chapter7.html","id":"optimizing-your-model","chapter":"7 Lead Scoring","heading":"7.2.1.3.2 Optimizing your model","text":"Many algorithms parameters can alter performance model. instance, random forest can different numbers trees applied model. know tuning algorithm appropriately?Let’s take look different parameters available ranger package. 29 , look first .Let’s select couple parameters tuneWe also consider resampling strategy. Since cross-validated results demonstrated similar error rates, let’s use holdout sample. ’ll use classification error measure.Let’s put everything together tune instance pass new criteria model.Now determine search different parameters search. ’ll use random search. means levels randomly searched within parameters set.Now can take look best resultThe best results came minimum node size {r tune_instance$result_learner_param_vals$min.node.size max depth 4.Tuning improved model. 0.0097881 percent. Now can apply model back test data observe results.three-part data partition useful. use calibrate results. Let’s take look new predictions look.new model fair job discriminating renewed renew.certainly improved accuracy. Now let’s take look comparing model produced random forest model produced different algorithm.","code":"\n#-----------------------------------------------------------------\n# Parameter set\n#-----------------------------------------------------------------\nparam_set <- as.data.frame(learner_ranger_rf$param_set$ids())\nhead(param_set)\n#>   learner_ranger_rf$param_set$ids()\n#> 1                             alpha\n#> 2            always.split.variables\n#> 3                     class.weights\n#> 4                           holdout\n#> 5                        importance\n#> 6                        keep.inbag\n#-----------------------------------------------------------------\n# Tuning parameters\n#-----------------------------------------------------------------\ntune_rf_params <- ParamSet$new(list(\n  ParamInt$new(\"min.node.size\", lower = 10, upper = 200),\n  ParamInt$new(\"max.depth\",     lower = 2,  upper = 20),\n  ParamInt$new(\"num.trees\",     lower = 500,  upper = 600)\n))\n#-----------------------------------------------------------------\n# set resampling and eval parameters\n#-----------------------------------------------------------------\nresamp_strat     <- rsmp(\"holdout\")\nmeasure_mod_data <- msr(\"classif.ce\")\nevals_10         <- trm(\"evals\", n_evals = 10)\n#-----------------------------------------------------------------\n# Build a tuning instance\n#-----------------------------------------------------------------\ntune_instance <- TuningInstanceSingleCrit$new(\n  task         = task_mod_data,\n  learner      = learner_ranger_rf,\n  resampling   = resamp_strat,\n  measure      = measure_mod_data,\n  search_space = tune_rf_params,\n  terminator   = evals_10\n)\n#-----------------------------------------------------------------\n# Randomly select options within tuning min and max\n#-----------------------------------------------------------------\ntuner_rf = tnr(\"random_search\")\n#-----------------------------------------------------------------\n# Run the models\n#-----------------------------------------------------------------\ntuner <- tuner_rf$optimize(tune_instance)\n#> INFO  [19:22:42.586] [bbotk] Starting to optimize 3 parameter(s) with '<OptimizerRandomSearch>' and '<TerminatorEvals> [n_evals=10, k=0]'\n#> INFO  [19:22:42.613] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [19:22:42.663] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [19:22:42.672] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [19:22:44.708] [mlr3] Finished benchmark\n#> INFO  [19:22:44.823] [bbotk] Result of batch 1:\n#> INFO  [19:22:44.824] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [19:22:44.824] [bbotk]            123        18       542   0.175093        0\n#> INFO  [19:22:44.824] [bbotk]  errors runtime_learners\n#> INFO  [19:22:44.824] [bbotk]       0             2.03\n#> INFO  [19:22:44.824] [bbotk]                                 uhash\n#> INFO  [19:22:44.824] [bbotk]  a7f71e6e-0279-49e3-9392-bf0e913576d5\n#> INFO  [19:22:44.827] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [19:22:44.842] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [19:22:44.846] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [19:22:46.368] [mlr3] Finished benchmark\n#> INFO  [19:22:46.389] [bbotk] Result of batch 2:\n#> INFO  [19:22:46.390] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [19:22:46.390] [bbotk]            200        17       507  0.1726855        0\n#> INFO  [19:22:46.390] [bbotk]  errors runtime_learners\n#> INFO  [19:22:46.390] [bbotk]       0             1.52\n#> INFO  [19:22:46.390] [bbotk]                                 uhash\n#> INFO  [19:22:46.390] [bbotk]  2b0ee838-3f99-4758-b825-865f826197d6\n#> INFO  [19:22:46.393] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [19:22:46.409] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [19:22:46.413] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [19:22:47.927] [mlr3] Finished benchmark\n#> INFO  [19:22:47.948] [bbotk] Result of batch 3:\n#> INFO  [19:22:47.949] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [19:22:47.949] [bbotk]            120        11       508  0.1739987        0\n#> INFO  [19:22:47.949] [bbotk]  errors runtime_learners\n#> INFO  [19:22:47.949] [bbotk]       0              1.5\n#> INFO  [19:22:47.949] [bbotk]                                 uhash\n#> INFO  [19:22:47.949] [bbotk]  78ecf0db-b266-4fec-8e91-4ecf3325403f\n#> INFO  [19:22:47.952] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [19:22:47.968] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [19:22:47.973] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [19:22:51.768] [mlr3] Finished benchmark\n#> INFO  [19:22:51.788] [bbotk] Result of batch 4:\n#> INFO  [19:22:51.789] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [19:22:51.789] [bbotk]             79        11       520  0.1755308        0\n#> INFO  [19:22:51.789] [bbotk]  errors runtime_learners\n#> INFO  [19:22:51.789] [bbotk]       0             3.78\n#> INFO  [19:22:51.789] [bbotk]                                 uhash\n#> INFO  [19:22:51.789] [bbotk]  2c8eb807-54dc-4096-8e12-3884840ce2c3\n#> INFO  [19:22:51.792] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [19:22:51.808] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [19:22:51.812] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [19:22:53.454] [mlr3] Finished benchmark\n#> INFO  [19:22:53.472] [bbotk] Result of batch 5:\n#> INFO  [19:22:53.473] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [19:22:53.473] [bbotk]            125        11       594  0.1739987        0\n#> INFO  [19:22:53.473] [bbotk]  errors runtime_learners\n#> INFO  [19:22:53.473] [bbotk]       0             1.64\n#> INFO  [19:22:53.473] [bbotk]                                 uhash\n#> INFO  [19:22:53.473] [bbotk]  f7a54be9-990f-408c-bd9e-9eb9eec101ec\n#> INFO  [19:22:53.476] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [19:22:53.491] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [19:22:54.691] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [19:22:56.804] [mlr3] Finished benchmark\n#> INFO  [19:22:56.825] [bbotk] Result of batch 6:\n#> INFO  [19:22:56.826] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [19:22:56.826] [bbotk]            109        16       597  0.1748742        0\n#> INFO  [19:22:56.826] [bbotk]  errors runtime_learners\n#> INFO  [19:22:56.826] [bbotk]       0             2.12\n#> INFO  [19:22:56.826] [bbotk]                                 uhash\n#> INFO  [19:22:56.826] [bbotk]  a85033db-bffd-48a4-836d-cfa6821f8aa7\n#> INFO  [19:22:56.829] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [19:22:56.845] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [19:22:56.848] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [19:22:57.825] [mlr3] Finished benchmark\n#> INFO  [19:22:57.849] [bbotk] Result of batch 7:\n#> INFO  [19:22:57.850] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [19:22:57.850] [bbotk]            124         4       580  0.1696214        0\n#> INFO  [19:22:57.850] [bbotk]  errors runtime_learners\n#> INFO  [19:22:57.850] [bbotk]       0             0.97\n#> INFO  [19:22:57.850] [bbotk]                                 uhash\n#> INFO  [19:22:57.850] [bbotk]  bd4a9e75-a513-4b41-a58c-dfd8701269be\n#> INFO  [19:22:57.852] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [19:22:57.869] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [19:22:57.874] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [19:23:00.682] [mlr3] Finished benchmark\n#> INFO  [19:23:00.703] [bbotk] Result of batch 8:\n#> INFO  [19:23:00.704] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [19:23:00.704] [bbotk]             16        17       560  0.1792515        0\n#> INFO  [19:23:00.704] [bbotk]  errors runtime_learners\n#> INFO  [19:23:00.704] [bbotk]       0              2.8\n#> INFO  [19:23:00.704] [bbotk]                                 uhash\n#> INFO  [19:23:00.704] [bbotk]  70ce399c-11bc-4d60-b932-1591b3f728f6\n#> INFO  [19:23:00.706] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [19:23:00.723] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [19:23:00.727] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [19:23:01.642] [mlr3] Finished benchmark\n#> INFO  [19:23:01.665] [bbotk] Result of batch 9:\n#> INFO  [19:23:01.666] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [19:23:01.666] [bbotk]             97         5       520  0.1696214        0\n#> INFO  [19:23:01.666] [bbotk]  errors runtime_learners\n#> INFO  [19:23:01.666] [bbotk]       0             0.89\n#> INFO  [19:23:01.666] [bbotk]                                 uhash\n#> INFO  [19:23:01.666] [bbotk]  2075e200-407c-4245-baa1-12ef9184bdfd\n#> INFO  [19:23:01.668] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [19:23:01.686] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [19:23:01.690] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [19:23:04.448] [mlr3] Finished benchmark\n#> INFO  [19:23:04.469] [bbotk] Result of batch 10:\n#> INFO  [19:23:04.470] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [19:23:04.470] [bbotk]             42        20       560  0.1772817        0\n#> INFO  [19:23:04.470] [bbotk]  errors runtime_learners\n#> INFO  [19:23:04.470] [bbotk]       0             2.74\n#> INFO  [19:23:04.470] [bbotk]                                 uhash\n#> INFO  [19:23:04.470] [bbotk]  0ff95f01-0a9e-4c9c-84e0-4cb4fe4e1118\n#> INFO  [19:23:04.476] [bbotk] Finished optimizing after 10 evaluation(s)\n#> INFO  [19:23:04.476] [bbotk] Result:\n#> INFO  [19:23:04.477] [bbotk]  min.node.size max.depth num.trees learner_param_vals\n#> INFO  [19:23:04.477] [bbotk]            124         4       580          <list[5]>\n#> INFO  [19:23:04.477] [bbotk]   x_domain classif.ce\n#> INFO  [19:23:04.477] [bbotk]  <list[3]>  0.1696214\n#-----------------------------------------------------------------\n# Get the best parameters\n#-----------------------------------------------------------------\nbest_params <- tune_instance$result_learner_param_vals\n#-----------------------------------------------------------------\n# Observe new classification error\n#-----------------------------------------------------------------\ntune_instance$result_y\n#> classif.ce \n#>  0.1696214\n#-----------------------------------------------------------------\n# Rerun model\n#-----------------------------------------------------------------\nlearner_ranger_rf$param_set$values = \n  tune_instance$result_learner_param_vals\nlearner_ranger_rf$train(task_mod_data)\n#-----------------------------------------------------------------\n# Build a tuning instance\n#-----------------------------------------------------------------\nprediction_tuned <- learner_ranger_rf$predict(task_mod_data, \n                                        row_ids = test_mod_data)\n#-----------------------------------------------------------------\n# Observe confusion matrix\n#-----------------------------------------------------------------\nprediction_tuned$confusion\n#>         truth\n#> response    r   nr\n#>       r  2714  504\n#>       nr   54  155\n#-----------------------------------------------------------------\n# Observe optimized classification\n#-----------------------------------------------------------------\nmeasure = msr(\"classif.acc\")\nprediction_tuned$score(measure)\n#> classif.acc \n#>   0.8371754"},{"path":"chapter7.html","id":"comparing-the-results-of-different-model-types","chapter":"7 Lead Scoring","heading":"7.2.1.4 Comparing the results of different model types","text":"know model constructed best model problem? ’ll try different ones. types problems unlikely much success altering classification system. go demonstration purposes.can choose available learners following command. know looking classification problems:Many algorithms accept certain data types. Numerical types always safe. ’ll use data set using test another algorithm.Now can build new task consisting group learners. ’ll use gradient boosting algorithm, random forest, naive bayes algorithm.can take look available measures following command:can view measures algorithm following command:xgboost model performed slightly better random forest. Perhaps use instead.","code":"\n#-----------------------------------------------------------------\n# Observe all learners\n#-----------------------------------------------------------------\nmlr3::mlr_learners\n#> <DictionaryLearner> with 27 stored values\n#> Keys: classif.cv_glmnet, classif.debug,\n#>   classif.featureless, classif.glmnet, classif.kknn,\n#>   classif.lda, classif.log_reg, classif.multinom,\n#>   classif.naive_bayes, classif.nnet, classif.qda,\n#>   classif.ranger, classif.rpart, classif.svm,\n#>   classif.xgboost, regr.cv_glmnet, regr.debug,\n#>   regr.featureless, regr.glmnet, regr.kknn, regr.km,\n#>   regr.lm, regr.nnet, regr.ranger, regr.rpart,\n#>   regr.svm, regr.xgboost\n#-----------------------------------------------------------------\n# Observe all learners\n#-----------------------------------------------------------------\n\ntask_mod_data_num <- TaskClassif$new(id   = \"task_bench\", \n                                 backend  = mod_data_numeric, \n                                 target   = \"renewed\", \n                                 positive = \"r\")\n\nlearner_num <- \n  list(lrn(\"classif.xgboost\", predict_type = \"prob\"), \n  lrn(\"classif.ranger\", predict_type = \"prob\"))\n\nset.seed(44)\ntrain_mod_data_num <- \n  sample(task_mod_data_num$nrow, 0.75 * task_mod_data_num$nrow)\ntest_mod_data_num  <- \n  setdiff(seq_len(task_mod_data_num$nrow), train_mod_data_num)\n#-----------------------------------------------------------------\n# Build new task and learner\n#-----------------------------------------------------------------\ndesign_bnch <- benchmark_grid(\ntask_bnch        <- TaskClassif$new(id  = \"task_class2\", \n                               backend  = mod_data_numeric, \n                               target   = \"renewed\", \n                               positive = \"r\"),\nlearners_bnch    <- \n  list(lrn(\"classif.xgboost\", predict_type = \"prob\"), \n       lrn(\"classif.ranger\",  predict_type = \"prob\"),\n       lrn(\"classif.naive_bayes\", predict_type = \"prob\" )),\nresamplings_bnch <- rsmp(\"holdout\")\n)\n#-----------------------------------------------------------------\n# benchmark our designs\n#-----------------------------------------------------------------\nbmr = benchmark(design_bnch)\n#> INFO  [19:23:06.081] [mlr3] Running benchmark with 3 resampling iterations\n#> INFO  [19:23:06.085] [mlr3] Applying learner 'classif.xgboost' on task 'task_class2' (iter 1/1)\n#> INFO  [19:23:06.332] [mlr3] Applying learner 'classif.ranger' on task 'task_class2' (iter 1/1)\n#> INFO  [19:23:09.533] [mlr3] Applying learner 'classif.naive_bayes' on task 'task_class2' (iter 1/1)\n#> INFO  [19:23:09.961] [mlr3] Finished benchmark\n#-----------------------------------------------------------------\n# Observe available measures\n#-----------------------------------------------------------------\nmlr3::mlr_measures\n#> <DictionaryMeasure> with 62 stored values\n#> Keys: aic, bic, classif.acc, classif.auc,\n#>   classif.bacc, classif.bbrier, classif.ce,\n#>   classif.costs, classif.dor, classif.fbeta,\n#>   classif.fdr, classif.fn, classif.fnr, classif.fomr,\n#>   classif.fp, classif.fpr, classif.logloss,\n#>   classif.mauc_au1p, classif.mauc_au1u,\n#>   classif.mauc_aunp, classif.mauc_aunu,\n#>   classif.mbrier, classif.mcc, classif.npv,\n#>   classif.ppv, classif.prauc, classif.precision,\n#>   classif.recall, classif.sensitivity,\n#>   classif.specificity, classif.tn, classif.tnr,\n#>   classif.tp, classif.tpr, debug, oob_error,\n#>   regr.bias, regr.ktau, regr.mae, regr.mape,\n#>   regr.maxae, regr.medae, regr.medse, regr.mse,\n#>   regr.msle, regr.pbias, regr.rae, regr.rmse,\n#>   regr.rmsle, regr.rrse, regr.rse, regr.rsq,\n#>   regr.sae, regr.smape, regr.srho, regr.sse,\n#>   selected_features, sim.jaccard, sim.phi, time_both,\n#>   time_predict, time_train\n#-----------------------------------------------------------------\n# Compare the models\n#-----------------------------------------------------------------\nmeasures = list(\n  msr(\"classif.auc\", id = \"auc\"),\n  msr(\"classif.ce\", id = \"ce_train\")\n)\nmeasure_list <- as.data.frame(bmr$score(measures))\nmeasure_list[,c(6,11,12)]\n#>            learner_id       auc  ce_train\n#> 1     classif.xgboost 0.6953685 0.1744364\n#> 2      classif.ranger 0.6795682 0.1792515\n#> 3 classif.naive_bayes 0.6735490 0.2061720"},{"path":"chapter7.html","id":"manually-calling-a-logistic-regression-model","chapter":"7 Lead Scoring","heading":"7.2.1.5 Manually calling a logistic regression model","text":"Logistic regression form regression “builds linear model based transformed target variable.” (Ian H. Witten 2011) target variable take form 1 0. Applied problem, renew renew. Let’s call basic logistic regression model fun.Table 7.6: Summary stats logistic modelThis output looks slightly different regression outputs seen. won’t cover logistic regression , wanted make aware . generally performs well great place start predicting two possible outcomes. Additionally, need careful models. estimated pseudoRSquared, multiple ways . Try function pscl::pR2(mod) (Zeileis, Kleiber, Jackman 2008) see different methods.","code":"\n#-----------------------------------------------------------------\n# Compare the models\n#-----------------------------------------------------------------\nmod <- glm(renewed ~ ticketUsage + tenure + \n           spend + distance,\n           data = mod_data_numeric,\n           family = binomial(link = \"logit\")\n           )\nmod_sum <- \ntibble::tibble(\n  deviance      = unlist(summary(mod)$deviance),\n  null.deviance = unlist(summary(mod)$null.deviance),\n  aic           = unlist(summary(mod)$aic),\n  df.residual   = unlist(summary(mod)$df.residual),\n  pseudoR2      = 1 - mod$deviance / mod$null.deviance\n)"},{"path":"chapter7.html","id":"measuring-performance","chapter":"7 Lead Scoring","heading":"7.2.2 Measuring performance","text":"Measuring performance can difficult. measures can technical confusing. using holdout samples, can ignore many degree. sampled calibrated correctly, using “ce” classification error visual inspection confusion matrix often enough. Let’s build plot classification error give us additional insight going .Let’s look density plot renewal scores classes.looks like ROC curve demonstrates can efficient targeting certain groups.","code":"\n#-----------------------------------------------------------------\n# Compare the models\n#-----------------------------------------------------------------\ncalc_rates <- learner_ranger_rf$predict_newdata(mod_data_numeric)\nmod_data_numeric$pred <- predict(mod,newdata = mod_data_numeric,\n                                 type = 'response')\n#-----------------------------------------------------------------\n# Density plot of error\n#-----------------------------------------------------------------\ntitle   <- 'Density plot of renewal'\nx_label <- 'Prediction'\ny_label <- 'Density'\ndensity_pred <- \nggplot(data = mod_data_numeric, \n       aes(x=pred,color=renewed,lty=renewed))    +\n  geom_density(size = 1.2)                       + \n  scale_color_manual(values = palette)           +\n  scale_x_continuous(label = scales::percent)    +\n  xlab(x_label)                                  + \n  ylab(y_label)                                  + \n  ggtitle(title)                                 +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# ROC curve\n#-----------------------------------------------------------------\nlibrary(pROC)\n#define object to plot\nroc_object <- roc(mod_data_numeric$renewed, mod_data_numeric$pred)\n\ntitle   <- 'ROC curve for renewals'\nx_label <- 'Specificity'\ny_label <- 'Sensitivity'\n\nroc_graph <-\nggroc(roc_object,colour = 'dodgerblue',size = 1.2) +\n  xlab(x_label)                                    + \n  ylab(y_label)                                    + \n  ggtitle(title)                                   +\n  graphics_theme_1"},{"path":"chapter7.html","id":"using-this-data","chapter":"7 Lead Scoring","heading":"7.3 Using this data","text":"Lead scores EASY use. Qualifying leads likely important analytics exercise can quickly easily deployed. thinking strategically simplest finest. simply need apply preferred model new data. algorithms performed similar way (isn’t uncommon). . Scored data typically placed quantiles deployed order based desire happen.","code":""},{"path":"chapter7.html","id":"building-cumulative-gains-charts","chapter":"7 Lead Scoring","heading":"7.3.1 Building cumulative gains charts","text":"know model efficacy practice? Let’s take sample model data pretend new group plan deploy renewal campaign. Let’s begin borrowing data last analysis. Let’s pretend individuals new trying renew .curve look like renewal rates differed group?can see curve pushed middle. curve begins flatten, campaign less efficient. problems complex. data isn’t perfect can difficult get amazing results see texts.","code":"\n#-----------------------------------------------------------------\n# Build data fro cumulative gains chart\n#-----------------------------------------------------------------\nmod_data_sample <- \n  mod_data_numeric                                      %>% \n  dplyr::select(pred,renewed)                           %>%\n  dplyr::mutate(custId = seq(1:nrow(mod_data_numeric))) %>%\n  dplyr::sample_n(5000)                                 %>%\n  dplyr::arrange(desc(pred))\n\nqt <- quantile(mod_data_sample$pred,\n               probs = c(.1,.2,.3,.4,.5,.6,.7,.8,.9))\n\nf_apply_quant <- function(x){\nifelse(x >= qt[9],1,\n  ifelse(x >= qt[8],2,\n    ifelse(x >= qt[7],3,\n      ifelse(x >= qt[6],4,\n        ifelse(x >= qt[5],5,\n          ifelse(x >= qt[4],6,\n            ifelse(x >= qt[3],7,\n              ifelse(x >= qt[2],8,\n                ifelse(x >= qt[1],9,10)))))))))\n}\n\nmod_data_sample$group <- sapply(mod_data_sample$pred,\n                                function(x) f_apply_quant(x))\n\ntable(mod_data_sample$group,mod_data_sample$renewed)\n#>     \n#>       nr   r\n#>   1   53 447\n#>   2   52 448\n#>   3   56 444\n#>   4   52 448\n#>   5   52 449\n#>   6   71 428\n#>   7  101 399\n#>   8   83 417\n#>   9  124 376\n#>   10 299 201\n#-----------------------------------------------------------------\n# Build data for cumulative gains chart\n#-----------------------------------------------------------------\nmod_data_sample$renewedNum <- \n  ifelse(mod_data_sample$renewed == 'r',1,0)\nmod_data_sample$perpop <- \n  (seq(nrow(mod_data_sample))/nrow(mod_data_sample))*100\n\nmod_data_sample$percRenew <- \ncumsum(mod_data_sample$renewedNum)/sum(mod_data_sample$renewedNum)\n\ntitle   <- 'Cumulative gains chart'\nx_label <- 'Population'\ny_label <- 'Cumulative Renewal Percentage'\ncgc <- \nggplot(mod_data_sample,aes(y=percRenew,x=perpop))           +\n  geom_line(color = mod_data_sample$group,size = 1.2 )      +\n  geom_rug(color = mod_data_sample$group,sides = 'b' )      +\n  geom_abline(intercept = 0, slope = .01, size = 0.5,lty=3) +\n  xlab(x_label)                                             + \n  ylab(y_label)                                             + \n  ggtitle(title)                                            +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Build data for improved cumulative gains chart \n#-----------------------------------------------------------------\nmod_data_gain <- mod_data_sample %>%\n                 group_by(group) %>%\n                 summarise(cumRenewed = sum(renewedNum))\n\nnew_renewals <- c(500,490,455,400,300,200,120,90,70,20)\nmod_data_gain$cumRenewed <- new_renewals\n\nmod_data_gain$gain <- \n  cumsum(mod_data_gain$cumRenewed/sum(mod_data_gain$cumRenewed))\n\n\ntitle   <- 'Cumulative gains chart'\nx_label <- 'Group'\ny_label <- 'Gain'\n\ncgc_imp <- \nggplot(mod_data_gain,aes(y=gain,x=group))                  +\n geom_line(color = mod_data_gain$group,size = 1.2)         +\n scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9,10))    +\n geom_abline(intercept = 0, slope = .01, size = 0.5,lty=3) +\n xlab(x_label)                                             + \n ylab(y_label)                                             + \n ggtitle(title)                                            +\n graphics_theme_1"},{"path":"chapter7.html","id":"key-concepts-and-chapter-summary-6","chapter":"7 Lead Scoring","heading":"7.4 Key concepts and chapter summary","text":"Systematically interacting potential customers core component direct marketing. covered major concepts:RFM scoresLead scoring random forestCross ValidationModel optimizationModel comparisonsEvaluating model efficacyHow use models practiceThese subjects cover specific techniques lead scoring, also cover model construction depth.Recency, Frequency, Monetary Value scores relatively simple construct referred “poor man’s analytics.” easy interpret simple use.random forest great tool data short--wide. results easy interpret tend work well logistic regression. bread--butter machine learning.Cross validating results important part analytics process sometimes overlooked building model.Models can optimized different ways depending model. includes regression models machine learning models.readily-available tools can use compare models . always best try couple different things.Leveraging lead-scores simple makes salespeople effective.","code":""},{"path":"chapter8.html","id":"chapter8","chapter":"8 Marketing promotions","heading":"8 Marketing promotions","text":"can difficult measure efficacy promotions. take wider view promotions consider advertising form promotion becomes even difficult. clubs began eliminate many forms promotion unable measure outcomes. makes sense level, don’t go far. Major Sport brands unique ways brands tend carry team performing well. However, can’t always good. reconcile issues standpoint promotions? chapter give specific example measuring impact promotion cover nuance subject context club.teams promotions? Like marketing techniques, promotions exist increase sales. sports, typically response less anticipated demand capacity. However, concept can paradoxical. Promotions perform best team performing best. take multi-year point--view marketing likely make different decisions look fiscal-year standpoint. also brand considerations.Additionally, competing philosophies use promotions. many levers franchise can pull increase likelihood someone purchasing ticketing product biggest one price. also important note typically cost 70 justifying return investment typically considered (assuming goal isn’t solely maximizing revenue).Typical sales promotions aimed value: getting product less money. However, see myriad promotions sports:Giveaways bobbleheads hatsPost-game concertsBuy one ticket, get one freeLoyalty programs season ticket holdersFlash sales dynamic pricing promotionsAdditionally, promotions may also negative impacts brand integrity. Constantly reverting price promotions can damage brand (Keller 2003).“objective value pricing uncover right blend product quality, product cost, product prices fully satisfies needs wants consumers profit targets firm”.Furthermore, many channels notoriously difficult evaluate. Advertisement promotion occur number places:OOH (home) refers billboards, etc.RadioTelevisionSEM (Search engine)Online agencies (Double-Click)Social MediaEarned media television coveragePodcastsFoundation activities charitable venturesSocial platforms become increasingly important part advertisement. However, platforms walled gardens algorithms demonstrating ROAS. don’t incentive tell something isn’t working. SEM even bigger problem can confounding. know type “Game Hen Tickets” goggle seems like pretty easy serve ad can potentially tracked sale. SEM beach-head secondary market sellers. secondary markets Stubhub tends worry transactions. Since operate across spectrum tickets, impossible outspend.Media evaluation handled specific ways. .. based methods using neural networks measure exposure become popular past several years. Media equivalency tough typically propped arbitrary figure. ’ll discuss evaluating advertising later chapter.know marketing activity worthwhile? many cases don’t. Baselining sales can difficult activities naturally baked-sales. effects, see salary figure 8.1 clearly outside marketer’s influence.following graph produced public data ticket sales average salary MLB teams. points colored based population region.\nFigure 8.1: Relationship ticket sales salary\nclearly level correlation payroll tickets sold. However, isn’t clear size market impact salary sales. Although appear small markets tend cluster near bottom salary large markets cluster near top. use clustering algorithm data see true, illustration. also begs question.able make reasonable estimates ticket sales using macro factors payroll wins, impact marketing efforts actually ?sticky point, valid. never find president owner liquidates marketing department. Indeed lot push tickets. important factor many clubs likely investment team. true baseball number games. exposed perturbations performance events. NFL major advantage.also lot complexity specific market conditions. number questions can ask focusing ticket sales.many single game tickets sold individuals database?recycle fans promotions?average ticket sales higher major promotion?turnstile higher major promotions… F&B sales impacted?Can identify causal link sales/turnstile/revenue major promotions?major promotions promote purchase less expensive tickets reducing yield?reasonably good evidence major promotions increase ticket sales, increased ticket sales efficient use marketing dollars? Additionally, type major promotion efficient? ’ll walk example evaluate major promotions talk measuring forms media exposure. chapter may seem little repetitive. going conduct analysis season data, slightly different way. Promotions likely small samples puts us non-parametric world. scary place go-tools may work .","code":""},{"path":"chapter8.html","id":"measuring-the-impact-of-promotions","chapter":"8 Marketing promotions","heading":"8.1 Measuring the impact of promotions","text":"Measuring impact promotions can difficult. analysis builds analyses already completed. ’ll begin now familiar season_data data set. great spot begin order evaluate promotions.Table 8.1: Dataset evaluating promotionsThis data allow us look two typical promotions baseball team might conduct. Bobbleheads Concerts ubiquitous across sports. lead believe effective ticket drivers. However many considerations.\nFigure 8.2: Ticket sales season\nLet’s take little closer look data. wide differences many games. saw chapter 6 variables can used predict sales. type graphic can misleading. simply took averages sales appear promotions increase ticket sales. However, product isn’t consistent. wide variation sales contains seasonality, trends, endogenous factors influence turnout.code chunk produces tile plot figure 8.3. can see concerts almost always occur weekend. true Bobbleheads. can control determine impact sales like? ’ll take look data different ways clarity. often useful experiment graphics. data may tell different stories.\nFigure 8.3: Ticket sales season day week\nLet’s take brief aside talk color interpolation. several ways interpolate colors R. addition, virids library (Garnier 2021) interesting color package produces vivid results used correct context. Give try.appear difference days week. Let’s look slightly different way.see predictable pattern assumed previous plot. analysis confounded many factors can’t take sales face value.\nFigure 8.4: Ticket sales season\nplot good job discriminating promotions. appears concerts, bobbleheads, promotions generate sales average day week taken account. factor opponent impact ?\nFigure 8.5: Ticket sales season\n","code":"\n#-----------------------------------------------------------------\n# Season data structure\n#-----------------------------------------------------------------\ndata <- FOSBAAS::season_data\n\ndata_struct <- \n  data.frame(\n  variable = names(data),\n  class    = sapply(data, typeof),\n  values   = sapply(data, function(x) paste0(head(x)[1:2],  \n                                      collapse = \", \")),\n            row.names = NULL\n  )\n#-----------------------------------------------------------------\n# Promotions by season\n#-----------------------------------------------------------------\ndata$count <- seq(1:nrow(data))\n\nx_label  <- ('\\n 2022-2024 Home Games')\ny_label  <- ('Tickets Sold \\n')\ntitle   <- (\"M-Promos tend to have higher than average sales\")\nticket_sales <- \n  ggplot(data, \n         aes(x = count,y = ticketSales, \n             color = factor(promotion)),\n             group = promotion)                                  +\n  ggtitle(title)                                                 +\n  xlab(x_label)                                                  +                         \n  ylab(y_label)                                                  +                           \n  scale_y_continuous(labels = scales::comma)                     +\n  geom_point(aes(y=ticketSales,x=count), size=2)                 +\n  geom_smooth(data=subset(\n    data,promotion == 'bobblehead' | promotion == 'concert' | \n    promotion == 'other' | promotion == 'none'),\n    method='lm',formula=y~x,se=FALSE,fullrange=TRUE,size=1.2)    +\n  geom_vline(xintercept = 1, lty=4, color='grey30')              +\n  geom_vline(xintercept = 81, lty=4, color='grey30')             +\n  geom_vline(xintercept = 162, lty=4, color='grey30')            +\n  geom_vline(xintercept = 243, lty=4, color='grey30')            +\n  scale_color_manual(\n    breaks = c('bobblehead','concert','other','none'),\n    values=palette, name='Promotion: ',\n    labels=c(\"BHead\",\"Concert\",\"Other\",'None'))                  +    \n  annotate(\"text\", x = 40,  y = 1000, \n           label = \"2022\", color='black')                        +\n  annotate(\"text\", x = 120, y = 1000, \n           label = \"2023\", color='black')                        +\n  annotate(\"text\", x = 200, y = 1000, \n           label = \"2024\", color='black')                        +\n  graphics_theme_1                                               + \n  theme(\n    axis.text.x      = element_blank(),\n    legend.position  = \"bottom\",\n    panel.grid.major = element_blank(),  \n    panel.grid.minor = element_blank(), \n    axis.ticks.x=element_blank()\n  )\n#-----------------------------------------------------------------\n# Promotions by day of the week\n#-----------------------------------------------------------------\nx_label  <- ('\\n Promotion')\ny_label  <- ('Day of Week\\n')\ntitle    <- \n  ('Average Sales (10,000s) by promotion and day of week \\n')\n\npromos <- \n  data                                         %>% \n  group_by(dayOfWeek,promotion,season)         %>%\n  summarise(avgTickets = median(ticketSales))\n\ntile_sales <- \n  ggplot(promos, aes(y=dayOfWeek,x=promotion))                   +\n  facet_grid(.~season)                                           +\n  geom_tile(aes(fill = avgTickets))                              + \n  geom_text(aes(label = round((avgTickets/10000), 2)),\n                color='grey10')                                  +\n  scale_fill_gradient(low = \"white\", high = \"dodgerblue\", \n                      space = \"Lab\",\n                      na.value = \"grey10\", guide = \"colourbar\")  +\n  ggtitle(title)                                                 +\n  xlab(x_label)                                                  +                       \n  ylab(y_label)                                                  + \n  scale_y_discrete(limits=c('Mon','Tue','Wed','Thu',\n                            'Fri','Sat','Sun'))                  + \n  scale_x_discrete(limits = c('bobblehead','concert',\n                              'none','other'),\n                   labels=c('bh','concert','none','other'))      +\n  graphics_theme_1 + theme(\n    legend.position  = \"bottom\",\n    axis.text.x      = element_text(angle = 0, size = 10, \n                                    vjust = 0, color = \"grey10\"),\n    legend.title     = element_text(size = 10, face = \"plain\", \n                                    color = \"grey10\"), \n    legend.text      = element_text(size = 7, color = \"grey10\")\n    )\n#-----------------------------------------------------------------\n# Color interpolation\n#-----------------------------------------------------------------\nlibrary(viridis)\nscale_fill_viridis(direction = 1, option = \"B\",trans=\"log2\") \nscale_fill_distiller(palette = 'Greens',direction = 1)\n#-----------------------------------------------------------------\n# Promotions by season and day of week\n#-----------------------------------------------------------------\ndata$count <- seq(1:nrow(data))\n\nx_label  <- ('\\n 2022-2024 Home Games')\ny_label  <- ('Tickets Sold \\n ')\ntitle    <- (\"Impacts are less clear when DOW is considered\")\n\ndow_sales <- \n  ggplot(data, aes(x = count,y = ticketSales, \n                   color = factor(dayOfWeek),\n                   shape = factor(promotion),\n                   group =factor(dayOfWeek)))                +\n  ggtitle(title)                                             +\n  xlab(x_label)                                              +                            \n  ylab(y_label)                                              +\n  scale_x_continuous( breaks = 1:242)                        +\n  scale_y_continuous(labels  = scales::comma)                +\n  geom_point(aes(y=data$ticketSales,x=data$count), size=3.5) +\n  geom_vline(xintercept = 81,  lty = 4,  color ='grey30')    +\n  geom_vline(xintercept = 162, lty = 4, color ='grey30')     +\n  geom_vline(xintercept = 242, lty = 4, color ='grey30')     +\n  scale_color_manual(breaks = c(\"Mon\", \"Tue\",'Wed','Thu',\n                                'Fri','Sat','Sun'),\n                     values = palette,\n                     name   ='Day: ',\n                     labels = c(\"Mon\", \"Tue\",'Wed','Thu',\n                              'Fri','Sat','Sun'))            +   \n  scale_shape_manual(\n    breaks = c('bobblehead','none','concert','other'),\n    name='Promotion: ',\n    values = c(17,19,3,7),\n    labels=c(\"bh\",'none','con','other'))                     +   \n  annotate(\"text\", x = 40,  y = 1000, \n           label = \"2022\", color='grey10')                   +\n  annotate(\"text\", x = 120, y = 1000, \n           label = \"2023\", color='grey10')                   +\n  annotate(\"text\", x = 200, y = 1000, \n           label = \"2024\", color='grey10')                   +\n  graphics_theme_1                                           +   \n  theme(\n    axis.text.x      = element_blank(),\n    legend.position  = \"right\",\n    panel.grid.major = element_blank(),  \n    panel.grid.minor = element_blank(), \n    axis.ticks.x     = element_blank())\n#-----------------------------------------------------------------\n# Box plot, sales by day of week\n#-----------------------------------------------------------------\nx_label  <- ('\\n 2022-2024 games by DOW')\ny_label  <- ('Tickets Sold \\n ')\ntitle    <- (\"Sales by DOW and promotion\")\n\ndow_sales_box <- \n  ggplot(data, aes(x     = dayOfWeek, \n                   fill  = factor(promotion), \n                   color = factor(promotion)))               +\n  ggtitle(title)                                             +\n  xlab(x_label)                                              +                               \n  ylab(y_label)                                              +\n  scale_y_continuous(labels = scales::comma)                 +\n  scale_x_discrete(limits=c(\"Mon\", \"Tue\",'Wed','Thu',\n                            'Fri','Sat','Sun'))              + \n  geom_boxplot(aes(y=ticketSales))                           +\n  scale_color_manual(\n    breaks = c('bobblehead','concert','none','other'),\n    values=c('grey40','grey40','grey40','grey40'), \n    name='Concert: ',\n    labels=c('bobblehead','concert','none','other'),\n    guide = 'none')                                          +    \n  scale_fill_manual(\n    breaks = c('bobblehead','concert','none','other'),\n    values=c(palette), \n    name='Promotion: ',\n    labels=c('bobblehead','concert','none','other'))         +\n  graphics_theme_1                                           +   \n  theme(legend.position  = \"bottom\")"},{"path":"chapter8.html","id":"regressing-on-our-data","chapter":"8 Marketing promotions","heading":"8.1.1 Regressing on our data","text":"seen, regression takes rigor correctly. works best lot data, luxury don’t tend many cases. ’ll go though another example, going look analysis slightly different way. ’ll also use different framework time. ? ? lots ways thing R like certain ways better.’ll use tidymodels (Kuhn Wickham 2022) package exercise 71. Tidymodels (like MLR3) make preprocsssing evaluation much simpler. ’ll begin installing libraries little processing data., going build linear regression model. also know interested much influence promotions ticket sales. ’ll make one change data set function f_change_order() make results easier interpret complete exercise.’ll use rsample library (Silge et al. 2022) build training test set. many ways partition data used different one every time. Find one like. ’ll split data twenty-five percent going holdout sample. aware try use model predict sales value new data model won’t work. can frustrating. Just making aware. happen.Tidymodels leverages tidy principles builds recipes layers. find easier work frameworks. Pipes just easier read understand. However, programmers might find procedural nature irritating.Piping steps like dummy coding variables easy system. makes preprocessing data mechanical exercise.can check recipe couple commands.select linear regression model deploy.Tidymodels uses concept workflows process regression steps.Finally, ’ll apply workflow fit model training data set.can also extract modelWe know sample size issues data. Can trust coefficients?can use coefficients model help us explain impact specific promotions relative variables. bobblehead worth 4,062 ticket sales.However isn’t entire story. Concerts, bobbleheads, promotions different costs. concert, costs high. lot consider:Talent feesProduction feesReplacing damaged turfLightingThis far exclusive list, electric bill stadium may shock . promotion bobblehead much discrete cost. per unit fee incur costs associated storing dispensing item. makes calculus fairly simple considering dimension ticket sales. Concerts may increase ancillary purchases food beverage. Additionally, excess tickets sold bobblehead may least expensive seats stadium. one net revenue?","code":"\n#-----------------------------------------------------------------\n# preprocessing our data\n#-----------------------------------------------------------------\nlibrary(tidymodels)\nlibrary(readr)      \nlibrary(broom.mixed) \nlibrary(dotwhisker)  \nlibrary(skimr)\nlibrary(dplyr)\n\ndata <- FOSBAAS::season_data\ndata <- data[,c(\"gameNumber\",\"team\",\"month\",\"weekEnd\",\n                \"daysSinceLastGame\",\"promotion\",\"ticketSales\")]\n#-----------------------------------------------------------------\n# Alter promotions \n#-----------------------------------------------------------------\nf_change_order <- function(x){\n  if(x == \"none\"){\"anone\"}\n  else{x}\n}\ndata$promotion <- sapply(data$promotion,function(x) \n                         f_change_order(x))\n#-----------------------------------------------------------------\n# Splitting our data set\n#-----------------------------------------------------------------\nset.seed(755)\ndata_split <- initial_split(data, prop = .75)\ntrain_data <- rsample::training(data_split)\ntest_data  <- rsample::testing(data_split)\n#-----------------------------------------------------------------\n# Build a recipe\n#-----------------------------------------------------------------\nsales_rec <-  recipe(ticketSales ~ ., data = train_data)\n#-----------------------------------------------------------------\n# Add functions to the recipe\n#-----------------------------------------------------------------\nsales_rec <- recipe(ticketSales ~ ., data = train_data) %>% \n             update_role(gameNumber, new_role = \"ID\")   %>%\n             step_dummy(all_nominal(), -all_outcomes()) %>%\n             step_zv(all_predictors())\n#-----------------------------------------------------------------\n# Check the recipe \n#-----------------------------------------------------------------\n\ndata_test <- sales_rec                 %>% \n             prep()                    %>% \n             bake(new_data = test_data)\n\nhead(data_test)[c(4:9)]\n#> # A tibble: 6 × 6\n#>   ticketSales team_ATL team_BAL team_BOS team_CHC team_CIN\n#>         <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n#> 1       25759        0        0        0        0        0\n#> 2       26464        0        1        0        0        0\n#> 3       29787        0        1        0        0        0\n#> 4       35277        0        0        0        1        0\n#> 5       40594        0        0        0        1        0\n#> 6       32073        0        0        0        1        0\n#-----------------------------------------------------------------\n# Define a model\n#-----------------------------------------------------------------\nlm_model <- linear_reg()           %>% \n            set_engine('lm')       %>% \n            set_mode('regression')\n#-----------------------------------------------------------------\n# build a workflow\n#-----------------------------------------------------------------\nsales_wflow <- workflow()           %>% \n               add_model(lm_model)  %>% \n               add_recipe(sales_rec)\n#-----------------------------------------------------------------\n# build a workflow\n#-----------------------------------------------------------------\nset.seed(755)\nfolds        <- vfold_cv(train_data, v = 10)\nsales_fit_rs <- sales_wflow          %>% \n                fit_resamples(folds)\n\ncv_metrics <- collect_metrics(sales_fit_rs)\n#-----------------------------------------------------------------\n# Run the model and extract the results\n#-----------------------------------------------------------------\n\nsales_fit <- sales_wflow            %>% \n             fit(data = train_data)\n\nresults   <- sales_fit             %>% \n             extract_fit_parsnip() %>% \n             tidy()\n#-----------------------------------------------------------------\n# build a workflow\n#-----------------------------------------------------------------\nmodel         <- extract_fit_engine(sales_fit)\nmodel_metrics <- glance(model)"},{"path":"chapter8.html","id":"how-to-place-promotions-on-a-schedule","chapter":"8 Marketing promotions","heading":"8.2 How to place promotions on a schedule","text":"covered top approach forecasting chapter 6. know dates select particular promotion. calculus typically revolves around two considerations:Maximizing ticket impactMaximizing revenue impactHow know making takeoffs? Certain scenarios may make correct decision unclear. Let’s look example chapter 6. Now confirmed bobbleheads concerts increase ticket sales, let’s add another bobblehead. add promotion? event scores produced season_2025 data set may give us clues. also many qualitative considerations discuss.Let’s start visualizing data. boundaries exist event scores? clustered data section use clusters identify candidates promotion.\nFigure 8.6: Event attractivness range cluster\ngoal maximize ticket sales shouldn’t plan one promotions games possibility game sells . sellout within confidence interval sellout disqualified consideration. Let’s take look candidate dates. ’ll assume want maximize sales. decided go route believe ancillary revenue associated concessions retail make ticket premium forgoing. However, also looking games potential terms higher prices. Let’s start identifying borders clusters. easy way use duplicated function. ’ll reverse output makes sense header.Let’s take look scatter plot predicted prices vs. predicted ticket sales. know interaction sales prices. also idea much promotion bobblehead may impact sales. ’ll use upper bound confidence interval give us idea games might best candidates bobblehead. select one games band possibility miss opportunity sell tickets exceed capacity. said, want maximize revenue one games might attractive price premium overwhelms capacity constraint.get lot fancy confidence interval estimates, analysis doesn’t warrant . Simple often good.\nFigure 8.7: Predicted sales prices cluster\ncan see games might make best candidates. Let’s look border games clusters 2 5. Let’s take look candidate game see appropriate adding promotion.Table 8.2: Promotion candidateThis game late summer already commands high price. isn’t another major promotion night plenty headroom sales. looks like great night add promotion. However, considerations.Promotions often sponsors may preference specific promotion takes place. also need consider promotional item. bobblehead may need six months lead time. analysis needs take place well advance can produced properly messaged. Additionally, theme item may appropriate promotions happening. also need consider happening region. competition another venue event night?","code":"\n#-----------------------------------------------------------------\n# Clustered events\n#-----------------------------------------------------------------\nseason_2025 <- read.csv('files/season_2025.csv')\n\nseason_2025$cluster <- factor(season_2025$cluster)\nx_label  <- ('\\n Game')\ny_label  <- ('Event Scores \\n')\ntitle   <- ('Event score clusters by event')\nes_box <- \n  ggplot2::ggplot(data = season_2025, \n                  aes(x = order,\n                      y = eventScore,\n                      color = cluster))             +\n  geom_point(size = 1,alpha = .5)                   +\n  geom_boxplot()                                    +\n  scale_color_manual(values = palette)              +\n  scale_y_continuous(label = scales::comma)         +\n  xlab(x_label)                                     + \n  ylab(y_label)                                     + \n  ggtitle(title)                                    +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Identify game breaks\n#-----------------------------------------------------------------\n\nseason_2025$border <- \n  ifelse(duplicated(season_2025$cluster) == TRUE,FALSE,TRUE)\n#-----------------------------------------------------------------\n# Identify candidate games\n#-----------------------------------------------------------------\nseason_2025$cluster <- factor(season_2025$cluster)\nx_label  <- ('\\n Pred Sales')\ny_label  <- ('Pred Price \\n')\ntitle   <- ('Predicted sales and price by cluster')\nes_scatter <- \n  ggplot2::ggplot(data      = season_2025, \n                  aes(x     = predTickets,\n                      y     = predPrices,\n                      color = cluster,\n                      shape = border))                      +\n  annotate(\"rect\", xmin = 45000 - confint(model)[36,2], \n                   xmax = 45000, \n                   ymin = 0,  ymax = 60,\n                   alpha = .4, fill = 'coral')              +\n  geom_point(size = 3,alpha = 1)                            +\n  geom_segment(aes(x = 39000, y = 50, \n                   xend = 37500, yend = 41),\n                   arrow = arrow(length = unit(0.5, \"cm\")),\n                   color = 'black')                         +\n  scale_color_manual(values = palette)                      +\n  scale_x_continuous(label = scales::comma)                 +\n  scale_y_continuous(label = scales::dollar)                +\n  xlab(x_label)                                             + \n  ylab(y_label)                                             + \n  ggtitle(title)                                            +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Get Confidence Intervals\n#-----------------------------------------------------------------\ncandidate <- subset(season_2025, \n                    season_2025$border == T & cluster == 2)"},{"path":"chapter8.html","id":"evaluating-external-and-internal-marketing-assets","chapter":"8 Marketing promotions","heading":"8.3 Evaluating external and internal marketing assets","text":"marketing textbook, sake covering major analytics categories sports team cover marketing. also covered number subjects directly related marketing segmentation pricing. discussing something different . Although measuring media assets big business, phrase “’s Turtles way ” comes mind. get bottom, arbitrary figure likely used gauge value signage social media post. also can briefly talk internal marketing assets. pro sports team might sell tickets, media, merchandise, also marketing platform. ’ll discus section.Honestly, find sort analytics slightly boring. aren’t really dealing concrete truths. marketing asset worth someone willing pay . Ultimately, worth tends gauged one two ways; many people can exposed quality exposure. eyes platform, better. also good putting advertisements front relevant audiences, become Google.Advertising big business trying prove much worth pro sports team analogous astrology eyes. Digital giants methods attribution. always looked methods skepticism. business selling advertising incentive tell doesn’t work?Overall, assets usually evaluated form exposure equivalency. impressions based formula look something like :\\[\\begin{equation}\n\\ {Value} = {Exposure} * {Quality} * {Costs}\n\\end{equation}\\]arbitrary cost talking references value associated outreach. much cost reach potential customer. don’t mean sound cynical, number difficult estimate.","code":""},{"path":"chapter8.html","id":"external-marketing-assets","chapter":"8 Marketing promotions","heading":"8.3.1 External marketing assets","text":"Marketing assets incredibly varied. internet essentially marketing platform. Additionally, messaging varies throughout year. instance, Brand messaging may predominate -season. season, ticket sales messaging predominate category.lump digital marketing two categories. Platform digital local digital.Platform digital marketing involves tech giants. sophisticated methods reach customers potential customers. Advertising social media still new scheme advertising. Increasingly, money spent areas. tends easier attribute sales spend using mediums easy track customers process. However, becoming difficult. can classify advertising platforms display, video, Social, Audio. major players arena include:Google Display NetworkFacebookInstagramTwitterAmazonYoutubeTicTokSpotifyEach platforms attribution methodologies. methodologies can also vary. really want make effort attribute sales channels combine capabilities old-school tricks. instance, special offer may advertised one channels. attribute sales happen outlet facebook? three main methods leveraged:First touch/Last touch: fan purchases tickets, first last platform visited? ’ll attribute entire purchase platform.Weighted models: models attribute portion sale advertising touchpoints. Weighted models may also decay time. instance, fourteen days since touchpoint longer attribute sale portion marketing engine.Algorithmic models: models custom built might use regression tool accurately weight touchpoints.means analytics involved actually looks like flow chart diagram math equation. However, isn’t always true. sports typically deal many relatively small transactions happen predictable cadence. isn’t ideal scenario using algorithmic methods. expression think “Good enemy great.” wouldn’t think subject.Local digital marketing refers leveraging display local mechanisms. include city regional newspaper websites. Fandom tend decrease individual lives local market. channels typically good readership local market.Local digital marketing refers leveraging display local mechanisms. include city regional newspaper websites. Fandom tend decrease individual lives local market. channels typically good readership local market.Search engine marketing separate category display believe issues context sports. discussed briefly earlier. efforts made flatten secondary markets, form marketing may less important clubs future. someone typing “game hen tickets” search engine know wanted tickets. care buy ? Sometimes. However, loosing strategy. depend individual clicking ad attribution can’t beat secondary market makers terms spend.Search engine marketing separate category display believe issues context sports. discussed briefly earlier. efforts made flatten secondary markets, form marketing may less important clubs future. someone typing “game hen tickets” search engine know wanted tickets. care buy ? Sometimes. However, loosing strategy. depend individual clicking ad attribution can’t beat secondary market makers terms spend.Television advertising still important place marketing budget. However, traditional broadcasts declined years. fact, TV cable industries crisis. Younger consumers altering consumption habits alarming rates. believe channel continue wane importance.Television advertising still important place marketing budget. However, traditional broadcasts declined years. fact, TV cable industries crisis. Younger consumers altering consumption habits alarming rates. believe channel continue wane importance.Radio. Buying selling radio spots also multitude considerations. measured? Third party researchers pay people allow track consumption habits extrapolate. Additionally, terrestrial radio broadcasts decreased popularity due subscription services.Radio. Buying selling radio spots also multitude considerations. measured? Third party researchers pay people allow track consumption habits extrapolate. Additionally, terrestrial radio broadcasts decreased popularity due subscription services.Outdoor Advertising. home advertising also varied. Highway signage probably biggest category outdoor advertising. many local players, arena also dominated large players. Prices also vary considerably. Overall, form advertisement relatively inexpensive, widely available, can purchased precise increments.Outdoor Advertising. home advertising also varied. Highway signage probably biggest category outdoor advertising. many local players, arena also dominated large players. Prices also vary considerably. Overall, form advertisement relatively inexpensive, widely available, can purchased precise increments.Print. Classic print advertising include taking advertisements newspapers magazines. channel dramatically decreased importance years.Print. Classic print advertising include taking advertisements newspapers magazines. channel dramatically decreased importance years.Many channels difficult impossible actually measure. business, building formal experiments around effective print ads isn’t going top priority. build strategy around marketing mix? Looking sales cadence probably best place start. start know. know sales typically happen game game basis. Understanding customer journey critical part marketing strategy. inform strategy anything. can also dramatically decrease spend channels aren’t readily measurable.also paradox sports related marketing. team good make appear marketing playing part success. high--high strategy really works use price lever. effective marketing team isn’t good. spend amount places expect level baseline success. don’t know can actually validated. strategic standpoint, mean? means skeptical ruthlessly test experiment.","code":""},{"path":"chapter8.html","id":"internal-marketing-assets","chapter":"8 Marketing promotions","heading":"8.3.2 Internal marketing assets","text":"Internal assets similar external assets. However, added wrinkle. association pro sports team creates added brand equity. doesn’t happen advertise facebook. Additionally, tangential association brands advertising team. makes evaluating value piece signage social media post delivered team much difficult.calculate value piece signage outfield ballpark Nashville? process involve steps. likely need engage third party get information television radio. large media research companies specialize sort research. basic process works like :Calculate amount exposure. many people see sign channel?Evaluate quality exposure. often asset visible.Calculate unit value audience channelBuild coefficient calculation takes account brand associationWhat sorts internal assets discussing? sports team large toolbox terms advertising assets. includes:venue signageTelevision radio broadcastsCategory rights co-brandingSocial media postsCommunity outreach foundationsUniform patches logosVenue naming rightsEach assets varies. instance, signage wide category best inventory located behind homeplate, outfield, around marquee. costs associated assets vary considerably. Additionally, value assets influenced -market reference prices competition.best place begin analyzing rate card. much inventory used much isn’t utilized. imbalances? opportunities create new inventory foul poles? Many sponsorship deals staggered multi-year terms. Exclusivities also common restrict access certain categories.Objectively justifying sponsorship can difficult. buyer perspective, may easy reconcile opportunities. building decision matrix options relatively simple, taking brand considerations account .","code":""},{"path":"chapter8.html","id":"key-concepts-and-chapter-summary-7","chapter":"8 Marketing promotions","heading":"8.4 Key concepts and chapter summary","text":"Promotions response less demand supply can difficult evaluate. covered high-level topics:Evaluating promotion efficacybuilding model using TidyModels frameworkPutting promotions scheduleValuing internal external marketing productsWe learned promotions impact appear impact. can vary market, promotions simply work better. Marginal costs associated promotion always considered. also qualitative considerations last promotion took place, lead time, sponsorship considerations.tidyModels framework easy way take care mechanics model construction. handles wide variety machine learning tasks flexible method try multiple techniques.Placing promotions schedule related pricing. obviously complex interactions price demand. Sports little unique product multiple qualitative considerations win-loss record, playoff chances, seasonality, promotional items.Evaluation return marketing spend difficult. Many assets defy valuation. terms internal assets, pricing can challenging, typically three considerations. Viewership, engagement quality, brand association.","code":""},{"path":"chapter9.html","id":"chapter9","chapter":"9 Consumer Research","heading":"9 Consumer Research","text":"chapter discuss different research techniques within context sports team looking leverage research better understand customers. people hate research yet claim love . Normal people hate research. don’t, hope read chapter. tedious, painful, typically repetitive nature, opens scrutiny. Research difficult unless PhD worked research industry, unlikely extensive experience . Unfortunately, need understand .One main issues face results research projects often considered cursory fashion. person understands results best designer analyst. partly many internal consumers interested narrow band concerns. can discouraging.Furthermore, proper research enormous topic. chapters 5 6 mention easy write entire book pricing segmentation. easy fill multiple volumes consumer research. can find number tomes ebay market sites traded significant discount. wise purchase add library. ’ll discuss books used years continue read chapter. Basic research techniques haven’t changed much long time references advised. chapter discuss two main topics:basics experiment design hypothesis testingPlanning designing surveysWe’ll able cover sliver topics techniques likely face point career. already discussed ANOVA mechanisms analyze experiments, didn’t discuss experiments need designed order validate techniques. term statistical significance gets thrown around. concept statistical significance often well understood. used narrow context around hypothesis testing mathematical artifact. doesn’t mean survey isn’t good useful. ’ll discuss detail little later.included experiment design topic people lack experience. Indeed, many business leaders aren’t well versed research practical way difficult time navigating consuming . ’d like come away chapter key pieces knowledge help grasp research. might read little cynically, food thought.can’t always trust researchDifferent techniques can lead practical outcomesSampling difficult involvedAnalyzing results takes special careConflicting outcomes can confound applicationIt isn’t always worth conductingOne main difficulties consumer research sports stated observed behavior can often different. always true, can true sports. instance, season-ticket-holders may respond well survey might indicate price increases, yet still renew high rates. People try game surveys hoping responses can tilt Fortuna’s wheel 72 favor.Additionally, research methods may combined advanced discovery like purchasing data data-broker Acxiom. ’ve spoken problems third party data quite bit. Research conducted fellows Deloitte found serious inaccuracies data purchased well known data broker (John Lucker Bischof 2017).“survey findings suggest data brokers sell serious accuracy problems, may less current complete data buyers expect need.Accurate data enormous problem. Garbage-, garbage-. ’ve seen practice conducting surveys comparing stated responses questions gender, ethnicity, age data purchased market. Whenever practical, “Trust, verify.”Despite issues encounter, research fundamental component business strategy. bigger problem, important conducting worthwhile research becomes. Understanding frame questions can answered form research overlooked skill set. Getting good takes experience. Hopefully, chapter get thinking high level give rudiments toolbox can use effective confident conducting analyzing research.part, probably conduct rather simplistic satisfaction surveys email deployed games periodically throughout season. Conducting surveys follows distinct process. ’ll loosely frame chapter around process designing survey borrowed “Designing Surveys, guide decisions procedures.” (Johnny Blair 2014) steps include:Planning survey designQuestionnaire design pretestingFinal design planningSample selection data collectiondata coding, analysis reportingSince common research method likely employ emailed survey, focus. chapter demonstrate basics survey design, data preparation, analysis. ’ll also discuss philosophies creating applying results. Since already demonstrated techniques use analysis, going put focus planning survey design.","code":""},{"path":"chapter9.html","id":"designing-experiments-and-hypothesis-testing","chapter":"9 Consumer Research","heading":"9.1 Designing experiments and hypothesis testing","text":"many ways design experiment also extremely deep subject. Additionally, nothing give case impostor syndrome faster trying become expert clinical experiment design. stuff full confusing statistics jargon combines multiple knowledge bases. Since putting together specific designs (say fractional factorial design) fairly mechanical, can follow recipe correctly. Making sure underlying assumptions regarding analysis plan fulfilled can much challenging. Luckily, aren’t going subject peer review mechanism people conduct research profession. ’ll try hit high points demonstrate couple practical examples. terms framework discussion, used “Designing experiments analyzing data” (Scott E. Maxwell 1990) Maxwell Delaney “Applied Survey Data Analysis” (Steven G. Heeringa 2010) Heeringa, West, Berglund high level reference.Experiment design really validity. concerned results merit real-world applications. also multiple types validity statistical conclusion validity73 construct validity74. Much analysis covered date statistical conclusion validity. , results meet specific criteria related testing hypothesis. Construct validity refers drawing correct conclusions analysis. many potential problems validity experiment. Always keep back mind.’ll also hear lot common terminology designing experiment. common terms hear likely calls Treatment Control group. basic structure need /B ^ [https://en.wikipedia.org/wiki//B_testing] test. sounds straight-forward enough. One group receives stimulus (perhaps specific advertising mechanism) another group doesn’t. use fact test whether stimulus measurable impact specific outcome purchase intent. designs tests can simple case /B test complex. Ultimately, hope subject findings tested groups statistical tests.looking level assuredness answering question whether valid differences individuals two populations. Afterward, simply interpret everything correctly. luck, ’ll able build causal models using results.","code":""},{"path":"chapter9.html","id":"hypothesis-testing","chapter":"9 Consumer Research","heading":"9.1.1 Hypothesis testing","text":"Hypothesis testing confusing just sentence structure encounter. don’t statistics background, need take time understand hypothesis testing. next couple paragraphs simply going touch high points. steps statistics want consider. start pecifying hypothesis. Let’s use example.want understand season ticket holders spend money concessions non season ticket holders. Null hypothesis mean spend season ticket holders - mean spend non season ticket holders = 0. reject null hypotheses implies mean spend two populations equal 0. Go ahead reread sentence. know doesn’t make sense. Don’t blame . didn’t create stuff.Let’s take sample groups. ’ll create fake data exercise.Table 9.1: Sample spend dataWe going use sample make inference general population. four possible outcomes hypothesis test:null hypothesis false. reject . means equal zero season ticket holders spend differently regular fans.null hypothesis true. reject . season ticket holders spend average regular fans.null hypothesis true, reject . called type error.null hypothesis false fail reject . called type II error.basic way test hypothesis using student’s t test. Let’s apply take look output.Table 9.2: t.test resultsIn case reject null hypothesis differences means zero. alternative hypothesis true. also alternatives base t.test. also need consider sample sizes equal assumptions making actually true.","code":"\n#-----------------------------------------------------------------\n# Create spend data\n#-----------------------------------------------------------------\nsth <- as.data.frame(matrix(nrow=1000,ncol = 2))\nfan <- as.data.frame(matrix(nrow=1000,ncol = 2))\n\nnames(sth) <- c('spend','type')\nnames(fan) <- c('spend','type')\n\nset.seed(715)\nsth$spend <- rnorm(1000,25,7)\nfan$spend <- rnorm(1000,18,7)\nsth$type <- 'sth'\nfan$type <- 'fan'\n\n# Sample data\nset.seed(354)\nsth_samp <- sth[sample(nrow(sth), 400), ]\nfan_samp <- fan[sample(nrow(fan), 400), ]\n#-----------------------------------------------------------------\n# T Test\n#-----------------------------------------------------------------\nt_test <- t.test(sth_samp$spend,fan_samp$spend)"},{"path":"chapter9.html","id":"sampling","chapter":"9 Consumer Research","heading":"9.1.2 Sampling","text":"brings another major consideration experiment design. validity experiment depend heavily sampling methodology. questions asking sample? Many questions related sample potentially bias results:sample representative population trying test?sample large enough apply broader population?sample need random?can imagine, get complex.preceding example, know many people sample? can use power analysis75 give us idea. ’ll use pwr (Champely 2020) package evaluate data. following example adapted Kabacoff (Kabacoff 2011).results test table 9.3.Table 9.3: t.test resultsAccording test, need 127 participants group order calculate effect size .4 90% certainty five percent chance concluding difference isn’t. , reread sentence.extremely simplistic example. number potential issues underlying normality data, whether values independent, samples correlated, many . aren’t going go detail. just need understand getting good sample takes little rigor. Use reference follow process. ’ll able explain later (might just explaining ).","code":"\n#-----------------------------------------------------------------\n# Power analysis\n#-----------------------------------------------------------------\neffect_size <- 3/sd(sth_samp$spend)\nsignif_lvl  <- .05\ncertainty   <- .9 \n\npwr_test <- pwr::pwr.t.test(d         = effect_size,\n                            sig.level = signif_lvl,\n                            power     = certainty,\n                            type      = 'two.sample')"},{"path":"chapter9.html","id":"experimental-design","chapter":"9 Consumer Research","heading":"9.1.3 Experimental design","text":"One common experiment design called factorial design. 76. Factorial designs common similar everything else, can spiral complexity. goal make sure testing possible combination factors test subject. note, full factorial design isn’t always feasible comes cost trade-offs. may possible deploy many surveys like. case fractional factorial design may used. 77 section going cover simplified example. Honestly, wouldn’t really use something like sports unless designing conjoint formal experiment. also many designs combinatorial design78. interested , ’ll research methods . just want aware . nothing else, can impress third party researcher likely encounter future.Let’s take look basic factorial design experiment. use AlgDesign (Wheeler 2022) library build design. Let’s assume two sections want look : Dugout Seats Homeplate seats. want understand consumer preferences section based high, low, medium price. Let’s say three potential prices.Table 9.4: Factorial design surveyIn case nine different combinations need put front survey recipients. numbers represent factor tested. first three rows indicate high, medium, low prices dugout seats tested low price home plate seats:survey give respondent option purchase either seat level:seat prefer indicated prices?case show picture view seats. testing fans value view specific seats relative one another. may also mixed effects79 . careful sorts experiments. analyze results? ways. Let’s create data see .survey responses look like :Table 9.5: Factorial design surveyWe can create interaction plot following code.\nFigure 9.1: Perceptual map Nashville Sports Properties\nplot suggest effect view based seating location consistent single game ticket buyers season ticket holders. season ticket holders tend value views equivalent, single game buyers may willing pay home plate seats given choice home plate dugout seats. can see complex designs potentially become. simple example, gives basics design experiment produce basic analysis.","code":"\n#-----------------------------------------------------------------\n# Factorial Design\n#-----------------------------------------------------------------\nlibrary(AlgDesign)\n\nfd <- gen.factorial(levels   = 3,\n                    nVars    = 2,\n                    center   = TRUE,\n                    factors  = FALSE,\n                    varNames = c(\"Dugout Seats\",\n                                 \"Homeplate Seats\"))\n#> Warning in xtfrm.data.frame(x): cannot xtfrm data frames"},{"path":"chapter9.html","id":"planning-and-design","chapter":"9 Consumer Research","heading":"9.2 Planning and design","text":"Since covered segmentation chapter 4 ’ll base example around gathering information related building pyscographic segments. useful example one many things. opportunities :Hypothesis test. instance, certain segments tend spend others?Look causal links attitudinal data long term purchase behavior.Discover groups within population follow specific behavioral patterns.look look correlations behavior specific activities.best way illustrate process creating specific scenario.markeing department Nashville Game Hens interested approaching marketing efforts sophisticated fashion. want understand couple different things fanbase:understand market structure. coming games compare general population region? want break market structure logical segments can discovered multiple channels use marketing campaigns.understand something brand affinity means. brand percieved market?two broad problem statements. can see topics might cover multiple facets price sensitivity, media consumption, general interests, financial status. can also see question two won’t allow us use internal data. forced collect . ’ll clearly need carefully structure plan order make sure actually answer questions asking. fact, two statements didn’t really ask questions need . going contextually interpret internal client.need structure plan attacking effort can sure address request. actually concerned ? Let’s interpret questions:Question one concerned generalization fan. basically need create specific number groups people alike groups compare broader population sample includes fan base regional national population. can already tell difficult. one-hundred-thousand people sample really one-hundred-thousand segments. Attempting generalize arbitrary number boxes can well understood difficult.Question two concerned individual feels brand. competitive set may need uncovered. sports teams region measure order evaluate brand? broader competitive set consider? -park experience impact brand? isn’t run---mill satisfaction survey. going require work.Another main concerns face example influences decision-making-process tension discoverability, accuracy, utility. analysis used target specific individuals? analysis something can cast broader segments people understand fit segmentation scheme? accurate results ? aren’t accurate utility promises expensive project may significantly reduced. Make sure everyone clearly understands expectations trade-offs.","code":""},{"path":"chapter9.html","id":"understanding-the-makeup-of-a-clubs-fans","chapter":"9 Consumer Research","heading":"9.2.1 Understanding the makeup of a clubs fans","text":"brand perspective, sports special. don’t typically see someone walking around city hat advertising favorite shampoo. Sports crosses incredibly wide swath individuals major cyclical components. Endemic concerns might include:several ticket classes relatively small populations -sized revenue contributionResponses may different season different times seasonThere multiple levels forms consumptionReferencing first point, corporations may make large portion season tickets. think demographic information use ? distinction someone purchases tickets business versus fan purchases specifically enjoy game? enjoying game actually mean? Individuals may consume number formats channels controlled club controlled club. many implicit explicit signals might imply fandom. mainly concerned ticket sales segmentation project?case, concerned ticket sales. seen techniques. factor analysis conducted section 5.4 example potentially useful technique. data come survey asking someone attends game attend game. point plan data collecting. knew going use factor analysis derive insight survey designed survey accommodate fact.Ticket sales typically public information teams tending announce sales 80. Let’s take moment analyze data MLB 2019.teams high number ticket sales. many individuals think represents? make assumptions.Average 20,000 FSEs season ticketsSeason ticket holders buy average 4 ticketsSingle game buyers come average 1.5 games buy 3 tickets per gameGroups larger 20 make 10% salesThere many assumptions make, point clear. looking purchasers opposed people attended games, results may look different. Additionally, aren’t considering fans watch TV follow media. default-fans? Fans identify team, don’t exhibit explicit signals fan. people purchase merchandise, don’t consume primary product? Market structure complex industry.question remains. large actual market precisely attempting segment? answer question might dreaded “Yes.” real answer, like everything analytics depends. project forks different directions depending answer. Let’s refer back question. two objectives:understand market structure. coming games compare general population region?understand something brand perceived interpret perception may useful making decisions.might obtain information question 1? least part data probably already exists.governmental data access like census data?Observational researchEmailed surveysThird party researchObservational research simple walking onto concourse game writing see. Technologies also deployed . Facial recognition technologies 81 can even automate procedure large extent.get information number 2? Number two also looks like combination techniques:Emailed surveysThird party dataWhat features interested looking ? stage won’t know matter. instance, looking people might purchase season tickets, wealth factors like household income might important. minimum ’ll want basic demographic features :AgeNumber childrenMarriage statusGenderEthnicityWhy want feature ethnicity? matter ethnic group individual belongs ? useful advertising department terms channel location particular OOH efforts. Additionally, data used longitudinally variety purposes.Look carefully potential questionnaire data quality issues. Carefully considering question hand best way fetter issues may manifest later stage. time spend considering question, better final results .","code":""},{"path":"chapter9.html","id":"selecting-the-right-research-method","chapter":"9 Consumer Research","heading":"9.2.2 Selecting the right research method","text":"sports team may several needs consumer brand-equity perspective. However, sports fandom can fickle thing. discussed, fandom exists many reasons. Switching teams isn’t easy switching different hair treatment. Fans tend sticky, one main reasons research often takes back-seat decision making process. However, sports clubs facing headwinds along several dimensions regarding consumer behavior brand loyalty media consumption (Jeff Fromm 2018). Properly developed deployed research method confront issues informed way. sports, slightly different needs research relative industries unique brand positioning fans fashionability ebbs flows along team fortunes. questions might include:competitive set?consumers like product?consumer’s constraints (financial, monetary)?fan’s awareness levels sponsors?fan’s think prices?perceived market relative properties?introduce loyalty program?Different needs different requirements terms research. also multiple types surveys designed specific tasks. specialty surveys must designed particular way require knowledge interpret. Examples include:Conjoint experiments82Van Westendorp83Max-Diff84Predictive market studies85While significant variation within techniques, relatively techniques commonly deployed. Different techniques also strengths weaknesses. Van Westendorp survey may demonstrate prices certain products (like seating section) perceived, doesn’t inform us willingness--pay (covered chapter 6. conjoint experiment might give better indication willingness pay. also design formal field experiments accomplish thing.studies might require longer time horizons. Longitudinal studies may useful measuring change perception time. Like topics covered, focused research looking uncover causal links activity outcome tends goal. stages several substages need thought-. planning process critical.","code":""},{"path":"chapter9.html","id":"building-and-designing-the-questionaire","chapter":"9 Consumer Research","heading":"9.2.3 Building and designing the questionaire","text":"’ve seen building surveys can extremely complex trying build durable study. section ’ll focus building questionaire. Coding survey questionaires painful work. try follow following principles:plan going use analyze data.Don’t ask question going use .Don’t survey much often.Pay attention survey burden terms lengthSome surveys may focused looking uncover just one pieces insight. surveys may broad exploratory nature. (Johnny Blair 2014) Luckily, many research tools can help design effective surveys. Products Qualtrics86 Survey Monkey87 even automatically analyze survey terms burden. also contain banks questions can use resource.Often times ’ll break larger survey specific sections:DemographicsBrandSatisfactionProductAs rule, try keep surveys short relevant possible. Think carefully sequencing. instance, important thing need know? Put questions front followed qualifying information.","code":""},{"path":"chapter9.html","id":"qualifying-your-sample","chapter":"9 Consumer Research","heading":"9.2.4 Qualifying your sample","text":"often critical qualify sample. Qualifying sample make results relevant may reduce costs 88 deploying survey. questions may behavioral demographic. section might ask questions :gender?MaleFemaleThis important might quota. sample 85% male may desirable. also guides ask questions increasingly becoming controversial. Inclusivity also something consider. can find number guides writing inclusive surveys89. consistnet. Additionally, might quota limits specific ethnicities.ethnicity?White CaucasionBlack African AmericanEastern IndianAsianNative American Pacific IslanderLatinxDid denote ethnicities correctly? ’s important think different nomenclatures go fashion. example, Latinx begun replace “Hispanic Latino.” judge merits replacing terms. Just aware everyone opinion . Build backstop awareness. Hispanic also difficult concept. makes someone Hispanic? also black white? important? ’ll make judgment calls based trying achieve survey.people may even poison sample. Many surveys qualify question :anyone family work marketing research?someone works marketing research may understand mechanisms trying deploy. bias results certain types questions conjoint analysis.sports, also common ask fan avidity:fan Nashville Game Hens?However, often unwise ask specific avidity screening section. better method might ask question like :Please rate following activities terms interest:survey distributed third party, won’t giving away fact survey Game Hens. might also allow disqualify anyone isn’t interested attending game. depends trying accomplish.addition screening questions, dummy questions typically added survey guard indifferent respondents. instance, might embed question like survey:Please select fourth answer question:Answer 1Answer 2Answer 3Answer 4You surprised many people get question wrong. probably best disqualify responses someone gets question incorrect.","code":""},{"path":"chapter9.html","id":"typical-survey-questions","chapter":"9 Consumer Research","heading":"9.2.5 Typical survey questions","text":"certain questions almost always tend asked surveys. include section demographics usually interest commonly included questionnaires. preferred ways broach subject think important discuss specifically.able track respondents, may already demographic information. survey doesn’t require answer question important think information sequenced survey. good practice ask respondent like answer demographic questions:like answer demographic questions?prevent people ignoring rest survey aren’t interested answering demographic questions. might also consider putting demographic questions end survey. ensure collect information might deem important. Additionally, ’ll want consider using data longitudinally. Changing format questions may make analysis difficult comparing data older data sets.Let’s walk typical examples demographic questions. seems little pedantic, ask questions important.Instead asking age, prefer ask birth year calculate age:year birth?make easier compare data data collected past future. also makes easy place people demographic buckets. Additionally, ’ll able build accurate scatter plots data. can, try collect numerical data. age can sensitive subject, haven’t seen problem simply asking birth year.Asking educational achievement also common. care? Education often proxy wealth. Someone may comfortable telling income, education isn’t taboo.highest level education completed?High schoolSome collegeCollegeGraduate degreeMarital status also commonly asked. ’ll want include category . typically see many arrangements separated, divorced, widowed. don’t typically include . asking something sports going ? Yes, sports may salve loneliness, going find widowers? just don’t see lot practical application context. possible, keep simple omit can’t think possible use outside exploratory analysis. answers might simple proxies asking things sexuality.Marital Status?SingleMarriedPartnershipOtherAsking children also typical. sports seen family activities others. Additionally, stoking interest young people longer term strategic play. much effort merchandisers put appealing young people sports?children age 18 living household?YesNoAlmost sports look develop youth understanding many children attend games important. Knowing age children also important. big difference 5 year old 16 year old. respondent answers ‘yes’ preceding question typically present something like following question:3.. Please select age(s).\n- 0 2\n- 3 5\n- 6 10\n- 11 15\n- 16 17In case using multi-select. multi-select difficult work back-end require additional data wrangling. complicated question structure, less likely actually use . strike balance bewteen ideal practical.Geography especially important club. Season ticket holders likely live relatively small area surrounds stadium ballpark. Idiosyncrasies market also make understanding relative geography important. lack rail infrastructure may make stadium less accessible certain parts region. generalized way get geography asking zipcode.Please enter zipcode:ticketing shipping info, may need data point. obviously best get actual address can collect latitude longitude, zipcode next best readily available piece information.Income little tricky. survey isn’t anonymous, many people may choose answer question deceptively answer . ’ll go ahead include net present value company life insurance figure. use money. Additionally, question interesting continuous nature. can force later like. One big decisions granular make question.annual household income?$10,000$10,000 - $49,999$50,000 - $99,999$100,000 - $149,999$150,000 - $199,999$200,000 - $249,999More $250,000This data ordinal nature, meaning want displayed specific logical order. Often questions answers randomly reshuffled. done protect bias introduced someone habitually answering questions way. answers need randomized, question long, might make sense alphabetize answers. Let’s look example:answer best describes occupation?Administration/ManagerialAgricultureClerical/White CollarCraftsman/Blue CollarEducatorFinancialHomemakerLegalMedicalProfessional/TechnicalRetiredSales/ServiceSelf EmployedStudentOtherAre occupations logically organized? Agriculture occupation industry? care someone military works youth pastor church? color question. don’t specific use , may consider omitting . likely attract large number responses. someone accountant consider Clerical Finance? don’t typically like question.many demographic questions conceived, preceding examples cover basics likely deploy. Pay attention answers displayed data type. stuff always requires specific care.","code":""},{"path":"chapter9.html","id":"avidity-questions","chapter":"9 Consumer Research","heading":"9.2.5.1 Avidity questions","text":"Understanding avidity also important sports. also tends misunderstood. can measure someone’s avidity sports? several explicit implicit signals might demonstrate avidity. individual following example avid fan?person purchases season tickets $700A person purchases season tickets $20,000A person purchases single game tickets $1,500These explicit signals likely demonstrate level affinity avidity. also interactions play. actually just measuring wealth? cautious avidity. simply organize spend leveled buckets probably aren’t measuring avidity.Implicit signals subtle.person purchases merchandise onlineA person visits website oftenA person enrolls child free kid’s club membershipThe person might purchased items gift. just don’t know. might also ask person specific questions. common one might :big fan ?love Game Hens. live die .really like Game Hens. one favorite teams.identify Game Hens, casual fan.don’t realy care Game Hens. fan.hate Hens. hope lose every game.tend think sort question likert scale. Something like 1-5 1-7. typically used convention can think lies underneath questions way. “love Game Hens. live die .” represents five. can see difficult word questions. Pay attention . can easily word question ambiguous difficult understand. Additionally, include odd even number answers. even number responses makes someone pick one side . useful information.","code":""},{"path":"chapter9.html","id":"brand-affinity","chapter":"9 Consumer Research","heading":"9.3 Brand affinity","text":"Gauging brand avidity difficult. ’ll explore tool take look one component brand avidity called perceptual map. ’ll use library anacor (Mair De Leeuw 2022) accomplish part task. package build assist correspondence analysis90. little research topic.function used aggregated perceptual data created chapter two. question read like :feel following sports properties? Please check apply teams listed.initial data set took form multi-select table:code produces map figure 9.2\nFigure 9.2: Perceptual map Nashville Sports Properties\nread perceptual map? case, Chickens tend seen Fun Expensive relative properties. mean? Perhaps want seen innovative. seen innovative market may make us attractive certain sponsors. Whatever motivation, perceptual map allows visualize feelings around brand relative brands. can use time take measurements around feelings relative initiatives hope alter perception favor.","code":"\n#-----------------------------------------------------------------\n# Calculating scores for perceptual data\n#-----------------------------------------------------------------\nlibrary(anacor)\npd <- as.data.frame(FOSBAAS::perceptual_data)\nrow.names(pd) <- c('Chickens','Titans','Predators') \n\nanHolder <- anacor(pd,ndim=2)\n\nanHolderGG <- \n  data.frame(dim1 = c(anHolder$col.scores[,1],\n                      anHolder$row.scores[,1]), \n             dim2 = c(anHolder$col.scores[,2],\n                      anHolder$row.scores[,2]),\n             type = c(rep(1,length(anHolder$col.scores[,1])),\n                      rep(2,length(anHolder$row.scores[,1]))))\n#-----------------------------------------------------------------\n# Create perceptual map\n#-----------------------------------------------------------------\nlibrary(scales)\nlibrary(RColorBrewer)\nlibrary(grid)\nlibrary(gridExtra)\n\nbasePal     <- c('grey60','mediumseagreen','grey40','steelblue1')\n\ng_xlab     <- '\\n PC1 (55.7%)'\ng_ylab     <- 'PC2 (44.3%) \\n'\ng_title    <- 'Nashville Area Sports Perceptual Map \\n'\n     \nPM <- \n  ggplot(data = anHolderGG,aes(x=dim1,y=dim2,\n                               colour=factor(type)))         +\n  geom_point(size = .5, fill = 'white', colour = 'White')    +\n  geom_segment(data = anHolderGG, aes(x = 0, y = 0, \n                                          xend = dim1, \n                                          yend = dim2), \n                  arrow = arrow(length=unit(0.2,\"cm\")), \n                  alpha = 0.75, color = \"steelblue2\")        +\n  geom_text(aes(label       =  rownames(anHolderGG)),\n                  size         = 3.2,\n                  position     = \"jitter\")                   +\n  scale_color_manual(values = basePal,\n                        name   = 'Perception',\n                        breaks = c(1,2),\n                        guide  = FALSE)                      + \n  xlab(g_xlab)                                               + \n  ylab(g_ylab)                                               + \n  ggtitle(g_title)                                           + \n  graphics_theme_1                                           +\n  geom_vline(xintercept = 0,lty=4,alpha = .4)                +\n  geom_hline(yintercept = 0,lty=4,lpha = .4)                 +\n  coord_cartesian(xlim = c(-.7,.9))              \n#> Warning: Ignoring unknown parameters: lpha"},{"path":"chapter9.html","id":"other-topics","chapter":"9 Consumer Research","heading":"9.4 Other topics","text":"didn’t cover much chapter necessity. simply much material. covered high points discussed fundamental components research hypothesis testing sampling. also covered basic survey design couple examples survey analysis. didn’t really talk causal models, examples sort research throughout book. often useful think survey design lens answering specific question causal model. Main subjects didn’t cover aware include:Social listeningBrand personality studiesModels based acquisitionProduct recommendationDeveloping positioning productsIntercept surveysIn-depth interviewsfocus groupsCompetitive intelligenceIncentivizationThe obvious subject didn’t cover social listening, cover little . Social listening become large industry dozens platforms can aid . important monitor respond social content, believe sentiment analysis little practical value. Additionally, wordclouds typical consumption techniques social interesting aren’t typically surprising informative. Feel free disagree . sentiment comes practice.find social listening value lens media equivalency. sponsorship social team needs understand much content placed social channels value content. multiple ways , likely need help platform task. Like media content evaluation, ’ll need take volume, quality, arbitrary value estimate much returning sponsors.going give nod focus groups. focus groups interesting sports? believe serve role public relations terms season ticket holders. especially true outside traditional focus group might looking customer preferences constraints. Fan councils become important, care must taken ensure groups remain task. can tendency degrade complaint centers.aren’t going mention topics . want learn , resources available everywhere. also hundreds agencies gladly work . Sports teams attractive marketing platforms working prestigious. Take advantage prestige. trick know quality research entails find partner understands well. difficult task think. ’ve warned.","code":""},{"path":"chapter9.html","id":"key-concepts-and-chapter-summary-8","chapter":"9 Consumer Research","heading":"9.5 Key concepts and chapter summary","text":"Research foundation strategic initiatives. Whether improving operations process, pricing tickets, building pitch decks sponsorship team, producing credible research critical. covered five main topics chapter:Planning survey designQuestionnaire design pretestingFinal design planningSample selection data collectiondata coding, analysis reportingResearch enormous topic, focusing wildly used practices can develop sound strategies leveraging creating research. covered number important concepts linked activities chapter 5 chapter 6.also covered sampling, typical question styles, data types, two specific examples covering brand affinity conjoint experiments. Keep principles mind deploy surveys:can’t always trust itDifferent techniques can lead practical outcomesSampling difficultAnalyzing results takes special careConflicting outcomes can confound applicationIt isn’t always worth conducting","code":""},{"path":"chapter10.html","id":"chapter10","chapter":"10 Operations and Analytics","heading":"10 Operations and Analytics","text":"sports team two fundamental core-competencies events management selling tickets. two efforts work takes place. Additionally, smooth operation critical brand perception. pillar product details-oriented business.overarching umbrella operations club context covers getting fans , around, facility safely, quickly, conveniently. design, system, technology components must constructed. also optimization considerations cost number point--sale units concessions retail, labor optimization security, food production, number seats particular sections, number bathrooms, etc. list extensive. Operations academic discipline studying systems ’ll apply techniques overlapping academic fields Operations Research, Industrial Engineering, Operations Management analyze solve couple ubiquitous operations problems.Many constraints find ballpark arena byproducts cost optimization. square footage campus size big influence able accomplish. Venues now designed advanced capabilities mind. capabilities might include:Ordering food beverages phoneMore expansive automated options F&BDigital ticketsMore advanced security scanning technologiesSecurity cameras feature facial recognitionIntegrated commerce systems loyalty mechanismsIntegrated public transitWhile systems help solve existing problems, can exacerbate issues. instance, attempting food delivery without considering kitchen locations lead quality control issues. Never consider technology solutions without carefully examining underlying execution capability. Businesses littered withered husks technology solutions failed deliver, ill fit, pet project inept executive, simply faded away internal external inability manage solution.Despite advances technology, venues always faced optimization problems around moving large numbers people systems short amount time. also additive factors parking public transit, nearby entertainment districts, game day promotional items, food beverage service, security screenings must considered. following sections analyzing problems detail.","code":""},{"path":"chapter10.html","id":"understanding-basic-issues-with-ingress","chapter":"10 Operations and Analytics","heading":"10.1 Understanding basic issues with ingress","text":"Capacity constraints obvious unavoidable ingress. 30,000 400,000 people trying get place time, lines unavoidable. Romans certainly understood issues well. Colosseum Rome designed 76 vomitoria accommodate 50,000 people91. Ironically, Colosseum likely efficient ingress egress modern stadium. Many venues require security check involving magnetometer. Bags must checked, tickets may need scanned, building fire codes create constraints, internal space taken concessions mechanical infrastructure. Romans didn’t worry running electricity HVAC throughout venues. issues also extend outside building apply road traffic, ADA considerations, public transit, ride-sharing partnerships.Eliyahu Goldratt published book 1984 called Goal (Goldratt 2004) introduces us Theory Constraints. book gives us simple framework approaching problem Ingress. interested Operations, must-read book. may appreciated depressing narrative, content outstanding. Goal, Goldratt introduces five-step Process -Going Improvement:Identify system’s bottlenecksDecide exploit bottlenecksSubordinate every decision ‘step two decisions’Elevate systems bottlenecksif, previous step, bottleneck broken, go back beginning.system can one bottleneck92. makes process iterative. replace weakest link chain stronger one, link become weakest. Let’s outline ingress process look data. Ingress process probably going look fairly similar virtually every venue world. typical flowchart ingress procedure can studied figure 10.1.\nFigure 10.1: Typical gate entry process\nFigure 10.1 demonstrates typical gate procedure. Although recent advances technology made process faster, basic process followed event venues. sort data extract process? typically scan data ticketing process perhaps magnetometer passes failures. ’ll know many people come gate point entry passed security. let’s take look aggregated data.Table 10.1: Aggregated scan dataThis data aggregated number scans minute summed bucketed. distribution scans can seeing figure 10.2. Can think information might useful ?\nFigure 10.2: Distribution ticket scans gates\ngeneral, ticket scans typically follow specific cadence identical events. ballpark four points entry fans point entry four scanners. can tell entry process slightly probabilistic. scans take amount time failure requires adjustments additional scan people move different paces. maximum capacity system?question actually difficult answer know gates likely busier others different times. looking gate maximum throughput duress. need multiple days observation get good average figure.Table 10.2: Maximum observed throughputThe highest observed average number scans per scanner thirteen scans per minute across ingress point. means person went system every four half seconds one minute. However, average may mean scans took ten seconds many took one second. can’t tell data. subsequent section discuss queuing theory analyze wait-time looking service times, inter-arrival times, line-length, queuing discipline. illustrating broader point example.can see missing key data. can also begin see problem sprawls becomes complex. data missing? prefer see:process components split timedInformation line-lengthSpecific information gate magnetometerInformation inter-arrival timesInformation staffing numbers procedureTrue positives false positive numbers scanner alarmsCan identify bottleneck? Judging diagram seems magnetometer triggered individual asked walk back . Perhaps change process add staff wand guests triggered gate. also eliminate bags alter magnetometer settings measure differences. solutions require much diligence. ’ll expand example look analogous project little depth. Reducing wait times concession stands.","code":"\n#-----------------------------------------------------------------\n# Access scan data\n#-----------------------------------------------------------------\nscan_data <- FOSBAAS::scan_data\n#-----------------------------------------------------------------\n# Scans\n#-----------------------------------------------------------------\nx_label <- 'observation'                                             \ny_label <- 'scans'                                          \ntitle   <- 'Ticket scans by minute at gates'\nscan_point <- \n  ggplot(data = scan_data,aes(x     = observations,\n                              y     = scans,\n                              color = date))                +\n  geom_point()                                              +\n  scale_x_continuous(label = scales::comma)                 +\n  scale_y_continuous(label = scales::comma)                 +\n  scale_color_manual(values = palette)                      +\n  xlab(x_label)                                             + \n  ylab(y_label)                                             + \n  ggtitle(title)                                            +\n  stat_smooth(method = \"lm\", formula = y ~ x + poly(x,2)-1) +\n  geom_vline(xintercept = 151, lty = 4)                     +\n  graphics_theme_1                                          +\n  guides(color = \n         guide_legend(override.aes = list(fill = \"grey99\")))\n#-----------------------------------------------------------------\n# Scans\n#-----------------------------------------------------------------\nmax_scans <- scan_data                                %>% \n             group_by(date)                           %>%\n             summarise(maxScans = max(scans),\n                       maxScansMean = (max(scans)/16),\n                       meanScans = mean(scans),\n                       medianScans = median(scans))\n "},{"path":"chapter10.html","id":"reducing-wait-times-at-concessions","chapter":"10 Operations and Analytics","heading":"10.2 Reducing wait times at concessions","text":"Everyone spent time standing line food beverage sporting event. asked someone wrong system might give couple predictable responses “many people” “people just don’t know .” reality, complex problem many components:System constraints (number grills, points--sale, etc)Labor (experience, training, motivation)Back--house systems (buffering demand, menu design)Front--house systems (Queuing discipline, ordering, payment systems)interesting thing don’t really acknowledge problem. Perhaps isn’t. can look system simply state don’t enough points sale. lines long. points sale spread lines lines shorter. course, incredibly reductive point view. Let’s little realistic begin constructing narrative:Executives Nashville Game Hens noticed long lines concession stands games. Lines tend get noticably worse weekends fans building. Additionally, customer satisfaction surveys indicate time spent waiting line concession stands major source dissatisfaction. concessions manager says “problem don’t enough points--sale deal capacity reach 30,000 fans.” poses solution require large capital investment add points sale. approached executive asked good idea…might begin solve problem making observations . might also look data available. ’ll assume don’t anything. talk concessions manager tells standard orders filled less ninety seconds. industry standard achieves average.Without data, project proceed stages. can also take cues nature problem help devise plan approaching solution. least three identified project components involve degree process improvement. Since looking process improvement, let’s see can simply apply DMAIC problem. DMAIC stands Define, Measure, Analyze, Implement, Control. comes us Six Sigma93, process improvement protocol adapted number business problems. Six Sigma ’s roots manufacturing, can appropriate tools .going use framework might well go way produce project charter lays full project plan. project plan needs include components. two concern include:Project objectivesSpecific goalsOther components charter cover include:TimelinesBudgetsStakeholdersPotential problemsI wanted point seemingly irrelevant step couple reasons. first reason project plan forces think solve problem. often difficult part. also forces create documentation around project. create basic documentation around projects eventually used benefit. problem tendency sprawl. Make habit building project charters projects involving multiple stakeholders external groups. make scoping projects much easier help keep everyone task budget.","code":""},{"path":"chapter10.html","id":"establishing-objectives","chapter":"10 Operations and Analytics","heading":"10.2.1 Establishing objectives","text":"Objectives can little vague, derive business case problem statement. Let’s take stab establishing objectives. ’ll use concepts “Six Sigma” (Campe 2007) example.business case simply statement indicates importance project. demonstrate project needs addressed:Customer satisfaction linked repeat purchases demonstrable impact year--year revenue. Additionally, customer satisfaction scores predicibly degrade attendance exceeds 30,000. Surveys concluded improving scores can best achieved decreasing wait time concessions throughout venue. 100 games left season, opportuinty demonstably impact season ticket renewals. Additionally, capital budgets must completed next sixty days.statement establish?project importantIt needs completedWe limited amount time get started","code":""},{"path":"chapter10.html","id":"understanding-our-problem","chapter":"10 Operations and Analytics","heading":"10.2.2 Understanding our problem","text":"’ll need little data begin understand problem goal collecting exploratory data. looking something help build problem statement. case, perhaps can simply go look line-length particular concept.typical concept eight fifteen items menu. Items purchased differential rates varying prep times. items can stored twenty minutes quality begins degrade. number points sale vary, concepts four twelve points sale. Customers enter queue wait point sale becomes available. place order, pay, wait point--sale order fulfilled exiting system.Queuing systems fascinating topic. detailed analysis queuing systems covered later chapter within section 10.5. However, unfamiliar queuing systems analyzed, recommend spending little time covering Queuing Theory 94. Queuing theory can extremely complex many applications across variety disciplines. simplify , just think goal design systems outcomes deterministic.Game Hens’ concession concepts typically follow multi-channel, single phase (MCSP) queuing discipline. MCSP system one-step (single phase) serving process register handles complete transaction multiple points--sale (channels) serving queue. Additionally, process follows FIFO (first-, first ) queue discipline customer waiting longest amount time serviced first.Let’s extract key elements system:Process follows FIFO queue discipline function asynchronouslyOrdering, payment, fulfillment happen point--saleFulfillment difficult buffer due differential storage times demand levelsNow little bit detail, can construct plan capturing measurements analysis.","code":""},{"path":"chapter10.html","id":"defining-our-problem-and-establishing-goals","chapter":"10 Operations and Analytics","heading":"10.2.3 Defining our problem and establishing goals","text":"strategically oriented projects tend sprawl begin think . one different. lots considerations. Let’s create problem statement unambiguously explains project.Problem statement: Wait times concessions services across ballpark consistently recieve low relative satisfaction scores (less 10% fans give highest rating) season ticket holders. Low satisfaction scores correlelated higher likelihood specific fan renew season membership. industry standards service times quoted vendor ninety seconds per transaction, observed wait times often exceed figure 100%.Goals precise objectives. Since don’t hard data process, wait establish firm goals. initial goal collect necessary data hopes can reduce wait times. stage, idea much may possible reduce wait times. Although industry standard, don’t know figure merit.","code":""},{"path":"chapter10.html","id":"measurment-and-analysis","chapter":"10 Operations and Analytics","heading":"10.3 Measurment and analysis","text":"almost never data need. case, ’ll need take note create plan gathering don’t . Sine take live measurements, ’ll need ruberic budget. Additionally, timeline impacted fact need take measurements game.","code":""},{"path":"chapter10.html","id":"data-audit-and-capture","chapter":"10 Operations and Analytics","heading":"10.3.1 Data audit and capture","text":"’ll want careful capture data. ’ll also want consistent thorough possible. Spend time thinking phase project. case, know going capture observational data. Additionally, data might make someone look bad. Since quoted ninety seconds number met, happens measurements differ? Keep sorts political impediments mind build rubric.many ways collect data context, fundamental components plan almost identical. Just google data collection plan. frameworks similar. start specific question trying answer lay specifics :Data typesWhat measuredHow data measuredHow data capturedHow ensure consistencyThe initial goal analysis identify bottleneck. able gather data information systems. know scan data captured gates know many people park given time. information may useful.Leveraging points--sale estimate wait times might possible. Perhaps cameras installed concept, use .. track clock customers. However, don’t means using people stopwatches observing concept. seems barbaric, set firm rules around constitutes measurement, collect enough data, able get reliable estimates.Additionally, concept information actually needed business perspective mature time. probably love able track every customer venue times, necessary? spend money understood every competing expense throughout business? use data? CEOs executives make decisions. Think problems broader perspective confronted . make distinction ideal practical.","code":""},{"path":"chapter10.html","id":"line-length-and-total-scans","chapter":"10 Operations and Analytics","heading":"10.3.2 Line length and total scans","text":"Figure 10.3 requires two separate data sets. Scans data line length data. already seen scan data.Table 10.3: scan data exampleLine-length data looks little different scan data. data collected manually counting number people line every minute. data also confounded fact four people may line, one may order. Likewise, one person may ordering one person. data quirks, try write . ’ll explain someone point.(#tab:line_length_a)line length dataLet’s take look see line-length relationship scans. looking see number people coming building influences line-length. following code produces graph figure 10.3.\nFigure 10.3: Relationship line length concession stand scans\ncan see pattern data line-length tends peak specific times, relationship cumulative scans isn’t tight. appears fans enter early tend rush concession stands fans come near begining game take seat go concession stands later. lot variability data.","code":"\n#-----------------------------------------------------------------\n# Create scan data\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\nscans_a <- f_get_scan_data(x_value = 230,\n                           y_value = 90,\n                           seed    = 714,\n                           sd_mod  = 10)\n#-----------------------------------------------------------------\n# Create line length data\n#-----------------------------------------------------------------\nline_length_a <- f_get_line_length(seed = 755,\n                                   n    = 300,\n                                   u1   = 22,\n                                   sd1  = 8,\n                                   u2   = 8 ,\n                                   sd2  = 5)\n\nline_length_a$action_time <- f_get_time_observations(17,21)\nline_length_a$date <- \"4/1/2024\"\n#-----------------------------------------------------------------\n# Line length and scans\n#-----------------------------------------------------------------\nscans_a$cumScans <- cumsum(scans_a$scans)\ndata  <- dplyr::left_join(scans_a,line_length_a, \n                          by = \"action_time\")\ndata$color <- as.numeric(as.character(gsub(\":\",\n                                           \"\",\n                                           data$action_time)))\n\nx_label  <- ('\\n Cumulative Scans')\ny_label  <- ('Line Length \\n')\ntitle    <- ('Line Length and cumulative scans')\nlegend   <- ('Time')\nline_length <- \n  ggplot(data, aes(y     = lineLength, \n                   x     = cumScans, \n                   color = color))                           +    \n  geom_point()                                               +\n  scale_color_gradient(breaks = c(2100,2000,1900,1800,1700),\n                       labels = c(\"9:00\",\"8:00\", \"7:00\", \n                                  \"6:00\",\"5:00\"),\n                       high = 'dodgerblue',\n                       low  = 'coral',\n                       name = legend)                        +\n  scale_x_continuous(label = scales::comma)                  +\n  xlab(x_label)                                              + \n  ylab(y_label)                                              + \n  ggtitle(title)                                             +\n  stat_smooth(method = \"loess\", formula = y ~ x)             +\n  geom_vline(xintercept = 13068, lty = 4)                    +\n  graphics_theme_1  "},{"path":"chapter10.html","id":"analyzing-the-results","chapter":"10 Operations and Analytics","heading":"10.3.3 Analyzing the results","text":"can analyze data much fashion analyzed data previous chapters. However, ’ll introduce new tool called generalized additive model. careful technique. might find silver bullet can easily model many distributions, little difficult work math equation provided regression. ’ll use mgcv (Wood 2022) library access gam function. often case, ’ll need little data preparation applying tool.can access summary gam model look standard statistics:Table 10.4: gam model outputWe can see data defy accurate fit looking figure 10.4.\nFigure 10.4: GAM output line length data\ncan see average fit data fair, predictive power won’t good given point graph. Ultimately, isn’t good relationship line-length scans ballpark. However, see data appears multi-modal. Lines decrease near start gameh. makes sense early arrivers place orders take seats.","code":"\n#-----------------------------------------------------------------\n# Generalized additive model\n#-----------------------------------------------------------------\nlibrary(mgcv)\nscans_a$cumScans <- cumsum(scans_a$scans)\ndata             <- dplyr::left_join(scans_a,line_length_a, \n                              by = \"action_time\")\n\ndata$color <- as.numeric(as.character(gsub(\":\",\n                                           \"\",\n                                           data$action_time)))\n\ngam1 <- mgcv::gam(lineLength ~ s(cumScans, bs='ps', sp=.2), \n                  data = data)\n\nnewPredict <- cbind(data, predict(gam1, interval = 'confidence'))\n\ngr <- \n  ggplot(newPredict, aes(y     = lineLength, \n                         x     = cumScans,\n                         color = color))                      +    \n  geom_point(alpha=.7)                                        +\n  scale_color_gradient(breaks = c(2100,2000,1900,1800,1700),\n                       labels = c(\"9:00\",\"8:00\", \"7:00\", \n                                  \"6:00\",\"5:00\"),\n                       high = 'dodgerblue',\n                       low  = 'coral',\n                       name = 'Time')                         +\n  geom_line(aes(y = `predict(gam1, interval = \"confidence\")`,\n                x = cumScans),\n                color = 'dodgerblue',size = 1.2)              +\n  scale_x_continuous(label = scales::comma)                   +\n  xlab('Cumulative Scans')                                    + \n  ylab('Line-Length')                                         + \n  ggtitle('Results of GAM on Line-Length Data')               +\n  graphics_theme_1   "},{"path":"chapter10.html","id":"understanding-wait-times","chapter":"10 Operations and Analytics","heading":"10.3.4 Understanding wait times","text":"Now data line-length can take closer look driving wait-time. can simulate results measuring broader process. built function f_get_wait_times() provide us data. function breaks wait-times three parts:Order time: amount time takes place orderPayment time: long took fulfill payment order placedfulfillment time: long took fulfill order paymentThis information capture direct measurements. include function creates data can see used simple exponentially distributed data components process. may may true practice, seen similar patterns actual data.function produce many results want order, payment, fulfillment times represented exponentially distributed times seconds.Table 10.5: Wait time dataWe can use data build simulation allows us deconstruct total-time change inputs. allow us demonstrate impact initiative mobile ordering. Mobile ordering theory allow effectively decouple orders possibly payment fulfillment. careful think technology first . just stated mobile ordering may help decouple orders payment fulfillment. ways accomplish goal? Additionally, might help demonstrate merits effective buffering system simplified menu.","code":"\n#-----------------------------------------------------------------\n# Simulate wait times function\n#-----------------------------------------------------------------\nf_get_wait_times <- function(seed,n = 300,time,rate1,rate2,rate3){\nset.seed(seed)\n\norder_times       <- rexp(n, rate = rate1)\npayment_times     <- rexp(n, rate = rate2)\nfulfillment_times <- rexp(n, rate = rate3)\ntotal_time        <- order_times       + \n                     payment_times     + \n                     fulfillment_times\n\nwait_times <- data.frame(transaction  = seq(1,n, by = 1),\n                         orderTimes   = order_times,\n                         paymentTimes = payment_times,\n                         fulfillTimes = fulfillment_times,\n                         totalTime    = total_time)\nwait_times[] <- apply(wait_times,2,function(x) round(x,0))\nreturn(wait_times)\n}\n#-----------------------------------------------------------------\n# Create wait times data set\n#-----------------------------------------------------------------\nwait_times_a <- f_get_wait_times(seed  = 755,\n                                 n     = 300,\n                                 rate1 = .03,\n                                 rate2 = .06,\n                                 rate3 = .15)"},{"path":"chapter10.html","id":"analyzing-the-distributions-of-wait-time-components","chapter":"10 Operations and Analytics","heading":"10.3.4.1 analyzing the distributions of wait time components","text":"Variance processes problem depending queuing discipline. Let’s take look distribution wait times.\nFigure 10.5: Distribution wait time components\ncan see graph order times appear widest variance. long tail extending toward 200 second mark. Variance kills interdependent processes. enemy looking find. reduce variance, make entire process deterministic improve wait times.","code":"\n#-----------------------------------------------------------------\n# Simulate wait times function\n#-----------------------------------------------------------------\nwait_dist <- \n  wait_times_a                                            %>% \n    select(orderTimes,paymentTimes,fulfillTimes)          %>%\n    tidyr::pivot_longer(cols = c('orderTimes',\n                                 'paymentTimes',\n                                 'fulfillTimes'),\n                        names_to = \"measurment\",\n                        values_to = \"seconds\")            %>%\n                   mutate(scale_seconds = scale(seconds))\n\nw_dist <-  \n    ggplot(wait_dist, aes(x = seconds, fill= measurment)) +    \n    geom_density(alpha=.75)                               +\n    geom_rug(color='coral',sides = \"b\")                   +\n    scale_fill_manual(values=palette)                     + \n    xlab('Seconds')                                       +\n    ylab('Density')                                       + \n    ggtitle('Distribution of wait-time components')       +\n    graphics_theme_1 "},{"path":"chapter10.html","id":"simulating-a-component-of-our-process","chapter":"10 Operations and Analytics","heading":"10.3.5 Simulating a component of our process","text":"Like many topics discussed, simulation large complicated topic. also incredibly useful problem facing. already looked coupld ways simulate data. used rexp rnorm multiple times create simulated data sets. seeing figure 10.5. creating random numbers based parameters exponential normal distributions. simply data set want simulate different conditions? know distribution fit approximate data?many software packages build simulations , useful understand might function base level. ’ll build couple basic transaction simulations evaluate well specific changes process might impact overall system. give appreciation software package might help solve problem. underlying knowledge makes better decision maker.Simulation daunting subject. Non-linear least squares fits can frustrating. Like saw chapter {#chapter6}, easy get complex math.Table 10.6: Simulated wait times dataLet’s begin looking correlations process component. looking see particular component closely correlated total-time. packages build correlation tables , go ahead build manually.’ll reorder factors graph give us aesthetic looking .’ll use forcats (Wickham 2022a) library access fct_reorder function. help order plot.\nFigure 10.6: Correlation table\nOrder time component highly correlated Total-Time. factors don’t appear correlated . infer sorts things data. Perhaps order time highly correlated total-time work can take place order. Fulfillment time may least correlated total time done good job buffering inventory. ooking see basic relationship variables. fact order time highly correlated variable important ’ll likely base solution around mitigating long order times likely impact.","code":"\n#-----------------------------------------------------------------\n# Simulate wait times function\n#-----------------------------------------------------------------\nwait_times <- FOSBAAS::wait_times_data[1:300,]\n#-----------------------------------------------------------------\n# Build correlation table\n#-----------------------------------------------------------------\nwt_cor <- wait_times[,-1]\nnames(wt_cor) <- c('Order Time','Payment Time',\n                   'Fulfillment Time','Total Time')\n\nwt_cor_result      <- cor(wt_cor)\nwt_cor_result      <- round(as.data.frame(wt_cor_result), 2)\nwt_cor_result$type <- row.names(wt_cor_result)\n\ncor_long <- \n  tidyr::pivot_longer(wt_cor_result,\n                      cols = c('Order Time','Payment Time', \n                               'Fulfillment Time','Total Time'))\n#-----------------------------------------------------------------\n# Build correlation table\n#-----------------------------------------------------------------\ncor_long$order <- factor(cor_long$type, \n                         levels=c('Fulfillment Time',\n                                  'Payment Time',\n                                  'Order Time',\n                                  'Total Time'))\n#-----------------------------------------------------------------\n# Correlation table \n#-----------------------------------------------------------------\nlibrary(forcats)\ncorrelation_table <- \ncor_long                                                 %>%\nmutate(name = fct_reorder(name, value, .fun='sum'))      %>%\nggplot(aes(x = order, y = name,fill = value))            +\n  geom_tile()                                            +\n  geom_text(aes(label=value))                            +\n  scale_fill_gradient2(low  = \"mediumseagreen\", \n                     mid  = \"white\", \n                     high = \"dodgerblue\")                +\n  xlab('')                                               + \n  ylab('')                                               + \n  ggtitle('Correlation table')                           +\n  graphics_theme_1                                       +\n  theme(axis.text.x = element_text(size = 8),\n        axis.text.y = element_text(size = 8))"},{"path":"chapter10.html","id":"analyzing-the-variance-between-wait-time-components","chapter":"10 Operations and Analytics","heading":"10.3.5.1 Analyzing the Variance between wait time components","text":"Variance enemy. system components linked depend , correlated. also wide amount variance.\nFigure 10.7: Variance components\ncloser look specified quantiles paints clearer picture.Table 10.7: quantiles wait timesTwenty-five percent orders take forty two seconds . appears biggest problem. accurate say process component likely causes variance system. Let’s construct simulation looking probability table observations. can build function create table little extra information. also beneficial little data. One day transactions certainly create sample-size issues. Let’s simulate thousand results. ’ll pretend collecting information couple weeks. Keep mind also cause problems. using regression might want consider mixed effects 95 model.Since total time composed three process components, can sure regression work pretty well terms explaining variance. regressed data model look amazing. Let’s try .Table 10.8: Summary stats time modelAs expected, model appears almost perfect. use equation estimate total wait times almost perfectly. see results like , probably something wrong.","code":"\n#-----------------------------------------------------------------\n# Box plot of wait times\n#-----------------------------------------------------------------\nwait_long <- \n  tidyr::pivot_longer(wait_times,cols = c('orderTimes',\n                                          'paymentTimes', \n                                          'fulfillTimes',\n                                          'totalTime'))\nwait_box <- \nwait_long                                             %>%\n  mutate(name = fct_reorder(name, value, .fun='sum')) %>%\n  ggplot(aes(x = name, y = value)) +\n  geom_boxplot(fill = 'dodgerblue') +\n  xlab('\\n Transaction component')                              + \n  ylab('Wait time in seconds')                                  + \n  ggtitle('Variance of transaction times')                      +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Quantiles\n#-----------------------------------------------------------------\nquantiles <-\napply(wait_times[,-1],\n      2,\n      function(x) quantile(x, probs = c(.1,0.25, 0.5, 0.75,.9)))\n#-----------------------------------------------------------------\n# Simple linear model for total times\n#-----------------------------------------------------------------\ntime_mod <- lm(totalTime ~ fulfillTimes + paymentTimes + \n                           orderTimes,data = wait_times)\n\nstats_time_mod <- \ntibble::tibble(\n  st_error     = unlist(summary(time_mod)[6]),\n  r_square     = unlist(summary(time_mod)[8]),\n  adj_r_square = unlist(summary(time_mod)[9]),\n  f_stat       = unlist(summary(time_mod)$fstatistic[1])\n)"},{"path":"chapter10.html","id":"building-a-simulation-of-our-data","chapter":"10 Operations and Analytics","heading":"10.3.5.2 Building a simulation of our data","text":"Finally, let’s build simulation. section demonstrate basics get started. However, method follow flexible can used many different ways. ’ll begin accessing wait_times_distribution data.let’s take sample wait times can use basis simulated data.Now can begin think distribution little carefully. can use ecdf function compute empirical cumulative distribution function. can think output inverse quantile function.Table 10.9: Demonstrating relationship quantiles ecdfAbout 78 percent observations less 50 seconds. simple plot function visualize just .\nFigure 10.8: Order time ecdf\ncan use function simulate data fits distribution. Let’s take random sample data range order times.can use histogram data compare actual order data.\nFigure 10.9: Simulated vs. actual results\nsimulated data approximates actual data well. , know used exponential distribution fit data.can’t trust results one simulation. ’ll want create many simulations average results. Let’s go ahead put together code base allow us produce reproducible experiment. First, let’s ask question want answer:decreased order times fifty percent, change average wait time?now five-hundred simulations phase total wait. Let’s take look averages.Table 10.10: Reducing impact order timesThe total average time almost identical observed. can reduce order time fifteen seconds get total time 41 seconds. However, seen tells part story. real problem variance. However, distributions fact payment, fulfillment, orders correlated, variance unlikely hurt us often.Let’s use simulation calculate stats many orders fulfilled specific time frames per point--sale duress. means queue always full. Let’s ask question answer different ways.one hour, much additional throughput register able process order taking reduced 50%?average wait time fifty six seconds means can fulfill sixty four orders per register per hour (60 seconds/56 seconds) * 60 minutes. number look like use samples simulation?Let’s graph results count list see many orders able fulfill thirty simulated hours.\nFigure 10.10: Simulated vs. actual results\nthirty simulations, eight fell level average service time. compound analysis multiple registers, inter-arrival times, different line-lengths, can see reducing variance key consistently maximizing throughput. also didn’t talk slack time orders, fatigued employees, factors add variance process.","code":"\n#-----------------------------------------------------------------\n# Access the distribution data\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\nwait_times_distribution <- FOSBAAS::wait_times_distribution_data\n#-----------------------------------------------------------------\n# Get a sample of the data\n#-----------------------------------------------------------------\nset.seed(755)\nwait_sample <- wait_times_distribution %>%\n               sample_frac(size = .7)\n#-----------------------------------------------------------------\n# Cumulative distribution function\n#-----------------------------------------------------------------\norders <- wait_sample$orderTimes \ncdf    <- ecdf(orders)  \n#-----------------------------------------------------------------\n# Cumulative distribution function\n#-----------------------------------------------------------------\ncdf_out <- cdf(50)\nqua_out <- quantile(wait_sample$orderTimes,probs = cdf_out)\n\necdf_quant <- tibble::tibble(ecdf_50 = cdf_out,\n                             quantile_ecdf_50 = qua_out)\n#-----------------------------------------------------------------\n# Cumulative distribution function plot\n#-----------------------------------------------------------------\ncdf_plot <- \nggplot(wait_sample, aes(orders))            + \n  stat_ecdf(geom = \"step\",\n            color = 'dodgerblue',\n            size = 1.1)                     +\n  xlab('\\n Orders')                         + \n  ylab('Percentage observations')           + \n  ggtitle('ECDF Order Time')                +\n  geom_vline(xintercept = 50.22143,lty = 2) +\n  geom_hline(yintercept = .7785714,lty = 2) +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Simulate orders \n#-----------------------------------------------------------------\nset.seed(715)\nn <- 400 # observations\nsim_orders <- rexp(n, rate=1/mean(wait_sample$orderTimes))\n#-----------------------------------------------------------------\n# Compare histograms\n#-----------------------------------------------------------------\nhist_tab <- \n tibble::tibble(sim_data = sim_orders,\n                actual_data = sample(wait_sample$orderTimes,400))\n\nhist_tab_long <- hist_tab %>% \n                 tidyr::pivot_longer(cols = c('sim_data',\n                                              'actual_data'),\n                                     names_to = \"measurment\",\n                                     values_to = \"seconds\") \n\nhist_comp <- \nggplot(hist_tab_long , aes(seconds))     + \n  facet_grid(.~measurment)               +\n  geom_histogram(fill = 'dodgerblue')    +\n  xlab('\\n seconds')                     + \n  ylab('count')                          + \n  ggtitle('Simulated vs. actual values') +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Simulate, Iterate, and aggregate\n#-----------------------------------------------------------------\nn <- 500 # observations\n\nsim_orders  <- list()\nsim_pay     <- list()\nsim_fulfill <- list()\n# Iterate\nfor(i in 1:500){\nset.seed(i + 715)\n# Simulate\n  sim_orders[[i]]  <-  \n    rexp(n, rate=1/mean(wait_sample$orderTimes))\n  sim_pay[[i]]     <-  \n    rexp(n, rate=1/mean(wait_sample$paymentTimes))\n  sim_fulfill[[i]] <-  \n    rexp(n, rate=1/mean(wait_sample$fulfillTimes))\n}\n# Aggregate\nmean_order   <- mean(sapply(sim_orders, mean))\nmean_pay     <- mean(sapply(sim_pay, mean))\nmean_fulfill <- mean(sapply(sim_fulfill, mean))\nmean_total   <- mean_order + mean_pay + mean_fulfill\n#-----------------------------------------------------------------\n# Aggregated results\n#-----------------------------------------------------------------\nmean_chart <- tibble::tibble(order   = mean_order,\n                             payment = mean_pay,\n                             fulfill = mean_fulfill,\n                             total   = mean_total)\n#-----------------------------------------------------------------\n# Results with variance\n#-----------------------------------------------------------------\ntotal_time_samp <- sim_orders[[1]]  +\n                   mean_pay[[1]]    + \n                   mean_fulfill[[1]]\ncount_list <- list()\nset.seed(715)\n\nfor(j in 1:30){\n  time            <- 0\n  count           <- 1\n  seconds_in_hour <- 60*60\n    while(time <= seconds_in_hour){\n      i           <- sample(total_time_samp,1)\n      time        <- time + i\n      count       <- count + 1\n    }\n  count_list[j]   <- count - 1\n}\n#-----------------------------------------------------------------\n# Observe variance in fans serviced per hour\n#-----------------------------------------------------------------\ncounts <- tibble::tibble(fans_serviced = unlist(count_list),\n                             simulation    = seq(1:30))\n\nservice_per_hour <- \nggplot(counts , aes(x = simulation,\n                    y = fans_serviced))  + \n  geom_line(color = 'dodgerblue')        +\n  xlab('\\n simulation')                  + \n  ylab('Fans serviced')                  + \n  ggtitle('Simulated services per hour') +\n  geom_hline(yintercept = 64,lty = 4)    +\n  graphics_theme_1"},{"path":"chapter10.html","id":"fitting-distributions","chapter":"10 Operations and Analytics","heading":"10.4 Fitting distributions","text":"Let’s take look ways fit distributions data. ’ll begin building helper function give us frequency table values.’ve already created data set. Access package.Table 10.11: Frequency table order timeThis frequency table represents histogram. Let’s take look graph.\nFigure 10.11: ecdf\ngives us cumulative distribution graph looked earlier.following sections cover fitting different models data. libraries much work inclined. Let’s look different potential fits data begin three common options:exponential fitA polynomial fitA generalized additive modelWe’ll also attempt fit logit curve demonstrate important point.Keep mind fit polynomial three degrees asking trouble. demonstrate good might look, bad idea. asking unpredictable model. Don’t .fit logit curve ’ll something little differently. ’ll use SSlogis function stats package estimate starting points curve. useful function, keep mind.get error. ? Basically, logit curve poor fit data set. common problem. can’t fit common distribution well? options. included fifth degree polynomial fit (, terrible practice), generalized additive model, ’ll introduce one . Let’s fit spline96.Now fit models, can judge one best? can start looking well fit data.\nFigure 10.12: Variance components\nmodels seem fit cumulative data pretty well exception exponential fit polynomial fit. several ways evaluate models one another. linear models, ANOVA typically first stop. non linear models, AIC 97 BIC typically used.Running get_diagnostics creates following table.Table 10.12: Model ComparisonsThe exponential line isn’t best fit. Plus, select random numbers frequency plug equation simulated distribution won’t approximate data well. Let’s simulate results using polynomial fit.can write function spit-new value based polynomial model.plug result earlier get:pretty close 50 got ecdf function. Let’s simulate values graph .Wow. Figure 10.13 demonstrates horrible fit! one reason don’t use high order polynomials modeling data.\nFigure 10.13: Polynomial fit\njust covered basics fitting distributions. now basic tools necessary explain y terms x variety ways. main takeaway careful fit. methods work better others aren’t careful can things might make look silly.","code":"\n#-----------------------------------------------------------------\n# Function to build a frequency table\n#-----------------------------------------------------------------\nf_build_freq_table <- function(variable){\n  \n  pr          <- as.data.frame(table(variable))\n  pr$prob     <- pr$Freq/sum(pr$Freq)\n  pr$variable <- as.numeric(as.character(pr$variable))  \n  \n  return(pr)\n\n}\norder_freq <- f_build_freq_table(wait_sample$orderTimes)\n#sum(order_freq$prob)\n#-----------------------------------------------------------------\n# Access frequency table data\n#-----------------------------------------------------------------\nfreq_table  <- FOSBAAS::freq_table_data\nknitr::kable(head(freq_table),caption = 'Frequency table order time')\nfreq_table$cumprob <- cumsum(freq_table$prob)\nfreq_table_graph <- \n  ggplot(freq_table,aes(x = variable,y=cumprob)) +\n  geom_line(size = 1.2,color = 'dodgerblue')     +\n  xlab('\\n Seconds')                             + \n  ylab('Percent of Values')                      + \n  ggtitle('Table of values')                     +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Apply fits\n#-----------------------------------------------------------------\nlibrary(mgcv)\nfreq_table$cumprob <- cumsum(freq_table$prob)\n#-----------------------------------------------------------------\n# Exponential fit\nfit_ex <- \n  nls(variable ~ a*cumprob^m, data = freq_table, \n      start = list(a = 300,m=.15)) \nfreq_table$pred_exp <- predict(fit_ex)\n#-----------------------------------------------------------------\n# Polynomial fit\nfit_py <- \n  lm(freq_table$variable~poly(freq_table$cumprob,5,raw=TRUE))\nfreq_table$pred_poly <- predict(fit_py)\n#-----------------------------------------------------------------\n# GAM fit\nfit_gm <- \n  mgcv::gam(variable ~ s(cumprob),data = freq_table)\nfreq_table$pred_gam <- predict(fit_gm)\n#-----------------------------------------------------------------\n# Apply logit fit\n#-----------------------------------------------------------------\nfit_lt <- nls(variable ~ SSlogis(cumprob, Asym, xmid, scal), \n              freq_table)\ncof    <- coef(summary(fit_lt))\n\nfit <- nls(variable ~ A/(1 + exp(((-I+cumprob)/S))), \n           data = freq_table,  \n           start = list(A = cof[1],I= cof[2],S = -cof[3]), \n           control = list(maxiter  =  10000), trace=TRUE)\n#-----------------------------------------------------------------\n# Spline fit\n#-----------------------------------------------------------------\nfit_sp <- with(freq_table, smooth.spline(cumprob, variable))\nfreq_table$pred_sp <- predict(fit_sp)$y\n#-----------------------------------------------------------------\n# Observe fit data\n#-----------------------------------------------------------------\n\ndist_fits <- \nggplot(freq_table,aes(y = cumprob,x = variable))            +\n       geom_point(alpha = .5,size = 1)                      +\n  geom_line(aes(x = pred_exp), size = 1.1 , lty = 2, \n            color = 'dodgerblue')      +\n  geom_line(aes(x = pred_poly), size = 1.1, lty = 3, \n            color = 'mediumseagreen') + \n  geom_line(aes(x = pred_gam), size = 1.1, lty = 4, \n            color = 'coral')  +\n  geom_line(aes(x = pred_sp), size = 1.1, lty =5, \n            color = 'orchid')  +\n  ylab('\\n Cumulative Probability')                          + \n  xlab('Order time in seconds')                              + \n  ggtitle('Distribution of order times')                     + \n  graphics_theme_1\n#-----------------------------------------------------------------\n# Compare fits\n#-----------------------------------------------------------------\nmodels <- list(expon  = fit_ex,\n               poly   = fit_py,\n               gam    = fit_gm)\n\nget_diagnostics <- function(mods){\n  mods <- models\n  aics   <- lapply(mods, function(x) AIC(x))\n  bics   <- lapply(mods, function(x) BIC(x))  \n  frame <- as.data.frame(matrix(nrow = length(mods), ncol = 3))\n    frame[,1] <- names(mods)\n    frame[,2] <- unlist(aics)\n    frame[,3] <- unlist(bics)\n  names(frame) <- c('model','AIC','BIC')\n  return(frame)\n \n}\n\nmodels_table <- get_diagnostics(models)\n#-----------------------------------------------------------------\n# get fit\n#-----------------------------------------------------------------\nf_get_fifth_degree_fit <- function(new_var,dist_fit){\n  var <-  coef(dist_fit)[1]              + \n         (coef(dist_fit)[2] * new_var    + \n         (coef(dist_fit)[3] * new_var^2) + \n         (coef(dist_fit)[4] * new_var^3) +\n         (coef(dist_fit)[5] * new_var^4) + \n         (coef(dist_fit)[6] * new_var^5))\n  return(var)\n}\n#-----------------------------------------------------------------\n# Equation output\n#-----------------------------------------------------------------\nf_get_fifth_degree_fit(.7785714,fit_py)\n#> (Intercept) \n#>    45.59297\n#-----------------------------------------------------------------\n# get fit\n#-----------------------------------------------------------------\npoly_fit <- \nsapply(seq(0,1,by=.005),\n       function(x) f_get_fifth_degree_fit(x,fit_py))\n\npoly_values <- tibble::tibble(y = seq(0,1,by=.005),\n                              x = poly_fit)\npoly_graph <- \nggplot(poly_values,aes(x=x,y=y))             +\n  geom_line(size = 1.5, lty = 3, \n  color = 'mediumseagreen')                  +\n  ylab('\\n Cumulative Probability')          + \n  xlab('Order time in seconds')              + \n  ggtitle('Simulated order times')           + \n  graphics_theme_1"},{"path":"chapter10.html","id":"queuing","chapter":"10 Operations and Analytics","heading":"10.5 Understanding queuing systems","text":"Queuing huge complicated field study ’s principles can applied many problems Computer Engineering queuing systems amusement parks airports. face sports. Queuing systems typically three parts described “Operations Management” Heizer Rendee: (Jay Heizer 2014)Arrivals inputs systemQueue discipline, waiting line itselfThe service facilityEach components specific characteristics. instance, arrivals consider:size arrival populationThe behavior arrivalsThe pattern arrivalsAnalyzing queuing models tends make heavy use simulation requires specific forms data collection. However, underlying assumptions correct, analyzing data becomes exercise plugging good data equations. ’ll adapt examples “Fundamentals Queuing Systems” (Thomopoulos 2012).Let’s imagine concessions concept six points sale queuing space can accommodate 50 people. queuing parlance, represents (M/M/k/N) queue represents multi-server, finite capacity system. 1/lambda represents average time arriving customers 1/mu represents average service times. assuming exponentially distributed inter-arrival times service times. reality, ’ll make observations determine distribution service time inter-arrival times. Let’s begin defining terms ’ll work problem using R.Average time arrivals:\n\\[\\begin{equation}\n\\tau_{} = {1}/{\\lambda}\n\\end{equation}\\]Average time service units:\n\\[\\begin{equation}\n\\tau_{s} = {1}/{\\mu}\n\\end{equation}\\]Average number arrivals per unit time:\n\\[\\begin{equation}\n\\lambda\n\\end{equation}\\]Average number units processed unit time continuously busy service facility:\n\\[\\begin{equation}\n\\mu\n\\end{equation}\\]Number service facilities:\n\\[\\begin{equation}\nk\n\\end{equation}\\]Utilization ratio:\n\\[\\begin{equation}\n\\rho = \\tau_{}/\\tau_{s} = \\lambda/\\mu \\text{ : } \\rho/k \\text{< 1 needed ensure system equilibrium}\n\\end{equation}\\]Number units system:\n\\[\\begin{equation}\nn \\text{ : n} \\geq \\text{0}\n\\end{equation}\\]can easily translate equations figures R. encounter Greek letters, recommendation write . sort analysis can quickly get confusing. Let’s carefully define initial inputs.Let’s imagine concession concept six points sale space 50 people line. takes 60 seconds service customer customer arrives line every 110 seconds.R several tricks can use make calculations easier. Since variables vectorized, adding sequences easy. need loop. first thing need calculate probability n (defined ) equals 0. given slightly alarming-looking equation:Probability n = 0:\n\\[\\begin{equation}\nP_{0} = 1/\\{\\sum_{n=0}^{k-1} \\rho^n/n! + \\rho^k/k![(k^{N-k+1} - \\rho^{N-k+1})/(k - \\rho)k^{N-k}]\\}\n\\end{equation}\\]Pay attention order operations. difficult part translating equation getting parenthesis correct places.running code, can see P0 = 0.4305395. complete exercise calculating probability n units system two equations different levels n. n = (0,k)\n\\[\\begin{equation}\nP_{n} = \\rho_{n}/n!P_0\n\\end{equation}\\]n = (k + 1,N)\n\\[\\begin{equation}\nP_{n} = \\rho_{n}/[k!k^{n-k}]P_{0}\n\\end{equation}\\]can write helper function help us reconcile two options.following function accepts inputs spits desired output.Executing function returns dataframe representing list values.Table 10.13: f_get_MMKN outputWe now basic understanding analyze queues. stuff can get complex multitude software packages can take care . However, simply plugging information equations, difficult part just getting good data choosing correct models accommodate . lots variation, instance inter-arrival times may normally distributed instead exponentially distributed. ’ll adapt model accommodate variation.","code":"\n#-----------------------------------------------------------------\n# Define terms for queuing equation\n#-----------------------------------------------------------------\nlambda                # Average arrivals per unit of time\nmu                    # Average number of units processed \nk      <- 6           # Number of points of sale\nN      <- 50          # Number that the queue can accommodate\ntau_a  <- 1/lambda    # Average time between arrivals: 110 seconds\ntau_s  <- 1/mu        # Average service time: 90 seconds\nrho    <- tau_a/tau_s # Utilization ratio\nn                     # Units in the system\n#-----------------------------------------------------------------\n# M/M/k/N Queue Inputs\n#-----------------------------------------------------------------\nk        = 2          # Number of points of sale at concept\nN        = 5          # Number of people the queue can accommodate\ntau_a    = 10         # time between arrivals: 40 seconds/60\ntau_s    = 8          # Service time: 90 seconds/60 = 1.5\nlambda   = 1/tau_a    # Customer Arrivals per minute\nmu       = 1/tau_s    # Serviced customers per minute\n#-----------------------------------------------------------------\n# Translate to per hour figures\n#-----------------------------------------------------------------\nlambda_h = 60/tau_a   # Per hour\nmu_h     = 60/tau_s   # Per hour\nrho      = lambda/mu  # Utilization ratio\n#-----------------------------------------------------------------\n# M/M/k/N Calculating the Probability that n = 0\n#-----------------------------------------------------------------\n# Create the sequence for the P0 equation\nn   = seq(0, N-1, by = 1 )\n# Translate the equation into R:\nP0 <- \n 1/ sum(((rho^n)/factorial(n)) + ((rho^k)/\n(factorial(k)*((k^(N-k+1)) - (rho^(N-k+1))/((k-rho)*(k^(N-k)))))))\n#-----------------------------------------------------------------\n# M/M/k/N  Probability of n units in the system\n#-----------------------------------------------------------------\n# For n = (0,k)\nnk0 = seq(0, k, by = 1 )\nPnk0 <- rho^nk0/factorial(nk0)*P0\n\nsum(Pnk0)\n\n# For n = (k + 1,N)\nnk1 = seq(k + 1, N, by = 1 )\nPnk1 <- rho^nk1/(factorial(k)*k^(nk1-k))*P0\n\nsum(Pnk1)\n\nround(sum(Pnk0,Pnk1),2)\n#-----------------------------------------------------------------\n# Calculate figures\n#-----------------------------------------------------------------\nlambda_e <- lambda*(1 - Pnk1) # Lambda Effective\nrho_e    <- lambda_e/mu       # rho Effective\n# Expected in queue\nLq = sum((n-k)*Pnk0)\nLq = sum((n-k)*Pnk1)\nLs = rho_e\n# expected units in the system\nL = Ls + Lq\n# Expected service time\nWs = Ls/lambda_e # minutes in service\nWq = Lq/lambda_e # minutes in queue\nW = L/lambda_e   # minutes in system\n\n#-----------------------------------------------------------------\n# Calculate figures\n#-----------------------------------------------------------------\nf_get_MMKN <- function(k,N,ta,ts){\n  \n  lambda = 1/ta #: per minute\n  mu     = 1/ts #: per minute\n  rho    = lambda/mu #: utilization ratio\n  \n#-----------------------------------------------------------------\n  # Probability of n units in the system\n  # for\n  n = seq(0, N-1, by = 1 )\n  P0 <- 1/ sum(((rho^n)/factorial(n)) + \n               ((rho^k)/(factorial(k)*((k^(N-k+1)) - \n                   (rho^(N-k+1))/((k-rho)*(k^(N-k)))))))\n  \n  # Probability of n units in the system\n  # for\n  n = seq(0, k, by = 1 )\n  Pn0 <- rho^n/factorial(n)*P0\n  \n  # for\n  n = seq(k + 1, N, by = 1 )\n  Pn1 <- rho^n/(factorial(k)*k^(n-k))*P0\n  \n  Pn      <- c(Pn0,Pn1)\n  \n#-------------------------------------------------------------------\n  # calculations\n  len     <- max(length(Pn))\n\n  lambda_e  <- lambda*(1 - Pn[len])\n  rho_e    <- lambda_e/mu\n  \n  # Expected in queue\n  Ls = rho_e #   Ls = 1*Pn[2] + 2*sum(Pn[-c(1,2)])\n  \n  # for\n  n = seq(k+2, N + 1, by = 1 )\n  Lq = sum((n-(k+1))*Pn[n]) # Lq = 1*Pn[4] + 2*Pn[5] + 3*Pn[6]\n\n  # expected units in the system\n  L = Ls + Lq\n  \n  # Expected service time\n  Ws = Ls/lambda_e # minutes in service\n  Wq = Lq/lambda_e # minutes in queue\n  W  = Wq + Ws   # minutes in system\n  \n#-------------------------------------------------------------------\n  # Build output\n  frame <- data.frame(matrix(nrow = 7,ncol =2))\n  names(frame) <- c('Metric','Value')\n  \n  metric <- c('Servers:','System Capacity:','Time between arrivals:',\n              'Average service time:','Minutes in service:',\n              'Minutes in queue:','Minutes in system:')\n  values <- c(k,N,ta,ts,Ws,Wq,W)\n  \n  frame[,1] <- metric\n  frame[,2] <- values\n\n  return(frame)\n  \n}\n#-----------------------------------------------------------------\n# Run our function\n#-----------------------------------------------------------------\nFOSBAAS::f_get_MMKN(2,5,10,8)"},{"path":"chapter10.html","id":"key-concepts-and-chapter-summary-9","chapter":"10 Operations and Analytics","heading":"10.6 Key concepts and chapter summary","text":"Analytic techniques can applied wide variety operations problems. Additionally, Operations broad field study covers many different arenas. chapter covered two major subjects: Simulation Queuing. examples covered increasing throughput concessions. scratched surface, introduced number topics:Simulation: Monte Carlo simulations relatively simple create incredibly useful evaluating wide variety topics.Distribution fitting: Leveraging distributions critical simulation. Learn think distributions, point estimates. Distributions everywhere.Queuing analysis: Queues exist everywhere electrical circuits concession stands. Analyzing queuing system can give insight improve .basic project management tipsWe also learned think problems. easy intellectually lazy take time truly evaluate problem. Operations critical thinking terms system many interrelated components. also widely studied field, can conceptualize problem correctly, able find way analyze improve .","code":""},{"path":"Chapter11.html","id":"Chapter11","chapter":"11 Epilogue","heading":"11 Epilogue","text":"covered variety subjects related foundational elements business strategy sports business setting. setting specific working club. focused build strategies increase ticket sales revenue, strategies benefit business operations. Additionally, shown number concepts two different parts. Chapters 1-5 covers foundational elements needed order analyze specific problems. Chapters 5-10 looks specific problems demonstrates techniques can use solve .Chapter 1 introduced us think use data. Using data fundamental business strategyChapter 2 introduced us specific data sets create using R language. knowledge programming still critical analysis.Chapter 3 demonstrated build interpret foundational graphs using R. also covered summarizing exploring data.Chapter 4 discussed frame think problems face. also got see basic project management techniques .Chapter 5 introduced us customer segmentation. look different methods discuss use data.Chapter 6 covers basic pricing forecasting principles.Chapter 7 extends segmentation lead scoring.Chapter 8 demonstrated consider promotions covers brand management conceptsChapter 9 introduces formal research principles including hypothesis testing sampling.Chapter 10 covered couple techniques related stadium operations. Understanding simulation critical solving wide variety operations problems.lot material, scratched surface. fact, struggled even cover subjects real depth. Let’s take look four goals discussed beginning book discuss whether accomplished.give analytic toolboxTo teach sports team’s business worksTo apply knowledge real problems achieve desired outcomesTo build reference manual solutions common problemsUltimately, knowledge coalesces strategic thought. aren’t born understanding business strategy. foundations strategy built experience. Analytics tightly coupled strategy. something touches every functional unit business helps produce answers questions strategist manager devises. Let’s review analytics toolbox.","code":""},{"path":"Chapter11.html","id":"your-analytics-toolbox","chapter":"11 Epilogue","heading":"11.1 Your analytics toolbox","text":"covered lot code book introduced number concepts. high level, covered:manipulate summarize data coderegressionHow implement machine learning algorithmsFormal research methodsSimulation applicationsBuilding graphicsProblem solving framing projectsWe also covered several techniques, code can use implement techniques, interpret results. However, lot didn’t cover.Neural Networks ..exotic forms regression (mixed effects models, etc.)Bayesian methodsOther ensemble learning methodsDeploying web apps reporting information disseminationThis deliberate. need cover everything. understand think problem, applying specific algorithm trivial data sets proper place. also hundreds algorithms, -less things context:Predict numerical valuePredict classDemonstrate structureWe didn’t cover tools convolutional neural networks computer vision speech recognition. don’t really need specialists going able better us. instance, might want deploy facial recognition entering suite. might also like deploy speech recognition natural language processing replace sales staff. might want spatial analysis park using something like digital twin. Ultimately, vendors work much efficient manner. just need aware tools leveraging . don’t want salesperson wowing executives “..” isn’t even right tool using. Now know means.","code":""},{"path":"Chapter11.html","id":"sports-business","chapter":"11 Epilogue","heading":"11.2 Sports business","text":"also discussed think problems understand distinctions sports industries. Sports unique outcomes often control. can’t just apply techniques analysis learned business school business constricted many ways. geographic boundaries, agency agreements, labor issues, many unique circumstances. heart, many leagues simply trade organizations. work revolves around constrained optimization exercise. ’s similar operating mutual hedge fund. Sometimes ’s , sometimes ’s . job navigate seas think problems advance abstract fashion.Individuals also attracted sports variety reasons influence thought. Clubs deal public relations issues along variety avenues. can appropriated politicians inopportune times. brand critical aren’t always control . PR scandal? strategic problem didn’t even consider covering, understood. proactively protect brand public relations corporate communication? issues impact corporate sponsorship ticket sales? measures defensive offensive nature. know making right choices? may know. just best can.","code":""},{"path":"Chapter11.html","id":"problems-faced-by-a-club","chapter":"11 Epilogue","heading":"11.3 Problems faced by a club","text":"Additionally, covered many real-world problems. first part book concerned main subjects:Understanding context analytics strategy within sports business (specifically club)Building understanding common data sets (introduce R data)Exploring data (Understanding build common graphs use)Framing Projects (Understanding think problems strategic fashion)second half book covered application analytic techniques interpretation results. covered:SegmentationPricingLead scoringPromotionsResearchOperationsWe didn’t cover many subjects related functional departments exercises make covered possible. functions live .T. live Executive level. related subjects include:Asset valuation corporate sponsorship departmentsBuilding dashboards business intelligence functionsConstructing ETLsAdditionally, didn’t cover higher-order strategic problems. problems might involve legal rights fundamental features customer asset classes. Examples strategic issues include:Media rightsdigital rightsOther issues politics legal issuesGrowing top--funnel audiences (looking Gen Z Gen Alpha)Business development growthMergers aquisitions","code":""},{"path":"Chapter11.html","id":"how-do-i-learn-more","chapter":"11 Epilogue","heading":"11.4 How do I learn more","text":"Google . Seriously. million resources learning R Python online. can take free classes access entire books subjects like programming free. can said machine learning techniques. However, comes warning. Techniques change rapidly. Techniques random forests support vector machines may destined scrap-pile neural networks become easier deploy. TensorFlow (tensorflow 2020) accelerated trend several years ago.Additionally, student take classes network. Contact someone club ask can work project school. already work industry, try expand horizon. Don’t get locked working narrow projects. Look expand influence throughout office. broader work experience, better ’ll everything .also scores books number subjects related analytics. subject broad naturally gravitate toward specific components. may prefer visualization building databases. might fall--love modeling statistics. Figure components analytics like, purchase used book , read try duplicate can find. R, Python, TensorFlow, many technologies can used free. Download get started data. Thousands data sets can even found embedded R Python. R can find simple command:can see available data sets using variation preceding function:Find interesting data sets use . Many packages also excellent documentation full-scale vignettes. Take advantage . little time focus, ’ll surprised much learn.","code":"\ndata(package='ggplot2')\ndata(package = .packages(all.available = TRUE))"},{"path":"Chapter11.html","id":"what-does-the-future-hold","chapter":"11 Epilogue","heading":"11.5 What does the future hold","text":"Many examples saw book solved one degree another. problems well understood within outside sports. instance, explored pricing rudimentary standpoint. Dynamically pricing tickets based variable demand levels happening industry (applying specific algorithms) forty years. Probably much longer. ’s commodity. good thing. nascency analytics revolution analyst may considered malevolent presence board room. now necessity influence continue grow. skill sets needed wield analytics team prerequisite future leadership. Working field best way get experience. However, might even useful use skill sets directly specializing another vertical finance. life winding path isn’t linear.time, see functions become integrated CRM systems skill sets become comoditized students continue flock analytcs oriented degree programs funded cheap government student loans. neither bad good. look analytics similar vein Computer Engineering degrees. don’t need one-million people Computer Engineering degrees. know call second-rate computer engineer? bartender. might also run call center .T. work. point need people really good building better processors. Analytics similar . don’t need armies; need people really good .Furthermore, analytics begin change workforce sports. Chat-bots increasingly take sales service personnel. Accountants similarly doomed. formulaic, aren’t going long. workforce changes aren’t limited sports. better voting universal basic income machines coming white-collar jobs next already .Media continue rapidly change. GenZ GenAlpha (born 2000) continue shape media consumption content strategy. factors impact sports consumption media even critically, -person. trick attracting attention-deficit-disordered youths? Nobody knows yet, literature says something like (Jeff Fromm 2018):“Start humanizing brand. means giving brand personality consumers can engage .”Apparently, communication rules changed. Interactions hardware seem almost silly:“Pivitols learned swipe speak. Attempting swipe unswipable–like TV screens pages magazine–assumed image front broken.”admit swiped magazine page. Gen Z interacts social media technology much intuitive way older generations. Sports adapting, continue adapt. don’t know answer , interfacing young people huge strategic problem thought investment. club level, difficult think marketing initiatives context investment. don’t might find media equivilent professional fishing, code equivalent FoxPro. Remember FoxPro?Stadium operation also continue change. New venues purposefully constructed provide convenient immersive experiences fans. ’ve concerned years terrorist attack large public sporting event revolutionize approach ticketing, ingress, egress. thing happened 2001 attacks World Trade Center permanently altered air travel 98. ’ll see massive changes . Whether changes improve experience debate.Additionally, everything incremental. Nothing new concepts book. Neural Networks around decades. Looking back old textbook college “Decision Support Systems Intelligent Systems” published 2001 (Efraim Turban 2001) reviewed chapter Neural Networks. Fundamentally, nothing changed. Incremental improvements hardware, software, mathematics simply pushed technologies along. final chapter book section entitled “future Management Support SYstems. couple predictions stood :Groupware technologies collaboratoin communication become easier use, powerful, less expensive. make electronic group support viable initiative even small organizations.Thank COVID19. Zoom, Slack, Microsoft Teams, host technologies now ubiquitous necessary. magical Metaverse might take concept even interacting digital version 3D environment. Let’s meeting top Denali. don’t know cool . Let’s look another prediction:use voice technologies natural language processing facilitate usage MSS.Bingo. Amazon Alexa, Apple’s Siri, many others dramatically expanded use NLP. Thank neural networks. technologies may even supplant keyboard based search alter companies created unpredictable ways. ’d like share one :Frontline decision support technologies mostly support CRM become integral part medium-sized large corportations.accurate quote section. Salesforce become dominant player continuing expand ’s capabilities acquisition (2019 Tableau 2020 Slack) 99. CRM platforms increasingly using NLP enable conversational intelligence. developments take long time mature. Almost 20 years quote written development still proceeding rapid pace.Uncertainty, egos, intelligence, luck breed innovation, problems faced clubs complex ever. core business isn’t efficient, prohibits working potentially much rewarding projects. Hopefully, book give good idea implement basic analytical tasks serve strategic initiatives managers throughout organization. Take learn improve .","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
