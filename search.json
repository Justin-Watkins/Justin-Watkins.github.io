[{"path":"index.html","id":"abstract-and-forward","chapter":"Abstract and forward","heading":"Abstract and forward","text":" want get straight work, skip first chapters go directly chapter 5. work content book contained back half. first chapters familiarize data, ’ll examine tools ’ll use explore , explain contextualize projects. , continue reading chapter, explain put project together contains.book primarily conceived resource young person wants work sports business analytics group, strategy function, lesser extent, .T. team. skill sets blend together complement one another. However, book helpful almost anyone wants get deeper understanding analytics functionally used club setting.asked college professor teaching analytics course many analysts need future. speaking front analytics students, response different wanted hear. going need fewer. encouraged students apply analytics skills discipline marketing, .T., finance. Technology rapidly commoditizing analytics work. skills work another field, can incredibly effective manager. perspective, anyone working sports business benefit knowledge. helps understand benefits limitations analytics puts driver’s seat implementing strategies believe effective.Additionally, book partially inspired vestigial lament youth. took math class, always forced show work, hypocritical writers textbooks didn’t feel though precept applied . invariably tricked test algebraic edge case entirely new . stuff formulaic. Just demonstrate solve problems, can similar problems. way math used taught school less concepts calculation, modicum conceptual learning needed blend better contemporary teaching techniques. want correct mistake limited way. want show naked truth. book exposure. want show everything. end, serves four primary purposes:give analytically grounded toolbox can build uponTo teach sports team’s business worksTo demonstrate apply knowledge real problems achieve desired outcomesTo build reference manual solutions common problemsThe first three items list foundations core business strategy. need experience put together effective strategies. text give head-start working analytically oriented tasks help support strategy. won’t strategy analytics expert, ’ll better understand approach many common problems. Analytics (field) excellent tool understanding various issues, limits. Keep quote mind:hammer, everything looks like nail.famous Law Instrument1. Analytics data answer every question. must assume many people seem think way since something tend hear often. Since trying show work, traverse lot granular material. Due technical nature material, better suited people want develop specialized skill sets. less well suited people want understand application theory.technology democratized analytics, analytics still black box many managers. people depend tools. Instead, often retreat . Consider quote: “Expert Power influence wielded result expertise, special skill, knowledge.” (Stephen P. Robbins 2012) Understanding tools techniques gives degree expert power. However, manager without knowledge, idiom “tail wags dog” applies. don’t understand concepts want use , can lose credibility employees look less competent manage.don’t need know script code leverage book, programming/scripting knowledge enormous benefit. want understand basic scripting fully use text. heavily exposed , need learn working knowledge . still necessary prerequisite work implementing solutions field. code ’ll find book written R (R Core Team 2022). fact, entire book written using R libraries using free software. edited using software called Grammarly2. ’ll mostly follow best practices defined “Advanced R” (Wickham 2014). Python become prevalent recently, find R suitable non-programmers.R excellent ways unusual others. idiosyncrasies make operate differently many programming languages. noticeable difference flow control, loops avoided favor suite vectorized functions. functions also expanded improved generous developers. don’t know means, learn , don’t worry. examples use common constructs aren’t penalized run-time. data-science perspective, Python uses many packages duplicating R’s advantages (Pandas Numpy). ’ll come away appreciation language whether want appreciation .Additionally, hope expose lot reading material concepts. fact, undergraduate education give broad cursory knowledge base consisting facile ideas. probably didn’t put many concepts use practice, know things like “want find area curve, can probably use Integral.” great. means can Google problems really quickly. books mention text ones stumbled across trying solve problems. mention make life little easier. Analytics large field know everything. need know find answers; others probably done . Hopefully, introduced enough concepts speak critical subjects verticals. ’ll make simple references Wikipedia pages whenever talk specific topics don’t require -depth information --scope. enough get started.Furthermore, want teach little bit main business functions professional sports team. Sports unique business. fortunes often outside control employees. strategies need reflect fact. think efforts sell tickets ineffective, ? strategy comes play. know team dismantled year, change approach?’ll also frame strategy around goals. means kinds different things. club, mean plan achieve goal $x valuation. Sports clubs act differently assets. Additionally, rapid proliferation SPACs designed invest directly teams. Valuations beginning reach levels single owner may soon dinosaur. change way clubs run. club worth whatever tech oligarch, hedge-fund managing pirate, real-state heir, company, consortium wants pay, thousands groups. necessarily worth discounted cash flow based revenue.aren’t going cover corporate strategy. aren’t going talk setting shell companies, owning different parts value chain, human resources structure, shady finance accounting, new business opportunities, extensions, agency agreements make businesses function. Instead, closely examine strategy makes wheels turn dollars exchanged club fans.perspective, can create provisional definition strategy context:Strategy discipline amplifying revenue-generating efforts, operationalizing improved procedures, consultation decisions interact build cohesive business systems, longer shorter-term planning coordinated informed decision-making.deconstructable mouthful, don’t want narrow broad. also don’t like extend platitudes. definition explains think problems specific context. skeptical anything find , good. please don’t take word. analyst, demand convinced. ’ll think actively . type work isn’t intellectually lazy. don’t understand Atkinson cycle 3 makes car work able drive . Many analytic techniques can thought way. don’t need understand underlying mechanisms. just need know appropriately wield . gas pedal, brake. analyst strategist mechanic business systems club. everything always worked, wouldn’t need .first chapter 1 cover essential analytics elements, intersection information technology. also examine distinction disciplines Business Intelligence Analytics. ’ll also discuss technologies integrating disciplines scale functions continue evolve. chapter helps frame foundation rest book.ability leverage analytic techniques predicated access quality data. second chapter 2 familiarize data use going exercise creating . formats closely resemble data found --wild. includes customer data along standard demographic data formats. ’ll also build database ticketing data primary secondary markets used pricing exercise. ’ll also create activity data demonstrate common pitfalls ’ve seen building sales marketing plans. data constructed based imaginary professional baseball team. Baseball operates differently sports high number games. However, lessons learned can applied almost sport. fact, lessons book can used business problems various industries. Unfortunately, boring chapter. book isn’t coding, demonstrate precisely everything show . way, pay debt old math teachers owe .chapter 3, cover explore data. B.. technologies Tableau make performing ad hoc analysis relatively easy, writing analysis code enormous advantages. chapter covers many typical graphs shows build understand . also demonstrate summarize consolidate data. prevalent task easy ignore . Using programming language manipulate data much better using excel can’t express well enough. R Excel military-grade performance-enhancing drugs. Excel spreadsheet programs place, begin using paradigms found R, avoid whenever convenient. Point--click tools sometimes make finding solutions problems difficult know accomplish task.chapter 4, examine frame projects. basic project management knowledge valuable, ’ll cover . also easy chapter avoid, essential include . contains good examples frame project asking right questions. tries avoid pedantic MBA speak definitely covers arena.chapter 5, demonstrate several methods building customer segmentation schemes. Consumer segmentation crucial integrated sales, marketing, research strategy. myriad ways accomplish practical helpful segmentation scheme. much art science. chapter cover fundamental concepts working example demonstrate one way segmentation might performed. One essential skills analyst toolbox understanding deal missing data. ’ll cover firsthand .chapter 6, cover pricing forecasting. Pricing complex, cover computed. Increasingly, pricing exercises becoming commodity. means may working little directly, must understand methods prices decided upon practice. chapter also goes deeper another critical tool use, regression. Regression gold standard estimating numerical data, understanding complexity essential. ’ll also cover forecasting chapter. Forecasting also art science. cover forecasts useful discuss think .Chapter 7 demonstrate couple methods Lead Scoring. Lead scoring also fundamental integrated sales marketing strategy. extension segmentation. Lead scoring can also slightly controversial. different way think going sales. chapter discuss commonly used techniques demonstrate build machine-learning model. also illustrate crucial concepts endemic sports marketing world.extension pricing, ’ll look promotions chapter 8. Promotions critical marketing component, good bad ways looking . also make many assumptions working marketing. Economics referred “dismal science.” Economics dismal, Marketing just dreary needs science thought practice. chapter, also walk essential components marketing strategy discuss problems attributing sales marketing functions.Methods conducting research covered chapter 9. Conducting research tedious, time-consuming, often thankless, fundamental business strategy. Research also enormous subject. ’ll examine valuable techniques go beyond examining facile attitudinal questions. chapter introduce critical concepts haven’t conducted formal research. chapter needs longer. simply glances enormous subjects hypothesis testing sampling. can take one thing away chapter, want know sampling critical correctly always problem.Chapter 10 cover Operations. Operations broad topic, discuss important concepts. Simulation queuing addressed context real problems faced stadium ballpark operations. Simulations bread--butter operations problems, must understand work. often best way understand system. ’ll enjoy chapter don’t operations background like mathy subjects.book can’t comprehensive. gathered preceding paragraphs, book heavily concerned sales marketing. good reason. Professional sports teams high fixed variable costs enormous operational leverage locked payroll. primary sources revenue (Ticket sales, Concessions, Retail sales, Media, Sponsorship). sources derivative fans attend games watching games media outlets. concerned fundamentals strategy, start getting approach selling marketing fans correct. Get structural parts right. Suppose executives don’t think maximizing opportunities around sales marketing. case, won’t able focus higher-order projects prove valuable long short term.addition cover, Social data, Marketing mix, Retail, Media, staffing, F&B pricing, corporate sponsorship, operational components experience, gate entry parking, impact revenue strategy. Furthermore, structural issues throughout value chain need consider. instance, league -sized influence technology stack even digital rights fans. 4 strategy must take factors consideration. CRM technology strategy also direct influence success. CRM perspective, touch tangentially. covered every one subjects even went detail ones cover, book many times longer. However, come away excellent foundation working various positions throughout sports organization.Finally, book quickly become outdated. many ways, already obsolete. field analytics progressed rapidly past decade. New technologies (upgrades hardware software) make performing specific tasks easier. Amazon Web Services Google Cloud Platform offer sophisticated tools analytics, expect much low-hanging analytics fruit sports industries harvested platforms coming years. Research uncovers new methods approaching problems, higher-order skill sets make specific tasks commodities.Additionally, consumer behavior makes performing tasks obsolete. factors entirely control. hope text give deeper understanding sports industry, ’ll quickly able eclipse find push discipline industry along.","code":""},{"path":"index.html","id":"book-liscense","chapter":"Abstract and forward","heading":"0.1 Book Liscense","text":"work licensed Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Just give credit due. Nothing new, everything built came . aren’t trying advance theory , just show things.","code":""},{"path":"index.html","id":"code-liscense","chapter":"Abstract and forward","heading":"0.2 Code Liscense","text":"\n\n\n\nextent possible law,\n\nJustin Watkins\nwaived copyright related neighboring rights \nwork.\nwork published :\n\nUnited States.\nwhatever want .","code":""},{"path":"index.html","id":"contact-information","chapter":"Abstract and forward","heading":"0.3 Contact Information","text":"Justin Watkins: watkinsjudo@gmail.com","code":""},{"path":"chapter1.html","id":"chapter1","chapter":"1 Analytics and Strategy","heading":"1 Analytics and Strategy","text":"chapter give background ideologies around analytics sports business club level. also want separate disciplines strategy analytics. Analytics serves strategy, process derives analytics. chapter outline approach analytics. often many ways approach problem, issues may worth solving many reasons. Despite inherent difficulties sports, analytic approach problem-solving always core business strategy.Analytics reconciling opportunities. Many problems old still need solved. statement especially true Operations. unlimited applications analytics within industry business. Additionally, analytics can take many forms improve decision-making many functions already mentioned:Marketing: marketing mix, brand strategy, content, CRM, brand positioning, pricingBusiness Intelligence: descriptive prescriptive reportingFinance: capital expenditure, forecasting, corporate financeSales: lead scoring, sales strategy, customer journey mappingSponsorship: asset valuation, asset creationOperations: ingress egress, staffing, concessionsTechnology: supporting systemsAnalytics can even assist tasks outside core business functions human resources. instance, can optimize staffing compensation packages. Ad hoc analytic tasks typically involve nothing simple spreadsheet organizes data. Techniques applications varied may include techniques :SegmentationSimulationStatisticsOptimizationCognitive science behavioral economicsProgrammingDesigning experiments (DOE)lists endlessly long. Versatility hallmark strong analyst strategist. find books general strategy. Strategy amalgam knowledge focused specific problems. People good understand business fundamental level understand structure problems solve intelligently. forging alloy increase profitability making business efficient resilient.","code":""},{"path":"chapter1.html","id":"understanding-our-definition-of-strategy","chapter":"1 Analytics and Strategy","heading":"1.1 Understanding our definition of strategy","text":"Strategy complex term can distilled plan. strategy goal, goal can vary. Like companies (especially public companies), driving firm valuations may goal. typically drive higher valuation increasing revenue reinforcing strategic moat. example, streaming wars, Netflix, Disney, others racing produce content. Content strategic moat technical problems solved consumer behavior shifted. play highly competitive space dogfight every competitor.People often describe strategy turning everyone’s head right direction. suppose component strategy. Strategy can offensive defensive involve public relations lobbying efforts. can include manipulating supply demand market forces structuring media rights deals. can divest underperforming assets create entirely new asset classes. can hedge risks diversifying approach partnerships. might mean evaluating capabilities. Unfortunately, ’s broad term gets overused, often confused something else, can become abstraction isn’t framed around specific goals.Strategy also communication. must stress statement. suggest taking negotiation course want learn strategy. can also read books teach talk people. “win friends influence people” (Carnegie 1981) excellent place start. classic self-improvement book written nineteen-thirties relevant today . Another great book consider “Getting Yes, Negotiating Agreement Without Giving ” (Roger Fisher 2011). book positioning arguments, critical skill set. can’t , might well understand addition works. Working understanding people, presentation skills, understanding frame arguments without condescending critical strategy. can’t stress importance soft skills enough. Stop reading strong technical skill sets, take etiquette course learn act rhetoric course learn think. liberal arts -appreciated field detriment many practitioners.academic standpoint, many strategic frameworks. Harvard’s Michael Porter described firms “collection activities performed design, produce, market, deliver, support products.” (Keller 2003) Porter’s statement precisely describes , product represents something complex simple good service. famous contribution business strategy likely “Porter’s five forces.” 5 interesting framework , surface, doesn’t appear imminently valuable sports team. However, useful considering strategic problems Point--Sale system use throughout venue?. case, model can evaluate select based market forces. consolidation? mean? powerful position terms negotiating? product commodity, vendor must compete price. Perhaps interested marketing candidates sponsorship. purchase company. classic strategy problem: “Build, Buy, Lease.” can get creative.ever messed around stock market, ’ll hear much talk fundamentals. type analysis might find trading platform illusion analysis. Applying techniques like triple exponential smoothing6 moving averages stocks looks excellent, probably doesn’t matter. make novice better trader? might make worse. analysis great understand always contextual. can misleading damaging need help understanding context. Remember data always solution every problem, think critically problem exists. mechanical part analysis isn’t critical part . commodity. Focus application communication.","code":""},{"path":"chapter1.html","id":"technologies-place-in-strategy-and-analytics","chapter":"1 Analytics and Strategy","heading":"1.2 Technologies place in strategy and analytics","text":"consider application technology analytics along five ordered dimensions. can jump around degree, step generally built stage . ascend steps, execution becomes difficult complex. Execution strategy fall apart. Read last sentence . Additionally, system flow top . Keep destination mind move step.ridiculous may sound, must begin data structure. building engine, need correct parts. Data invariably messy. much effort goes cleaning, structuring, storing data use represents bulk time spent across spectrum analytics. best began .Additionally, multiple levels maturity front. instance, ticketing data may well structured, CRM data may lacking. Therefore, view analytics within structure proposed figure 1.1.\nFigure 1.1: Data strategy heirarchy needs\nFurthermore, consider technology means end, “end unto . needs clarified. technology strategy flow well-articulated business strategy. spend capital allow fans enter park leveraging facial recognition, problem solve? scanning ticket bottleneck system slows process? fans want ? regulatory environment permit ? sounds like complex system camera database. Focus system. Technology easy part solve. innovation sometimes bred throwing something wall see sticks, haven’t found primary vehicle technology adoption. organization takes misguided technology-first approach, found results painfully unremarkable. Remember sports teams made brand focused adopting technology. contradicts just outlined valid executed correctly. rule applies directions. Determine want accomplish want achieve deploying technology solution.Perhaps want revolutionize business. problems business solved revolutionary ways. internet feels much twenty years ago. now talking web 3.0 metaverse. already metaverse. called internet. isn’t revolutionary. represents platform big tech wants use transact. may better way things worse way everything. tremendous advantages want early mover, ’ll take risk. jury next decade metaverse. may 3D television. Remember ? course don’t. Look business model underlies marketing schemes. Perhaps metaverse simply way help bolster augmented reality products. understand mechanism, can plan comes next intelligently take risks.don’t want disparaging toward Information Technology. Strong .T. skill sets beneficial. skills progress, ’ll exposed myriad technologies:work Windows Apple, ’ll still need understanding Linux7Shell scripting critical skill dev-ops task8Start using git repository code9Tools docker allow package programs web development10You don’t need server. Google, Amazon, Microsoft massive deployment platforms.bold spending time get familiar technology components. ’ll rewarded highly regarded .main difference internet today twenty years ago people mostly use phone access browse . Incremental improvements time way changes take place. Core business procedures can continuously improved. rich people didn’t get brilliant product idea. things rich people , executed correctly, lucky. Think analytics business strategy way. ’ll walk component figure 1.1 discuss detail.","code":""},{"path":"chapter1.html","id":"data","chapter":"1 Analytics and Strategy","heading":"1.2.1 Data","text":"Getting foundational data elements place critical hierarchy components. won’t discuss Master Data Management specific technologies Customer Data Platforms, talk foundations data mean use word. organizations solved portions data management, ’s never-ending problem. always newer better systems, always need incorporate new data sales marketing infrastructure. process accelerating Google, Amazon, others vie cloud. Amazon Web Services Google Cloud Platform disruptive can deliver capabilities financially unfeasible many firms trying build -house. result, -prem DBMS slowly go extinct11. also old battle. twenty-five years ago, discussion might Thin-client vs. Fat-Client computing. discussion fundamentally .Let’s go ahead make assumption sports team needs data better spot. Getting data helpful position may take several varied techniques considerations:systems important incorporate?data “Big,” meaning engineering challenge hold data, velocity necessitate unique approach?housed?manages process (internal, partner, etc.)?much cost?necessary skill sets accomplish goals?understands manages data structure?Another important consideration plan data. can used? consideration dictate approach making available ETL (Extract, Transform, Load) procedures work. instance, need data available times, need current? current mean? Latency may insignificant. Indeed, isn’t situations sports. Twenty-four hours latency good enough.hand, many data points may considered specific intervals. gives indication prioritize tasks. Let’s illustrate problem.","code":""},{"path":"chapter1.html","id":"understanding-data-structures","chapter":"1 Analytics and Strategy","heading":"1.2.1.1 Understanding data structures","text":"’ll encounter numerous data structures working sports, none critical ticketing system. Regardless vendor (TicketMaster probably common), ’ll encounter form following ERD 1.2.\nFigure 1.2: Ticketing system data structure\nlooking ? many different database systems 12 available, venerable relational database one encounter . AWS Google improved optimized models, basic concept look familiar across platforms. ’ll ignore even high-level discourse technical parts database construction, cardinality normal forms discuss stuff examples.Consider database collection excel workbooks formally linked .D. column. ’ll call workbooks tables. tables allow get information transactions, historical purchase data, customer service rep data, ticketing information, . can read “crow’s foot” “Many” cross “one.” example, one customer can many plans. relationships can much complicated, basic level, data encounter look like . database relational (likely), basic SQL statements can used retrieve information:output query might look something like :reason, fear typing SQL isn’t caps. isn’t case-sensitive, doesn’t matter. data complex, example demonstrates basic table structure encounter. can see, appears Ted multiple accounts. Duplication bane database engineer. always confounds analysis one way another.Let’s take quick aside discuss Structured Query Language, SQL. SQL lingua-Franca database world. Although many technologies use “” SQL, ’ll get mileage SQL, prerequisite want work data almost capacity. good news learning basic level relatively simple, functional fluency can achieved relatively easily. also plenty free resources available learn practice . W3 schools excellent one. 13The integration CRM ticketing systems critical. , subject give multiple things think discuss:ETL constructed? API, direct database connection, etc.parity checks considered?heavy .T. tasks. Although end product crucial component analyst, understanding stuff works important. make better. Practice SQL; don’t choice want good analysis business setting.\nFigure 1.3: CRM system data structure\ndata figure 1.3 looks slightly different. CRM systems may different ways querying data. example, Salesforce uses SOQL, looks similar SQL, forces traverse relationships little differently. let’s take look data.data may need clarification relationships complex previous example.? Mr. Williams created (created) one account, accounts merged one. simple example problem likely encounter duplication problems constantly faced. one Ted Williams, system regards two people. Business rules can mitigate issues. instance, ticketing system forbid users using email already system. using email_address primary key advantages, always downstream issues must consider. Nothing perfect. Someone may use different email. one person, may impossible tell apart. rules may make problem worse.Garbage-, garbage-. must clean data move next level hierarchy. look B.., want cover essential simple query. core B.. work. asked similar question interviews years, almost everyone fails answer correctly despite simplicity. gain proficiency SQL, run code engine. written SQL server syntax answers question:Can create list Companies Industry ordered revenue?looks simple, make sure understand happening. Everyone fails test fail think invariably try aGROUP giving confused look faces wishing access Google.’ll constantly need exercise. working large data sets leveraging SQL, method preferable leveraging techniques since optimized sorts data gymnastics.","code":"------------------------------------------------------------------\n-- SQL example\n------------------------------------------------------------------\nSELECT  A.customer_id\n       ,A.email_addr\n       ,B.plan_id\n       ,B.price\nFROM Customer A LEFT JOIN Plans B ON A.customer_id = B.customer_id\nWHERE A.email_addr = \"Ted.Williams@someserver.com\"------------------------------------------------------------------\n-- SQL example example\n------------------------------------------------------------------\nSELECT  A.customer_id\n       ,A.ticketing_system_id\n       ,B.deal_id\n       ,C.opportunity_id\nFROM Customer A LEFT JOIN deal B ON A.customer_id = B.customer_id\n                LEFT JOIN opportunity C ON B.deal_id = C.deal_id\nWHERE A.email_addr = \"Ted.Williams@someserver.com\"------------------------------------------------------------------\n-- BI SQL example \n------------------------------------------------------------------\n\nCREATE TABLE #company (\n    company VARCHAR(20),\n    industry VARCHAR(20)\n)\nCREATE TABLE #revenue (\n    company VARCHAR(20),\n    revenue NUMERIC(12,2)\n)\n\nINSERT INTO #company (company, industry) VALUES\n('Coca-Cola','Beverages'),\n('Home Depot','Retail'),\n('Lockheed Martin','Aerospace'),\n('Boeing','Aerospace'),\n('BOA','Banking'),\n('Wal-Mart','Retail'),\n('Amazon','Retail'),\n('Total Wine','Retail')\n\nINSERT INTO #revenue (company, revenue) VALUES\n('Coca-Cola','2000000'),\n('Home Depot','1500000'),\n('Lockheed Martin','3000000'),\n('Boeing','5000000'),\n('BOA','900000'),\n('Wal-Mart','8500000'),\n('Amazon','1425000'),\n('Total Wine','75000')\n\nSELECT \n  RANK() OVER(PARTITION BY A.industry ORDER BY B.revenue DESC) [rank],\n  A.industry,\n  A.company,\n  B.revenue\nFROM #company A LEFT JOIN #revenue B ON A.company = B.company\nORDER BY industry, [rank]"},{"path":"chapter1.html","id":"business-intelligence","chapter":"1 Analytics and Strategy","heading":"1.2.2 Business Intelligence","text":"Business Intelligence loaded phrase can mean many different things. However, enabling B.. capability possible ’ve established good data structure. portion chapter discuss B.. high level differences data structure enables sophisticated reporting.usually place Business Intelligence two categories:ReportingResearchA Customer Relationship Management (CRM) component also typically falls Business Intelligence umbrella practice clubs. many reasons ; biggest simply legacy. Another reason CRM system typically houses much data may used reporting. ’s natural match relatively small company, people must wear multiple hats. Reporting systems Tableau, Qlik, Looker, Business Objects, etc., depend well-structured data. cautious tools well. can easily abuse get results looking . Philosophy use essential. data good spot, can tell people . Gathering insight data can take many forms, often placed one four categories figure 1.4.\nFigure 1.4: Four categories reporting\nfirst stop reactive reporting Description Diagnosis. data structured appropriately, can produce backward-looking reports. reports typically bread butter Business Intelligence department. example, answering questions much rep sold many tickets sold specific time frame.Prediction Perscription forward looking. might integrate predictive models reporting indicate whether sales goals likely met. ’ll talk might accomplished next section. Prescriptive reports might tell problem diagnosed. context, prescriptive report might enable manager reroute marketing dollars efficient channels. instance, report identify diminishing returns marketing spend particular social channel suggest one demonstrably greater efficiency.","code":""},{"path":"chapter1.html","id":"business-intelligence-data-structure","chapter":"1 Analytics and Strategy","heading":"1.2.2.1 Business Intelligence data structure","text":"Data structure B.. system doesn’t necessarily different may find typical relational database. can plug system Tableau 14 database likely get good capability. However, data efficiently restructured facts dimensions. Data structures can also take complex forms, data cubes 15, JSON-like hierarchical data, exotic forms can handle array-like data within specific database fields. ’ll focus simple fact dimensions.can think fact something aggregated. number. Dimensions features use understand numbers. Consider diagram figure 1.5.\nFigure 1.5: Business Intelligence data structure\ntables look similar saw figure 1.3. main difference conceptual. earlier diagram, tables necessarily prioritized others. Look tables consider customer central table features radiating specific customer. customer rep, purchases tickets, may plan, etc. diagram, Ticket central. Since B.. tools heart aggregation machines, structure fundamental. want perform math feature ticket price, put fact table. allows perfromantly answer kinds questions :much customer spend tickets 2018?much spent tickets 2021?many customers rep , much spend 2019?rule, prefer structure data want within database. isn’t always feasible efficient, advantages. can efficient flattening relationships fewer tables can make software run quickly. removed need traverse relationship. also means less dependent learning capabilities B.. platform. makes things straightforward perspective. works well person B.. work also data engineer. problem needs scaleable can get confusing. lead build custom systems aren’t constrained ways.also pitfalls aware . can work around issues. However, can also encounter problems Cartesian joins 16. means can double-count value aren’t careful. common constructed snowflake schema facts dimensions tables.","code":""},{"path":"chapter1.html","id":"analytics","chapter":"1 Analytics and Strategy","heading":"1.2.3 Analytics","text":"term analytics least broad business intelligence. context, distinguish business intelligence less concerned displaying information concerned interpretation. Additionally, getting base B.. functionality running straightforward applying analytic techniques data. Ultimately, business intelligence analytics work together form backbone (antiquated valid term) decision support systems.Analytics refers applying operation data gaining additional insight modification. Getting data structured appropriately critical. typically put analytics tools one two categories despite many , including simple spreadsheets:RegressionMachine learningLet’s take minute explain terms overlap. may may need become familiar regression. Regression can get technical, regression analysis dogmatic rigorous. ’ll use heavily, need understand works. recommend getting reference book subject. many. favorite “R Companion Applied Regression.” (John Fox 2019) Let’s try quick explanation ordinary least squares regression demonstrate power. build explanation R wait show much code next chapter.going explain regression simplest way possible. mathematician, don’t one leverage regression. Also, aren’t working clinical trials drug. working fuzzy business problems. exacting rigor isn’t necessary. following section take simple explanation basic form regression explain think .","code":""},{"path":"chapter1.html","id":"regression","chapter":"1 Analytics and Strategy","heading":"1.2.3.1 Regression","text":"Examine following meme created online meme generator:\nFigure 1.6: future thoughts regression\nincredibly reductive meme. can’t use linear regression generative art computer vision. However, use lot, many advantages. make sure understand . Eventually, agree meme working problems face. following section explains like think .familiar linear equation takes form:\\[\\begin{equation}\n\\ {y} = {m}{x} + {b}\n\\end{equation}\\]equation, Y explained x, m equal slope line b equal y-intercept. apply list x values (-5,-4,-3,-2,-1,0,1,2,3,4,5) linear equation slope two y-intercept five, get graph figure 1.7.\\[\\begin{equation}\n\\ {y} = {2}{x} + {5}\n\\end{equation}\\]\nFigure 1.7: Output linear equation\ndata points may fit perfect linear equation. Regression looking line points minimizes sum squared errors (see figure 1.8. sum squared errors represents ( SSE ) distance point line. Look word orthogonal see can vary. square errors negative numbers don’t impact results.\nFigure 1.8: Output linear equation\nmultiple linear regression, simply switching linear equation around adding terms:\\[\\begin{equation}\n\\ {y} = {b} + {m_1}{x_1} + {m_2}{x_2}\n\\end{equation}\\]basic form denoted similarly following equation:\\[\\begin{equation}\n\\ \\hat{y} = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\end{equation}\\]definition might read, “idea express class linear combination attributes predetermined weights.” (Ian H. Witten 2011) just think finding best average line data x explains y. standard error represents normal distribution split line. , don’t want push statistics reading much . Still, recommend picking book statistics Googling multiple-linear regression aren’t reasonably familiar . ’ll use lot analyst.Additionally, familiarize different forms regression; Orthogonal, Poissan, Ridge, etc. lots problems can solved apply correct technique.deep want go . use tool often subsequent chapters go detail. want take simple approach explain Machine Learning.","code":""},{"path":"chapter1.html","id":"machine-learning","chapter":"1 Analytics and Strategy","heading":"1.2.3.2 Machine Learning","text":"Machine learning different concept, hood, just statistics. explained many ways. heart, looking patterns data make predictions. Also, don’t worry .. taking job. Worry person knows use taking job.three main types machine learning:Unsupervised LearningSupervised LearningReinforcement LearningEach variations useful solving different types problems. ’ll also cover detail subsequent chapters. Additionally, several techniques fall machine learning umbrella, including:Decision TreesRandom forestsGradient boostingSupport vector machinesNeural networksYou can also use ensemble techniques. typically explain machine learning basic explanation completed decision tree. -depth explanation decision trees gets mathy. basic explanation comes book “Data Mining” (Ian H. Witten 2011). book utilizes program called WEKA, concepts implementations R Python.going begin looking data. digging code, now time install R R studio. talk next chapter. can install FOSBASS library accompanies book using following command:Type package’s name editor, multiple functions data sets appear type two semicolons. Type question mark data sets functions see documentation.decision tree simply organized set cascading questions answers simple understand high level. Let’s consider simple data set:Table 1.1: Renewal data setWe’ll see data several times throughout book. contains several variables column states whether fan renewed season tickets. First, let’s apply decision tree data. ’ll look one factor, distance. example, used two libraries: rpart (Therneau Atkinson 2022) rpart.plot (Milborrow 2022).\nFigure 1.9: Decision tree example\ntree (figure 1.9) simple read. top node, 2,564 people renew (nr), 11,300 . first split separates people live 49 miles venue. people, 906 renew, 663 renew. third split separates people live 67 miles park. , live 67 miles park, 515 renew.decision tree performs splits one multiple ways, many resources can illustrate methods. stage, main thing understand nodes similar nodes. node homogeneous sister nodes.machine learning simplest. set cascading questions split formulaic ways classifies response variable. methods differ, heart, many machine learning processes functionally similar despite using wildly diverging methods.","code":"\n#-----------------------------------------------------------------\n# Install FOSBAAS Library\n#-----------------------------------------------------------------\nlibrary(devtools)\ndevtools::install_github(\"Justin-Watkins/FOSBAAS\"\n                         ,ref=\"master\"\n                         ,auth_token = NULL\n)\n#-----------------------------------------------------------------\n# View data set documentation\n#-----------------------------------------------------------------\n?FOSBAAS::customer_renewals\n#-----------------------------------------------------------------\n# Customer renewal data\n#-----------------------------------------------------------------\nrenewal_data <- FOSBAAS::customer_renewals\n#-----------------------------------------------------------------\n# Customer renewal data\n#-----------------------------------------------------------------\nd_tree <- \nrpart::rpart(formula = renewed ~ distance, \n             method  = \"class\",\n             data    = renewal_data)\nrpart.plot::rpart.plot(d_tree,\n                       type  = 4,\n                       extra = 101)"},{"path":"chapter1.html","id":"automation-and-integration","chapter":"1 Analytics and Strategy","heading":"1.2.4 Automation and Integration","text":"developed basic B.. analytics capabilities, ’ll quickly want find ways put work. Ad hoc analytics place, truly reap rewards work, need build engine allows automate storytelling. analytics intersects .T. work. Operationalizing analytics procedures requires little different knowledge set. also approaches.AWS Google taken considerable strides building frameworks natively integrate analytics DBMS. Gone days writing SQL wrappers R python script sitting server basement. Let’s take simple approach explain mean .two functions (Automation Integration) self-explanatory. Automation refers removing human interaction. Integration refers operationalizing outputs extending data commerce engines.Automating procedures provides several benefits:labor multiplierIt enables strategic thought go staffing decisionsIt keeps reports --dateAutomation relies several interlocked technologies, related data engineering, primarily belongs information technology group.Integration refers two elements:Integrating solutions across organizationIntegrating third parties extend capabilitiesInterestingly, integrating third parties easy part relative integrating solutions internally. Integrating solutions internally much difficult. typically requires change management sponsor upper levels management. example, perhaps organization introduces productivity suite Slack17 Teams18. simply introducing deploying technology cure addiction email? answer “.” might increase adoption?Figure 1.10 demonstrates simplified version entire process data sources feedback loop creating marketing channel partners. course, refer marketing channel partners, referring Google’s add network, Facebook, many others.\nFigure 1.10: Operationalizing analytical procedures\nfeatures used reasonably distinct, new technologies making much easier link activities one system. someone leading efforts, job think apply measures instead accomplish . Additionally, focused tech . Several operational considerations impact automation integration, content creation, collateral, timing, verbiage, budgets. instance, distribute channel, ’ll need create artwork, clear message call action, website may need updated, communication may considered. Nothing ever easy.","code":""},{"path":"chapter1.html","id":"key-performance-indicators","chapter":"1 Analytics and Strategy","heading":"1.3 Key Performance Indicators","text":"section explain application KPI. piece feedback critical understanding well performing (typically arbitrary historic benchmark). ’ll also refer back earlier paragraph stock market. Analytics contextual. KPI? KPI figure links business performance desired outcome. example, Baseball, number walks player takes KPI likely predict -base-percentage. Analytics groups can establish causal link -base-percentage wins19. market player, might weigh walks heavily metrics. -base-percentage might KPI, walks might key increasing -base-percentage leading wins. following paragraphs discuss criticize commonly used KPI clubs: Percap, average price paid per ticket.easy make incorrect judgments armed blunt instrument data. Per-Cap (average ticket price) likely worst. Per-Cap commonly used KPI used comparing effectiveness sales pricing strategy:\\[\\begin{equation} TotalTicketRevenue / NumberOfSeatsSold \\end{equation}\\]surface, metric seems interesting. However, several issues :denominator changes every game sold . case, comparing fractions different denominators. mix tickets vary wildly game game. alters interpretation leads issue number two.number becomes diluted decreases tickets sold. tickets sold, higher proportion less-expensive tickets sold, tends drive per-cap. high per-cap good? answer depends. Let’s illustrate mean:\\[\\begin{equation} \\$1,400,000 Ticket Revenue / 34,000 Tickets sold = \\$41.80 \\end{equation}\\]\n\\[\\begin{equation} \\$1,600,000 Ticket Revenue / 40,000 Tickets sold = \\$40.00 \\end{equation}\\]reconcile $1.80 difference per-cap? higher number indicate priced efficiently lower revenue scenario? answer depends many factors. Considered isolation, number little meaning. better metric Yield. Yield just simple:\\[\\begin{equation} TotalTicketRevenue / AvailableTickets \\end{equation}\\]Yield intuitive increases every sale since isn’t penalized unsold inventory. previous example look like yield perspective?\\[\\begin{equation} \\$1,400,000 Ticket Revenue / 40,000 Available Tickets = \\$35.00 \\end{equation}\\]\n\\[\\begin{equation} \\$1,600,000 Ticket Revenue / 40,000 Available Tickets = \\$40.00 \\end{equation}\\]Yield higher revenue higher now hold denominator constant. Hopefully, indicates efficient selling seats demand higher prices buoyed revenue. number much easier interpret leverage hypothesis testing. easiest way consider metric visualize . simple scatter plot (figure 1.11) trick. actual scenario, average ticket price isn’t tightly correlated overall revenue games approach sellout percap approximates Yield.\nFigure 1.11: Scatterplot revenue percap\nAnother metric used better Sales date. metric asks many sales particular date previous year. metric may biggest liar commonly used KPI. can fraught distortion Baseball schedule. likely validity sport fewer games higher FSE base. FSE stands Full Season Equivalent represents number tickets sold individuals season (modified season) basis.Sales date problematic different reasons:compare different team, doesn’t consider admixture tickets. team 20,000 FSEs look dramatically different team 6,000 FSEs.schedule begun, admixture games significantly influence outcomes. mean?’ll see chapter 6 elements influence ticket sales. Sales--date doesn’t consider game dates, opponents, -field success, seasonality, game times, etc. Additionally, people purchase tickets different times various reasons. differences, sales specific ticket class may look great first twenty games better first forty games. Look line graph chapter 3 3.6. built cumulative line line, results look different. Refrain benchmarking sales--date. Instead, leverage forecasts consider underlying elements schedule. Consider simple example (figure: 1.12). illustration created publicly available data, ’ll demonstrate make little later.\nFigure 1.12: Relationship avg salary ticket sales\ncertainly appears relationship average salary player total number tickets sold. relationship might even substantial teams larger markets. case? top-approach forecasting demonstrates amount players paid (players paid lot money) ticket sales tend higher. don’t see tight clusters based market size. Higher pay may demonstrate better performance. Better performance might translate wins. wins attract earned media fans, ticket sales increase.obtuse example, demonstrates point. underlying mechanisms likely reasonable job explaining ticket sales. didn’t even cover bottom-forecasting elements marketing efforts!point section now clear. Many factors dictate performance. within control, . Running team similar running hedge fund. Sometimes ’s , sometimes ’s . However, need understand ’s can make smarter decisions. Bad KPIs don’t help make smarter decisions. Leveraging appropriate KPIs many salutary impacts, good analytics always looks underlying mechanism work.","code":"\n#-----------------------------------------------------------------\n# percap scatter plot\n#-----------------------------------------------------------------\nset.seed(714)\npercap_data <- tibble::tibble(\n  percap = rnorm(81,40,10),\n  revenue = rnorm(81,1000000,200000)\n)\nx_label  <- ('\\n Percap')\ny_label  <- ('Revenue \\n')\ntitle    <- ('Revenue vs. Percap')\nscatter_percap <- \n  ggplot2::ggplot(data  = percap_data, \n                  aes(x = percap, \n                      y = revenue))                   +\n  geom_point(alpha = .9, color = 'dodgerblue')        +\n  geom_rug(color = 'coral')                           +\n  scale_y_continuous(label = scales::dollar)          +\n  scale_x_continuous(label = scales::dollar)          +\n  xlab(x_label)                                       + \n  ylab(y_label)                                       + \n  ggtitle(title)                                      +\n  graphics_theme_1"},{"path":"chapter1.html","id":"why-do-people-buy-tickets-to-sporting-events","chapter":"1 Analytics and Strategy","heading":"1.4 Why do people buy tickets to sporting events?","text":"Sports fandom irrational. irrationality can make sports challenging infuriating work analytics standpoint. can determine marketing efforts making difference? simply slaves whims fancy mob? answer complex, explore little chapter forecasting pricing, chapter 6. Additionally, answer may (degree) exist genes.“ability (special conditions) transcend self-interest lose (temporarily ecstatically) something larger .”— Johnathan Haidt, “Righteous Mind.”interesting quote “Righteous Mind” (Haidt 2012). Humans programmed participate groups. people buy tickets lonely bored. mechanisms, “need belong” drive sports fandom, may manipulated. Maybe manipulated. Ultimately, many reasons someone might buy tickets sporting event:buy businessThey want something doThey brand loyalty driven upbringingThey motivated associate winning groupThey genuinely like appreciate sportThey feel good come gameThey want something familyThere also brand components associated history logos. one interesting. Recently, teams abandoned logos nicknames response societal corporate pressure. 20 mean analytics context? much brand worth? First, brand equity must pondered. least partially drives corporate sponsorship. Borrowed equity associating brand another brand positive image fundamental significant revenue driver clubs. Brand value might calculated using techniques Royalty Relief Method. 21 main idea brands tremendous value.many answers someone might purchase tickets baseball, football, basketball, hockey, soccer game. People United States worldwide complex emotional relationships sports brands. emotions better explain behavior model predicting likelihood purchase.side sports marketing often overlooked -served. Behavioral economics vast field. least one technique use later book roots behavioral economics, much literature subjects. “Power Moments” (Chip Heath 2017) outstanding book imminently useful one consider specific mechanisms engender loyalty. Central themes pride connection almost instantly accessible marketers club level. can analyst help drive desired outcomes? answer may depend solutions outside wheelhouse data scientist business analyst.","code":""},{"path":"chapter1.html","id":"key-concepts-and-chapter-summary","chapter":"1 Analytics and Strategy","heading":"1.5 Key concepts and chapter summary","text":"chapter explains rationale approaching analytics strategy organizational level. Strategy broad term include many elements, business development hedging strategies. focused analytics strategy covered main points related analytics hierarchy:Technology’s integral relationship analyticsBasic SQL data structures importance data integrityBusiness IntelligenceAnalyticsAutomation IntegrationKey performance indicatorsBehavioral economicsUltimately, chapter serves give high-level overview potential musings analytics business strategy:Technology isn’t analytics. tool serves business functions. don’t consider technology vacuum.now understand basics data structure databases. Without good data, project going nowhere. ’ll want acquire knowledge SQL, limited capability.Business Intelligence tends focus reporting research. Therefore, ’ll likely begin descriptive reports , organization matures, become forward-looking incorporating analytics functions.Analytics mainly focuses trying guess future outcomes. variety regression machine-learning tools disposal. tools commoditized past couple decades. world much easier navigate ten years ago.Automating tasks force multiplier. currently arms race going big-tech companies continue bolster analytics database systems. Google Cloud Platform Amazon Web Services two key examples. hundreds tools consuming, manipulating, deploying data.challenging part determining KPIs agreeing metric. KPIs easy. instance, NOI revenue-per-square-foot might great KPIs dealing real estate. KPIs contextual.People things different reasons. Modeling consumer behavior complex plays significant role sports. touched Behavioral Economics, rounding analytics toolbox reading field highly recommended.","code":""},{"path":"chapter2.html","id":"chapter2","chapter":"2 Code and Data","heading":"2 Code and Data","text":"Feel free skip chapter already know R familiarity sports data. chapter mostly included completeness considered including . important necessarily informative, gives us raw material divine insights.Data fuel analytics work, business strategy derived objective justifications. Every piece code book available, run machine, can hacked adapted similar problems. using R R Studio 22 primary analysis tool. R great choice analyzing sports data. find R bit idiosyncratic, don’t dream C++ use . like script little bit, don’t want full-developer, R ticket. Additionally, R Studio great IDE analysis. ton useful features make life much easier.don’t much coding experience, chapter gives opportunity familiarize code data encounter. ’ll cover basics exploring analyzing data set next chapter.chapter ’ll cover several subjects:main building blocks scripting analysisUnderstanding common data sets context professional sportsConstructing sample data sets mimic data setsBuilding R package reference data sets throughout rest bookThe particulars ticketing datasets vary small number companies dominate sports industry. TicketMaster common pro sports. Paciolin another player arena, common college sports venues. Ultimately, system won’t matter. ’ll transform data format useful analytics process.purposes, ’ll invent professional baseball team, Nashville Game Hens. Game Hens expansion team 2017 play typical baseball schedule 162 games.","code":""},{"path":"chapter2.html","id":"some-basic-notes-on-the-r-language","chapter":"2 Code and Data","heading":"2.1 Some basic notes on the R language","text":"chapter introduce R code. Ultimately, doesn’t matter tool use. Python, Julia, R, Stata, SAS, Matlab provide similar functionality. However, confronted code can little overwhelming first. leveraging code (particularly R) reasons.analysis done code step readily reproducible documentedR free, easy use, large user base, great IDER massively extensible smaller-scaled tasksIt great tool protyping something might put production perfomant environmentI found best get good one tool stick . However, say note caution. Let’s consider quote famous poem.“Things fall apart; center hold”Willaim Butler Yeats, “Second Coming”Technologies doomed obsolescence. many legacy technologies endured decades (Fortran, Cobal, R, Python, C, C++), evolved. Technologies doomed eclipsed better tools. instance, R isn’t best tool build large-scale web applications. Don’t rigid devote personal brand particular technology. might find scrap-pile … “Things fall apart.”R relatively simplistic outside relative many languages. limited number data structures, doesn’t use scalars, single-threaded, tends avoid standard flow-control. little peculiar. programming experience another language might find little difficulty . new programming, going look weird. R extended thousands developers simply writing scripts piggyback programmer’s’ work. Many done fabulous job creating free tool currently rivals others analytics space 23.Additionally, recommend using code instead point--click tool. Code demonstrates exactly done easy communicate anyone knowledge language. make life easier even makes difficult beginning. best thing R huge user base years many resources can leverage learn can . Ultimately, decision tool use needs driven want get . prototyping need operationalize code? dealing huge datasets? want something built speed comfort? going cover much language book. just many resources available better job . basic recommendations like use R:Get IDE like. RStudio currently best choice R wide margin.Download R (R Core Team 2022).24 works major platforms find free classes use . hundreds -tos available online. can also purchase many books teach language works. favorite “Advanced R” (Wickham 2014) highly recommend must-reference book.Practice ! doesn’t take long get --hump terms getting functional fluency. consider similar playing instrument. “Flight bumblebee” 25 doesn’t need first song learn piano. Start simple build knowledge. eventually develop intuitive understanding tool can .also important consider R drawbacks.language won’t tell . provides methods execute functions. still need understand approach solve problem. R won’t help unlike specialized statistical tools Minitab26.R can relatively slow don’t use correctly. constructed 27. However, ways speed significantly. Rcpp (Eddelbuettel et al. 2022) almost seamless api C++ allow build leverage C++ functions R environment. highly leveraged important tool R world.R may best choice putting something operation. Many people prototype R leverage another tool put work use. However, great tools smaller scale web deployments built Shiny framework.’s user base may decline favor tools. Python become popular recent years. Languages tend benefit network effects users , features built . User-base size important.following sections demonstrate datasets book created. think important couple different reasons.may familiar R. code sections get familiar looks works.don’t work data sports, data going foreign . opportunity explain data sets.One comment want make coding -general. easier write code read somebody else’s code. end, like comments. many cases code can act documentation, like add explicit descriptors. don’t make detailed. just give enough know code block . ’ll follow practice throughout book.data sets code used create available R package FOSBAAS publicly available, reason type following sections, suit . can download file code book : https://github.com/Justin-Watkins/FOSBAAS/blob/master/FOSBAAS_code.R.Additionally, book make use many libraries. can run following code make sure installed:Keep mind certain functions deprecate time. Additionally, can end problems certain functions names functions packages. R dependent packages ’ll want look way keep environments stable. renv (Ushey 2022) package handle work . begin writing complex code ’ll want use one dockering programs. older package called Packrat thing. also included reprex (Bryan et al. 2022) package list. reprex produces reproducible examples ’ll want use asking help piece code. really useful, check .","code":"\n#-----------------------------------------------------------------\n# Install libraries\n#-----------------------------------------------------------------\nlibraries <- \nc('AlgDesign','anacor','car','caret','data.table','devtools','dplyr','forcats',\n  'ggplot2','GPArotation','Hmisc','knitr','lubridate','maps','mgcv','mice',\n  'mlr','mlr3','nFactors','nnet','plyr','poLCA','pricesensitivitymeter','pROC',\n  'pscl','psych','purrr','pwr','ranger','RColorBrewer','renv','reprex','Rcpp',\n  'reshape2','reticulate','roxygen2','rpart','rpart.plot','rsample','scales',\n  'shiny','stargazer','stats','tibble','tidymodels','tidyr','tidyverse',\n  'viridis','xtable')\n\ninstall.packages(setdiff(libraries, \n                         rownames(installed.packages()))) "},{"path":"chapter2.html","id":"renewaldata","chapter":"2 Code and Data","heading":"2.2 Simulating customer renewal data","text":"code book typically three things :Write functionsApply functions dataGraph outputThe first data set create related building model estimate likelihood season ticket holder renew season tickets. data difficult create need build certain patterns data build . goal section build function produce data set command. Due fact builds , difficult generalize function. end doesn’t demonstrate good programming practice, many elements need progress .build function, denote prefix f-underscore word separated underscore. Columns data sets follow camel case (camelCase) first letter word capitalized spaces words. don’t like , found works . Whatever , choose one way . ’ll glad .begin creating function FOSBAAS::f_create_lead_scoring_data. best practice name function way tells exactly . aren’t familiar building functions, easy understand. already looked example. simple function outputs y terms x linear function m = slope b = y intercept:\\[\\begin{equation}\n\\ {y} = {m}{x} + {b}\n\\end{equation}\\]can think functions write build data sets way. input variable (multiple variables) function process variables output value. R function output y value x based linear equation look like .input values x, slope, y-intercept, function return value y. simplistic example, demonstrates functions well. Let’s try .Now simple function can use get y value x = 2, slope line 10, y intercept 7. Input, process, output. now understanding way function works, need way repetitively apply function data. can many different ways. programming languages ’ll use loop. wrapped following snippet system.time() function demonstrate differences speed operation.loop thing, open-ended generally used much less frequently. python, loops typically discouraged. However, find easier read methods found . choose use consider fast might run readability. Just work find comfortable aren’t working large data sets.third approach, one preferred use apply function. See ?apply. functions might confusing first, useful typically work much quickly.apply functions even improved upon specific ways. following snippet uses imap function purrr (Henry Wickham 2020) package.basic tools use everything going going forward. said, tradeoffs need contemplate. Speed readability important considerations. Ultimately, can use whatever feel comfortable .following sections demonstrate functions used build data sets find book. won’t go full detail every data set. Full documentation can found help section FOSBAAS package. Additionally, something little strange first function, feed helper functions. One helper functions requires use function well. won’t data sets, important feature R understand. Everything R can based function, can use similar way methods languages.already built functions. first one simply creates data lead scoring. Lead scoring means going use data predict groups people likely purchase ticket.function also accepts several arguments:seed variable enables us create reproducible data sets.number rows want.Five functions create tenure, spend, ticket usageTwo functions help assign renewalA renewal argument produce column indicating account renewed.want see code function uses, can use following command (Assuming installed package):Calling function produce data set looks something like . use parameters, get exactly data.Table 2.1: customer renewal dataThis data set includes several columns:account id representing specific individualIs account used company individualThe season yearThe plan type (Partial season Full season)Ticket usage percentageThe number years account usThe amount spent tickets 2021Distance ballparkDid account renew end seasonThis process little involved (see figure 2.1). Let’s walk step step.\nFigure 2.1: Data creation process\n","code":"\n#-----------------------------------------------------------------\n# Linear equation function\n#-----------------------------------------------------------------\nf_linear_equation <- function(x,slope,yIntercept){\n  y <- slope*x + yIntercept\n  return(y)\n}\n#-----------------------------------------------------------------\n# Linear equation function inputs\n#-----------------------------------------------------------------\nf_linear_equation( x          = 2,\n                   slope      = 10,\n                   yIntercept = 7) \n#> [1] 27\n#-----------------------------------------------------------------\n# fake data\n#-----------------------------------------------------------------\nx        <- seq(from=1,to=1000000,by=1)    # x values\nm        <- 10                             # Slope\nb        <- 7                              # Y Intercept\n\n#-----------------------------------------------------------------\n# 1. For Loop\n#-----------------------------------------------------------------\nline_value <- list()\n\nsystem.time(\nfor(i in x){\n line_value[i] <- x[i]*m + b\n}\n)\n#>    user  system elapsed \n#>    1.08    0.03    1.11\n\nline_value[1:3]\n#> [[1]]\n#> [1] 17\n#> \n#> [[2]]\n#> [1] 27\n#> \n#> [[3]]\n#> [1] 37\n#-----------------------------------------------------------------\n# 2. While Loop\n#-----------------------------------------------------------------\ni <- 1                               # Iterator\nline_value <- list()\n\nsystem.time(\nwhile(i <= length(x)){\n line_value[i] <- x[i]*m + b\n i <- i + 1\n}\n)\n#>    user  system elapsed \n#>    0.94    0.03    0.97\n\nline_value[1:3]\n#> [[1]]\n#> [1] 17\n#> \n#> [[2]]\n#> [1] 27\n#> \n#> [[3]]\n#> [1] 37\n\n#-----------------------------------------------------------------\n# 3. lapply\n#-----------------------------------------------------------------\nsystem.time(\nline_value <- lapply(1:length(x), function(i) x[i]*m + b)\n)\n#>    user  system elapsed \n#>    0.86    0.03    0.89\n\nline_value[1:3]\n#> [[1]]\n#> [1] 17\n#> \n#> [[2]]\n#> [1] 27\n#> \n#> [[3]]\n#> [1] 37\n\n#-----------------------------------------------------------------\n# 4. purrr:imap\n#-----------------------------------------------------------------\nsystem.time(\nline_value <- purrr::imap(x,~ .x*m + b)\n)\n#>    user  system elapsed \n#>       1       0       1\n\nline_value[1:3]\n#> [[1]]\n#> [1] 17\n#> \n#> [[2]]\n#> [1] 27\n#> \n#> [[3]]\n#> [1] 37\n#-----------------------------------------------------------------\n# Create lead scoring data\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\nf_create_lead_scoring_data(714, \n                           5000,\n                           \"2021\",\n                           f_calculate_tenure,\n                           f_calculate_spend,\n                           f_calculate_ticket_use,\n                           f_renewal_assignment,\n                           f_assign_renewal,\n                           renew = T)\n#-----------------------------------------------------------------\n# View your functions\n#-----------------------------------------------------------------\nedit(getAnywhere('f_create_lead_scoring_data'), \n     file = 'f_create_lead_scoring_data.r')"},{"path":"chapter2.html","id":"building-our-function","chapter":"2 Code and Data","heading":"2.2.1 Building our function","text":"following code creates dataframe nine columns assigns list names column. Think dataframe excel workbook. R uses <- assignment, however can use = sign prefer. use sapply function create sequence random letters numbers represent account ids. apply functions incredibly important. can type ?sapply console R studio want learn .looks weird. sth_data[,1] references first column data frame dataframe[row,column]. first argument sapply giving function list rows traverse. second argument uses something called anonymous function, confusing. Look want deeper understanding . begin make sense play . paste(sample(c(0:9, LETTERS), 12, replace=TRUE),collapse = \"\")) simply creates random twelve digit alphanumeric string. follow assigning season season column.Many season ticket accounts owned corporations. ’ll build list called corporate sample order assign “c” corporate “” individual account. use set.seed() function reproducibility. ’ll use sample function sample c list rate 20% corporations 80% individuals using prob argument.Season ticket holders can purchase full partial plans. proportions change based corporate individual account. statements generalized turned function. However, copy--paste , left alone. Always look opportunities generalize functions. function operates exactly way previous one.simulate distance stadium ’ll leverage rexp() function give us exponentially distributed list numbers can modify sample account. density pattern common many urban areas population density much higher certain centralized areas. ’ll show visualize pattern subsequent chapter. outcome individuals tend live away corporations.Next, ’ll build list numbers refer number season tickets purchased account. ’ll assign number tickets based distributions denoted prob argument sample() function. Basically, want corporations purchase tickets.tenure, set renew argument = TRUE, leverage f_tenure function assign number years based list arguments created within main function. Flow control conditions really important understand. simple way explain like : “(condition == TRUE), something. (condition == FALSE), something else”. R, == used compare things. = used assignment.function use mapply. works like sapply, accepts multiple arguments. also used statement. just means type less. tells R everything within belongs one frame data.f_calculate_tenure() function accepts four arguments. arguments constructed code chunks running. function simply long -else statement. like specific patterns like construct within tenure column.based season ticket holder spend tenure, plan-type, account type.function f_calculate_spend uses rnorm() function. function accepts mean standard deviation argument allows us sample number specific normal distribution. , generalized function little , since ’s just helper function one purpose hard-coded numbers .Ticket usage represents percentage tickets used:\\[\\begin{equation}\n\\ {ticketUsage} =  {totalTicketsUsed} / {totalTickets}\n\\end{equation}\\]Similarly previous examples, building ticket usage particular way.function f_calculate_ticket_use uses runif() function produces random number uniform distribution based minimum maximum value. numbers also hard-coded can produce specific patterns.Finally, check renew argument (using statement) true, determine account renewed tickets based values dataframe sth_data. renew_ = _F, return dataframe without renewed field.following statement uses two functions: f_assign_renewal() f_renewal_assignment(), little confusing. f_assign_renewal() helper function assigns renewal value based cluster assignment designated f_renewal_assignment().f_renewal_assignment() accepts f_assign_renewal() argument. couple things. First, clusters ticket_usage distance using kmeans() function. ’ll cover clustering exercises chapter 4. cluster assignments added together biased renewals assigned based high low number . Higher numbers likely renew. use loop instead mapply() function example. Vectorizing loop apply() function typically better method using R.function complex others dependency dplyr. also call list() used data structure R. list simply indexed array. Dataframes collections lists.just walked everything figure 2.1. process much succinct patterns constructing weren’t based specific features creating. However, able take look several programming features features endemic R.Building functionsThe apply family functionsIf statementsWhile/loopskmeans clusteringrnorm rexp functions distributionsrunif creating random numberssubsetting dplyrWe can now call function different input variables produce different result:Calling function produce data set looks something like :Table 2.2: customer renewal dataThis typical data set find wild. data also beautiful. beautiful mean isn’t missing features easy prep analysis. something typically find. ’ll discuss incomplete data problems book.can now use function produce many different data sets like. ’ll follow procedure building helper functions build data sets.","code":"#-----------------------------------------------------------------\n# 1. Create a data frame to hold our data\n#-----------------------------------------------------------------\n  sth_data <- data.frame(matrix(nrow=num_purchasers,ncol=9))\n  names(sth_data) <- c(\"accountID\",\"corporate\",\"season\", \n                       \"planType\",\"ticketUsage\",\"tenure\",\n# 2. Build ids and append to customer data frame\n  set.seed(seed)\n  sth_data[,1] <- sapply(seq(nrow(sth_data)), function(x)\n    paste(sample(c(0:9, LETTERS), 12, replace=TRUE),\n          collapse = \"\"))\n# 3. Assign a season year to the data \n  sth_data$season <- season\n#-----------------------------------------------------------------\n# 4. Assign corporate or individual to each account\n#-----------------------------------------------------------------\n  set.seed(seed)\n  corporate <- c(\"c\", \"i\")\n  sth_data$corporate <-  \n  sapply(seq(nrow(sth_data)), \n         function(x) sample(corporate, \n                            1, \n                            replace = TRUE, \n                            prob = c(.20, .80)))\n#-----------------------------------------------------------------\n# 5. Assign a plan type to each account\n#-----------------------------------------------------------------\n# Corporations\nset.seed(seed)\n  planType <- c(\"f\",\"p\")\n  sth_data[which(sth_data$corporate == \"c\"),]$planType <- \n    sapply(seq(nrow(sth_data[which(sth_data$corporate == \"c\"),])), \n           function(x) sample(planType, \n                              1, \n                              replace = TRUE, \n                              prob = c(.95, .5)))\n# Individuals\n  planType <- c(\"f\",\"p\")\n  sth_data[which(sth_data$corporate == \"i\"),]$planType <- \n    sapply(seq(nrow(sth_data[which(sth_data$corporate == \"i\"),])), \n           function(x) sample(planType, \n                              1, \n                              replace = TRUE, \n                              prob = c(.60, .40)))\n#-----------------------------------------------------------------\n# 6. Calculate the distance from the stadium\n#-----------------------------------------------------------------\n  set.seed(seed)\n  distances_corp <- rexp(num_purchasers) * 12\n  distances_indv <- rexp(num_purchasers) * 30\n# Corporate\n  set.seed(seed)\n  sth_data[which(sth_data$corporate == \"c\"),]$distance <- \n    sapply(seq(nrow(sth_data[which(sth_data$corporate == \"c\"),])), \n           function(x) sample(distances_corp, \n                              1, \n                              replace = TRUE))\n# Individuals\n  sth_data[which(sth_data$corporate == \"i\"),]$distance <- \n    sapply(seq(nrow(sth_data[which(sth_data$corporate == \"i\"),])), \n           function(x) sample(distances_indv, \n                              1, \n                              replace = TRUE))\n#-----------------------------------------------------------------\n# 7. Determine the number of tickets each account has purchased\n#-----------------------------------------------------------------\n  tickets <- c(10,8,6,5,4,3,2,1)\n  set.seed(seed)\n# Corporations\n  sth_data[which(sth_data$corporate == \"c\"),]$tickets <- \n    sapply(seq(nrow(sth_data[which(sth_data$corporate == \"c\"),])), \n           function(x) sample(tickets, 1, replace = TRUE, \n             prob = c(.02,.08,.10,.05,.50,.05,.20,0)))\n# Individuals\n  sth_data[which(sth_data$corporate == \"i\"),]$tickets <- \n    sapply(seq(nrow(sth_data[which(sth_data$corporate == \"i\"),])), \n           function(x) sample(tickets, 1, replace = TRUE, \n             prob = c(0,0,.10,.05,.40,.05,.30,.10))) \n#-----------------------------------------------------------------\n# 8a. Assign years the account holder has had tickets\n#-----------------------------------------------------------------\n  if(renew == T){\n  avgDist <- mean(sth_data$distance)\n  set.seed(seed)\n  tenures <- with(sth_data,mapply(f_calculate_tenure,\n                                  corporate,\n                                  planType,\n                                  distance,\n                                  avgDist))\n  sth_data$tenure <- as.vector(tenures)\n  }else{sth_data$tenure = 0}\n#----------------------------------------------------------------- \n# 9b. Function to calculate tenure\n#-----------------------------------------------------------------\nf_calculate_tenure<-function(corporate,planType,distance,avgDist){\nif(corporate == \"c\" & planType == \"f\" & distance <= avgDist){\n    ten <-round(abs(rnorm(1,mean = 14,sd = 6)),0)}\nelse if(corporate == \"i\" & planType == \"f\" & distance <= avgDist){\n    ten <-round(abs(rnorm(1,mean = 10,sd = 6)),0)}\nelse if(corporate == \"c\" & planType == \"p\" & distance <= avgDist){\n    ten <-round(abs(rnorm(1,mean = 3,sd = 2)),0)}\nelse if(corporate == \"i\" & planType == \"p\" & distance <= avgDist){\n    ten <-round(abs(rnorm(1,mean = 3,sd = 2)),0)}\nelse if(corporate == \"c\" & planType == \"f\" & distance >= avgDist){\n    ten <-round(abs(rnorm(1,mean = 9,sd = 3)),0)}\nelse if(corporate == \"i\" & planType == \"f\" & distance >= avgDist){\n    ten <-round(abs(rnorm(1,mean = 7,sd = 3)),0)}  \nelse if(corporate == \"c\" & planType == \"p\" & distance >= avgDist){\n    ten <-round(abs(rnorm(1,mean = 2,sd = 1)),0)}\nelse if(corporate == \"i\" & planType == \"p\" & distance >= avgDist){\n    ten <-round(abs(rnorm(1,mean = 2,sd = 1)),0)}\nelse{ten <-round(abs(rnorm(1,mean = 8,sd = 3)),0)}\n  return(ten) \n}\n#----------------------------------------------------------------- \n# 9a. SPEND\n#-----------------------------------------------------------------  \n  avgTenure <- mean(sth_data$tenure)\n  set.seed(seed)\n  spend <- with(sth_data,mapply(f_calculate_spend,\n                                corporate,\n                                planType,\n                                tenure,\n                                avgTenure))\n  sth_data$spend <- as.vector(spend) * sth_data$tickets\n#-----------------------------------------------------------------\n# 9b. Function to calculate spend\n#-----------------------------------------------------------------\nf_calculate_spend<- function(corporate,planType,tenure,avgTenure){\nif(corporate == \"c\" & planType == \"f\" & tenure >= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 7500,sd = 800)),0)}\nelse if(corporate == \"i\" & planType == \"f\" & tenure >= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 2100,sd = 500)),0)}\nelse if(corporate == \"c\" & planType == \"p\" & tenure >= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 2000,sd = 300)),0)}\nelse if(corporate == \"i\" & planType == \"p\" & tenure >= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 1200,sd = 200)),0)}\nelse if(corporate == \"c\" & planType == \"f\" & tenure <= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 5000,sd = 500)),0)}\nelse if(corporate == \"i\" & planType == \"f\" & tenure <= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 2000,sd = 300)),0)}  \nelse if(corporate == \"c\" & planType == \"p\" & tenure <= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 2000,sd = 400)),0)}\nelse if(corporate == \"i\" & planType == \"p\" & tenure <= avgTenure){\n    spend <-round(abs(rnorm(1,mean = 800,sd = 75)),0)}\nelse{spend <-round(abs(rnorm(1,mean = 2500,sd = 300)),0)}\n  return(spend) \n}\n#-----------------------------------------------------------------\n# 10a. Calculate the percentage of tickets used\n#-----------------------------------------------------------------\n  avgDist <- mean(sth_data$distance)\n  set.seed(seed)\n  ticket_use <- with(sth_data,mapply(f_calculate_ticket_use,\n                                     corporate,\n                                     distance,\n                                     avgDist))\n  sth_data$ticketUsage <- as.vector(ticket_use)\n#-----------------------------------------------------------------\n# 10b. Function to return ticket usage\n#-----------------------------------------------------------------\nf_calculate_ticket_use <- function(corporate,distance,avgDist){\nif(corporate == \"c\" & distance <= avgDist){\n  tu <- runif(1,min = .89, max = 1)}\n    else if(corporate == \"i\" & distance <= avgDist){\n      tu <- runif(1,min = .82, max = .94)}\n        else if(corporate == \"c\" & distance >= avgDist){\n          tu <- runif(1,min = .65, max = .9)}\n            else if(corporate == \"i\" & distance >= avgDist){\n              tu <- runif(1,min = .55, max = .85)}\n                else{tu <- runif(1,min = .65, max = .95)}\n  return(tu) \n}\n#-----------------------------------------------------------------\n# 11a.Return proper data frame\n#-----------------------------------------------------------------\n  if(renew == T){\n   sth_data_renew <-  f_renewal_assignment(seed,sth_data,\n                                           f_assign_renewal)\n   return(sth_data_renew)\n  }else{ return(sth_data)}\n#-----------------------------------------------------------------\n# 11b.Calculate renewal percentage\n#----------------------------------------------------------------- \nf_assign_renewal <- function(x,renew){\n  \n if(x == 10){sample(renew,1,prob = c(.99,.01))}\n  else if(x == 9){sample(renew,1,prob = c(.98,.02))}\n   else if(x == 8){sample(renew,1,prob = c(.95,.05))}\n    else if(x == 7){sample(renew,1,prob = c(.95,.05))}\n     else if(x == 6){sample(renew,1,prob = c(.92,.08))}\n      else if(x == 5){sample(renew,1,prob = c(.90,.10))}\n       else if(x == 4){sample(renew,1,prob = c(.85,.15))}\n        else if(x == 3){sample(renew,1,prob = c(.80,.20))}\n         else if(x == 2){sample(renew,1,prob = c(.30,.70))}\n          else if(x == 1){sample(renew,1,prob = c(.25,.75))}\n           else{sample(renew,1,prob = c(.5,.5))}\n}\n#-----------------------------------------------------------------\n# 11c.Calculate renewal percentage\n#-----------------------------------------------------------------\nf_renewal_assignment <- function(seed,sth_data,f_assign_renewal){\n\n  require(dplyr)\n\n  ids <- as.data.frame(sth_data$accountID)\n  names(ids) <- \"accountID\"\n  \n  set.seed(seed)\n  centers1 <- kmeans(sth_data$ticketUsage, centers = 5)$centers\n  centers1 <- sort(centers1)\n  ids$clusterTU <- \n    kmeans(sth_data$ticketUsage, centers = centers1)$cluster\n  \n  set.seed(seed)\n  centers2 <- kmeans(sth_data$distance, centers = 5)$centers\n  centers2 <- rev(sort(centers2))\n  ids$clusterDI <- \n    kmeans(sth_data$distance, centers = centers2)$cluster\n  \n  ids$clustSum   <- ids$clusterTU + ids$clusterDI\n  sth_data_renew <- dplyr::left_join(ids,sth_data, \n                                     by = \"accountID\")\n  \n  x <- 1\n  renew <- c(\"r\",\"nr\")\n  a_renew <- list()\n  while(x <= nrow(sth_data_renew)){\n    clust <- sth_data_renew[x,3]\n    a_renew[x] <- f_assign_renewal(clust,renew)\n    x <- x + 1\n  }\n  \n  sth_data_renew$renewed <- unlist(a_renew)\n  sth_data_renew <- dplyr::select(sth_data_renew,accountID,\n                                  corporate,season,planType,\n                                  ticketUsage,tenure,\n                                  spend,tickets,distance,\n                                  renewed)\n  return(sth_data_renew)\n\n} # End\n#-----------------------------------------------------------------\n# Function to build a parabola\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\nnew_data <- f_create_lead_scoring_data(434, \n                                       100,\n                                       \"2023\",\n                                       f_calculate_tenure,\n                                       f_calculate_spend,\n                                       f_calculate_ticket_use,\n                                       f_renewal_assignment,\n                                       f_assign_renewal,\n                                       renew = F)"},{"path":"chapter2.html","id":"simulating-operations-data","chapter":"2 Code and Data","heading":"2.3 Simulating Operations data","text":"Operations data includes many different data sets. ’ll cover building ballpark ingress scans table . However, many others line length concessions stand number transactions F&B.","code":""},{"path":"chapter2.html","id":"ticket-scans","chapter":"2 Code and Data","heading":"2.3.1 Ticket scans","text":"First can build simple function help us build parabola. function builds parabola x intercept x = 1 x = 300. points refer number observations make Chapter 10. building quadratic function spits y value value x. function takes familiar form quadratic function:\\[\\begin{equation}\n\\ {f(x)} = {ax^2} + {bx} + c\n\\end{equation}\\]can build function create many different data sets. function calculate number scans per increment normal distribution.function produce data set looks like :tells number scans happened one minute increments particular event.","code":"\n#--------------------------------------------------------------------\n# Function to build a parabola\n#--------------------------------------------------------------------\nf_calc_scans <- function(x,y,j){\n  a <- y/(x^2 - 300*x + 300)\n  z <- a*(j^2 - 301*j + 300)\n  return(z)\n}\n#-----------------------------------------------------------------\n# Function to return a scans data frame\n#-----------------------------------------------------------------\nf_get_scan_data <- function(x_value,y_value,seed,sd_mod){\n  require(FOSBAAS)\n  x_val <- x_value\n  y_val <- y_value\n  obs   <- seq(1,300, by = 1)\n  set.seed(seed)\n  scans           <- mapply(f_calc_scans,x_val,y_val,obs)\n  scan_data       <- data.frame(observations = obs,\n                                scans        = scans)\n  scan_data$scans <- round(sapply(scan_data$scans,\n                          function(x) abs(rnorm(1,x,x/sd_mod))),0)\n  return(scan_data)\n}\nscan_data <- FOSBAAS::scan_data"},{"path":"chapter2.html","id":"simulating-and-understanding-ticketing-data-sets","chapter":"2 Code and Data","heading":"2.4 Simulating and understanding ticketing data sets","text":"Ticketing data seems like relatively straight-forward. isn’t. lot complexity. environment dynamic, cases can get big-data territory. context book, stay away big-data problems. Many big-data problems nothing small-data problems disguise. nature sports, three years data likely enough everything need.’ll begin simulating three seasons worth data. sake simplicity, ’ll transform data format useful applications ’ll demonstrating throughout book. four features data important:Customer details demographicsTicket purchases (including secondary market purchases)Plan purchasesQualitative data obtained surveysElements season important macro level. analysis cover top-approach forecasting sales revenue.","code":""},{"path":"chapter2.html","id":"simulating-season-data","chapter":"2 Code and Data","heading":"2.5 Simulating season data","text":"Professional baseball teams typically play 81 games. ’ll simulating three seasons data. building schedule complex process, aren’t hindered myriad constraints therefore make easy . schedule provide framework customer ticketing data build subsequent sections chapter. also important note top-approach opposed bottom-approach consisting sales granular level.","code":""},{"path":"chapter2.html","id":"function-to-manually-bias-our-season-data","chapter":"2 Code and Data","heading":"2.5.1 Function to manually bias our season data","text":"function f_simulate_sales modifies certain characteristics ’ll use forecast sales. creating distributions numbers used place variables defined range possibilities. Instead walking code time, ’ll just tell :Creates base attendance team. instance, play BOS, CHC, NYY, LAD, STL get highest sales base.day week falls weekend sales base higher weekday.playing summer months, sales higher.school , sales higher.last game year, sales higher.opening day, sales higher.bobblehead, sales higherThis function creates pattern ticket sales data based averages normal distributions. can access function FOSBAAS package.function accepts several arguments. One function f_simulate_sales. asks three random numbers, three modifiers simply coefficients bias results , season.resulting data set looks like :Table 2.3: Sample season purchase dataThis data set simulated schedule several fields:game numberopposing teamdateday weekthe monthif weekendif schools outthe days since last gameif opening dayif promotionthe number ticket salesthe season yearThis fine base package. modeling can obviously go much deeper include statistics Las Vegas odds playoffs, granular promotional data, many others. ignoring current divisional structure sake simplicity. Structuring calendar complex task involves many factors aren’t considering :Travel timeRestrictions based collective bargaining agreementTeam level requestsThis data looks similar data actually beginning season. ’ll use forecast ticket sales chapter six. Another important piece data manifest. manifest represents seating inventory. interesting represents upper bound available sell. typical manifest look like :Table 2.4: Sample manifest data","code":"\n#-----------------------------------------------------------------\n# Function to build season data\n#-----------------------------------------------------------------\nseed1      <- 309\nseed2      <- 755\nseed3      <- 512\nmodifier   <- 1.00\ndayMod     <- 1.10\nmonthMod   <- 1.15\nseasonYear <- '2023'\n\nseason23   <- f_build_season(seed1, \n                             seed2, \n                             seed3, \n                             seasonYear,\n                             f_simulate_sales, \n                             modifier, \n                             dayMod, \n                             monthMod)\nseason_data <- FOSBAAS::season_data\nmanifest_data <- FOSBAAS::manifest_data"},{"path":"chapter2.html","id":"simulating-customer-data","chapter":"2 Code and Data","heading":"2.5.2 Simulating customer data","text":"data set simply customer id name. ’ll make names id. built list names couple government websites (ssa 2020) (Census 2020).’ll use data frame simulate transformed data set pulled ticketing system. Now can use data create data set purchases secondary market.Table 2.5: Secondary market purchasesThis data set contains:seat id corresponding manifestA customer id corresponding customer listThe ticket type (single game si, season se)game id scheduleThe number tickets soldA key fieldThe original price ticketA modeled cluster fieldThe price sold secondary marketA completely clean data set like unlikely encountered outside lab. reality, customer data fraught duplication problems. data already transformed. practice join tables produce data set looks like .","code":"\ncustomer_data <- FOSBAAS::customer_data\nsecondary_data <- head(FOSBAAS::secondary_data)"},{"path":"chapter2.html","id":"simulating-demographic-data","chapter":"2 Code and Data","heading":"2.6 Simulating demographic data","text":"Demographic data can used variety tasks segmentation targeted marketing efforts.Demographic data typically purchased one several vendors may contain hundreds columns. data contains:customer IDThe first name customerThe last name customerThe full name customerGenderAgeLatitudeLongitudeDistance ballparkmarital statusethnicitychildren living householdThe county customer lives","code":"\ndemo_data <- head(FOSBAAS::demographic_data)"},{"path":"chapter2.html","id":"simulating-survey-data","chapter":"2 Code and Data","heading":"2.7 Simulating survey data","text":"’ll use example survey data construct specific segmentation scheme based factor analysis. ’ll base analysis example Chapman Feit’s excellent “R Marketing Research Analytics” (Chris Chapman 2015). ’ll cover survey construction later chapter.","code":""},{"path":"chapter2.html","id":"perceptual-data","chapter":"2 Code and Data","heading":"2.7.1 Perceptual data","text":"initial data set take form multi-select table:data takes form aggregated survey response specific answers counted aggregated team questions. question read like :feel following sports properties? Please check apply teams listed.question allow us create perceptual map. ’ll send 5,000 customers created Section 2.5.2.produces following table:Table 2.6: Perceptual data","code":"\n#-----------------------------------------------------------------\n#-----------------------------------------------------------------\nperceptual_data             <- as.data.frame(matrix(nrow=3,ncol=10))\nnames(perceptual_data)      <- c('Friendly','Exciting','Fresh',\n                                 'Inovative','Fun','Old','Historic',\n                                 'Winners','Great','Expensive')\nrow.names(perceptual_data)  <- c('Game Hens','Grizzlies',\n                                 'Predators')\n\nset.seed(2632)\nperceptual_data <- apply(perceptual_data,1:2,\n                         function(x) round(rnorm(1,3000,1000),0))"},{"path":"chapter2.html","id":"pricing-survey-data","chapter":"2 Code and Data","heading":"2.7.2 Pricing survey data","text":"data used perform Van Westendorp analysis demonstrate basics qualitative pricing analytics. ’ll want data specific format.Van Westendorp28 analysis asks respondent answer series questions product related perception price. questions take following form:price consider product expensive consider buying ? (expensive)price consider product priced low feel quality couldn’t good? (cheap)price consider product starting get expensive, question, give thought buying ? (Expensive/High Side)price consider product bargain—great buy money? (Cheap/Good Value)questions taken directly Wikipedia article. resulting data set resemble following:Table 2.7: Van Westendorp survey dataThese data sets represent main top-line data tend use working club.","code":"\n#-----------------------------------------------------------------\n#-----------------------------------------------------------------\nvw_data <- data.frame(matrix(nrow = 1000, ncol = 6))\nnames(vw_data) <- c('DugoutSeats', 'PriceExpectation', \n                    'TooExpensive', 'TooCheap', \n                    'WayTooCheap', 'WayTooExpensive')\nset.seed(715)\nvw_data[,1] <- 'DugoutSeats'\nvw_data[,2] <- round(rnorm(1000,100,10),0)\nvw_data[,3] <- round(rnorm(1000,130,20),0)\nvw_data[,4] <- round(rnorm(1000,60,15),0)\nvw_data[,5] <- round(rnorm(1000,50,10),0)\nvw_data[,6] <- round(rnorm(1000,160,20),0)"},{"path":"chapter2.html","id":"housing-your-data-and-functions-in-a-package","chapter":"2 Code and Data","heading":"2.8 Housing your data and functions in a package","text":"book isn’t meant expose use R language, R great features analytics work worth exploring. make lives easier move subsequent chapters. Analytically focused work tends repetitive housing functions package make life easier worth living. many resources available make building package easy ’ll cover basics .two packages make building packages really simple: devtools (Wickham, Hester, et al. 2022) roxygen (Wickham, Danenberg, et al. 2022). also numerous resources build package. Rstudio even built--tools makes process even easier. want demonstrate process really easy incredibly useful.","code":""},{"path":"chapter2.html","id":"building-a-simple-package","chapter":"2 Code and Data","heading":"2.8.1 Building a simple package","text":"’ll want open RStudio create new R file. Creating basic package takes steps:Now installed packages need can use create package:can create data set want include package.place data package.Finally, install package.can access data using packages namespace.lot , especially around documentation. However, now housed data package can easily access. begin using R heavily highly recommend putting time understanding create use packages. little feature save lot time. also allows share work service Github working remotely team.","code":"\n#-----------------------------------------------------------------\n# Step 1: Download the utility packages\n#-----------------------------------------------------------------\ninstall.packages(\"devtools\")\ninstall.packages(\"roxygen2\")\n##browseVignettes(\"devtools\")\n##browseVignettes(\"roxygen2\")\n#-----------------------------------------------------------------\n# Step 2: Create a shell for your package\n#-----------------------------------------------------------------\ndevtools::create(\"UselessRPackage\")\n#-----------------------------------------------------------------\n# Step 3: Build a data set\n#-----------------------------------------------------------------\ndevtools::document()\nuselessData <- seq(1:755)\n#-----------------------------------------------------------------\n# Step 4: Place data set in package\n#-----------------------------------------------------------------\nusethis::use_data(uselessData, overwrite = T)\n#-----------------------------------------------------------------\n# Step 5: Install the package\n#-----------------------------------------------------------------\ndevtools::install()\n#-----------------------------------------------------------------\n# Step 5: Access your data\n#-----------------------------------------------------------------\nUselessRPackage::uselessData"},{"path":"chapter2.html","id":"key-concepts-and-chapter-summary-1","chapter":"2 Code and Data","heading":"2.9 Key concepts and chapter summary","text":"chapter written introduce little code R explain data data sets. also demonstrates concepts cover ensuing chapters. also covered R may may good choice use analysis.Additionally, covered basics build package R. Analysis tends repetitive. Housing specific functions package simple way document functions easily access . R’s extensibility one best features makes incredibly flexible tool (especially coupled RStudio).Chapter 3 delve data creating graphics summarizing data. Subsequent chapters explore additional functionality delve solving specific problems. also spend significant amount time explaining think problems solve .","code":""},{"path":"chapter3.html","id":"chapter3","chapter":"3 Data Exploration","heading":"3 Data Exploration","text":"R makes easy exploratory work data. numerous packages functions make simple visualize tabulate data. ’ll leverage many packages outlined book “R Data Science” (Wickham 2017) graphics created excellent graphics package called ggplot2 (Wickham, Chang, et al. 2022). ggplot2 makes easy look data many different ways.’ll also recommend several methods analyzing data. “R Action” Robert Kabacoff (Kabacoff 2011) outstanding job demonstrating cursory data analysis variety data sets. ’ll reference book several times chapter. innumerable statistical methods rubrics analyzing data, graphs best job conveying information encounter book. always important don’t like building analytics may .","code":""},{"path":"chapter3.html","id":"building-a-consistent-design-language","chapter":"3 Data Exploration","heading":"3.1 Building a consistent design language","text":"several graphics paradigms available R elsewhere. D3 (d3 2020) outstanding javascript library building dynamic graphics. Python also contains several libraries seaborn (Waskom 2021) help construct statistical graphics. ’ll use ggplot2 (Wickham, Chang, et al. 2022) paradigm graphics book. ggplot2 make easy build consistent visually appealing graphics R.Let’s start constructing graphics theme use throughout examples. begin example creating color palette can use modify. Afterward, simply set variable equal set parameters corresponding components images creating.object graphics_theme_1 used subsequent graphs. keep looking consistent. can easily adjust element simply adding plot theme. override graphics theme illustrates elegance paradigm. Let’s pull data FOSBAAS package take look .Table 3.1: Example schedule dataWe’ve already talked data, next sections demonstrate sorts plots exploratory analysis typically perform help understand patterns data. want review data simply type ?FOSBAAS::season_data R console. part analysis critical. helps understand data can perform sophisticated modeling later steps. also best way communicate patterns data less experience . ’ll also offer word warning. people think distributions. Simple bar line plots get traction folks. Don’t attempt explain box plot someone meeting. lose room.","code":"\nlibrary(ggplot2)\nlibrary(dplyr)\n#-----------------------------------------------------------------\n# creating a color palette\n#-----------------------------------------------------------------\npalette <- c('dodgerblue','grey25','coral','mediumseagreen',\n             'orchid','firebrick','goldenrod','cyan',\n             'brown','steelblue','magenta')\n#-----------------------------------------------------------------\n# Creating a custom theme\n#-----------------------------------------------------------------\ngraphics_theme_1 <- ggplot2::theme() + \n  theme(axis.text.x  = element_text(angle  = 0, size  = 14, \n                                    vjust  = 0, color = \"grey10\"),  \n        axis.text.y  = element_text(angle  = 0, size  = 14, \n                                    vjust  = 0, color = \"grey10\"),  \n        axis.title.x = element_text(size   = 16, face = \"plain\", \n                                    colour = \"grey10\"), \n        axis.title.y = element_text(size   = 16, face = \"plain\", \n                                    color  = \"grey10\"), \n        legend.title = element_text(size   = 14, face = \"plain\", \n                                    color  = \"grey10\"), \n        legend.text  = element_text(size   = 11, \n                                    color  = \"grey10\"), \n        plot.title   = element_text(colour = \"grey10\", \n                                    size   = 14, angle = 0, \n                                    hjust  = .5, vjust = .5, \n                                    face   = \"bold\"), \n        legend.position   = \"right\", \n        legend.background = element_rect(fill     = \"grey99\", \n                                         size     = 3,  \n                                         linetype = \"solid\", \n                                         colour   = \"grey99\"), \n        legend.key        = element_rect(fill     = \"grey99\", \n                                         color    = \"grey99\"), \n        strip.background  = element_rect(fill     =  \"grey99\", \n                                         colour   = \"grey99\"), \n        strip.text        = element_text(size     = 14, \n                                         face     = \"plain\", \n                                         color    = \"grey10\"), \n        panel.grid.major  = element_line(colour   = \"grey80\"),  \n        panel.grid.minor  = element_line(colour   = \"grey80\"), \n        panel.background  = element_rect(fill     = \"grey99\", \n                                         colour   = \"grey99\"), \n        plot.background   = element_rect(fill     = \"grey99\", \n                                         colour   = \"grey99\"))\n#-----------------------------------------------------------------\n# High level schedule data\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\nseason_data <- FOSBAAS::season_data"},{"path":"chapter3.html","id":"histograms-and-density-plots","chapter":"3 Data Exploration","heading":"3.2 Histograms and density plots","text":"Histograms density plots critical understanding underlying structure data. Using point estimate average good way misinterpret data. Learn think distributions. DMV look line customers think height distribution. watch Olympics think distribution times athletes ran swam. everywhere else . following two graphs called simple commands: geom_histogram geom_density.code creates graph figure 3.1. Histograms incredibly useful understanding data set’s structure. unfamiliar , demonstrate count specific instance something (case, ticket sales). also included rug bottom graph. rug demonstrates data points actually lie graph. may unnecessary, find helps interpret graph little easily dealing smaller data sets. especially true density plots.following diagram also layers different seasons diagram using colors. colors use important. aren’t going cover design , basic understanding color wheel.29 Google . also several R packages deal color RColorBrewer (Neuwirth 2022). package designed maps demonstrates point trying make. Put thought colors use use . ’ll give graphics polished professional look.\nFigure 3.1: Histogram Ticket Sales\ncan interpret plot? Honestly, difficult see much difference. looks like 2024 higher percentage sellouts. also looks like huge spike around 31,000 tickets. Perhaps better ways help us understand data.small change, can create density plot. Density plots convey similar information histograms, can confusing individuals. Know audience choosing use one interpretable histogram. can see plot figure 3.2.kernel density plot represents density particular point. Think much area lives curve. plot looks like :\nFigure 3.2: Density Plot Ticket Sales\ngraph helps us visualize skew little better histogram. used alpha argument make graphs transparent. much easier see 2024 skews right 2022 2023.can interpret data curve certain point? can approximate fairly easily. multiple methods don’t necessarily integrate data. However, use integral R can help . Keep mind actually calculus working club business side probably something wrong. fact, using calculus probably something wrong. Take step back.can use density function help understand going curves.part intuitive. can now use data make observations specific points x axis.thirteen percent area curve points 40,000 tickets. included calculations demonstrative purposes. make density plots easier understand. perform calculations density curve help us understand data little granular fashion.Histograms density plots particular use performing certain statistical analyses data. ’ll use frequently. Familiarize learn love .","code":"\n#-----------------------------------------------------------------\n# Histograms\n#-----------------------------------------------------------------\nseason_data <- FOSBAAS::season_data\nx_label  <- ('\\n Ticket Sales')\ny_label  <- ('Count \\n')\ntitle    <- ('Distribution of Seasonal Ticket Sales')\nlegend   <- ('Season')\nhist_sales <- \n  ggplot2::ggplot(data  = season_data,\n                  aes(x = ticketSales,\n                      fill  = factor(season)))         +\n  geom_histogram(binwidth = 1000)                      +\n  scale_fill_manual(legend, values = palette)          +\n  geom_rug(color = 'coral')                            +\n  scale_x_continuous(label = scales::comma)            +\n  scale_y_continuous(label = scales::comma)            +\n  xlab(x_label)                                        + \n  ylab(y_label)                                        + \n  ggtitle(title)                                       +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Kernel density plot\n#-----------------------------------------------------------------\nx_label  <- ('\\n Ticket Sales')\ny_label  <- ('Density \\n')\ntitle    <- ('Distribution of Seasonal Ticket Sales')\nlegend   <- ('Season')\ndensity_sales <- \n  ggplot2::ggplot(data = season_data, \n                  aes(x    = ticketSales, \n                      fill = factor(season)))                +\n  geom_density(alpha = .5)                                   +\n  scale_fill_manual(legend,values = palette)                 +\n  geom_rug(color = 'coral')                                  +\n  scale_x_continuous(label = scales::comma)                  +\n  scale_y_continuous(label = scales::percent)                +\n  xlab(x_label)                                              + \n  ylab(y_label)                                              + \n  ggtitle(title)                                             +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Demonstrate AUC\n#-----------------------------------------------------------------\nden      <- density(season_data$ticketSales)\nbin_size <- (den$x[2] - den$x[1])\n\nround(sum(den$y) * bin_size,2) # Approximates to 1\n#> [1] 1\n#-----------------------------------------------------------------\n# Demonstrate AUC at 40,000 Tickets\n#-----------------------------------------------------------------\nsum(den$y[den$x >= 40000]) * bin_size\n#> [1] 0.1337782"},{"path":"chapter3.html","id":"box-faceted-and-scatter-plots","chapter":"3 Data Exploration","heading":"3.3 Box, faceted, and scatter plots","text":"can also easily split data season analyze distributions slightly differently. case, ’ve used facet_grid(season ~ .) argument split graph season. really isn’t anything different . function facet_wrap() something similar. Try see difference. faceted histogram can seen figure 3.3.\nFigure 3.3: Faceted histogram\nNotice restricted graph one color. Avoid multiple colors graphs split data. don’t need multiple ways distinguish individual subsets data . One way suffice.get interpretation graph preceeding one. 2024 skewed right likely indicating higher average ticket sales. 2023 appears clusters attendance, 2022 fairly evenly distributed. Perhaps differences schedule.Splitting graphs various features useful ’ll see frequently. can create box-plots changing one argument. following code produces box-plot figure 3.4.Boxplots preferred method looking distributions. case, blended boxplot scatter plot. interpret boxplot? black line middle colored section represents median. box represents 50th percentile values. interquartile (IQR) range representing values 25% 50%. whiskers represent calculation based range dots represent outliers.\nFigure 3.4: Segmented histogram box-whisker plots\ndata easier interpret fashion. IQR skewed much higher clearly larger median value. use median instead mean? median safer interpretation looking non-normal distributions (distributions skew). actually best look , get .’ll explore one type basic plot. violin plot blend density plot boxplot. following code produces plot figure 3.5.\nFigure 3.5: Segmented violin plot\nthink violin plots interesting, typically don’t use . prefer split density plots boxplots separate graphs. may feel differently. recommend showing one plots someone limited statistical experience. Perhaps shouldn’t use .","code":"\n#-----------------------------------------------------------------\n# Faceting a plot\n#-----------------------------------------------------------------\nx_label  <- ('\\n Ticket Sales')\ny_label  <- ('Count \\n')\ntitle    <- ('Distribution of Seasonal Ticket Sales')\nhistogram_sales_facet <- \n  ggplot2::ggplot(data = season_data, \n                  aes(x = ticketSales))                        +\n  facet_grid(season ~ .)                                       +\n  geom_histogram(binwidth = 1000, fill = palette[1])           +\n  geom_rug(color = 'coral')                                    +\n  scale_x_continuous(label = scales::comma)                    +\n  scale_y_continuous(label = scales::comma)                    +\n  xlab(x_label)                                                + \n  ylab(y_label)                                                + \n  ggtitle(title)                                               +\n  graphics_theme_1 \n#-----------------------------------------------------------------\n# Box plots\n#-----------------------------------------------------------------\nx_label  <- ('\\n Season')\ny_label  <- ('Ticket Sales \\n')\ntitle    <- ('Distribution of Seasonal Ticket Sales')\nbox_sales <- \n  ggplot2::ggplot(data  = season_data, \n                  aes(x = factor(season), \n                      y = ticketSales))               +\n  geom_boxplot(fill = 'dodgerblue')                   +\n  geom_jitter(alpha = .2,  height = 0, \n              width = .25, color  = 'coral')          +\n  geom_rug(color = 'coral')                           +\n  scale_y_continuous(label = scales::comma)           +\n  xlab(x_label)                                       + \n  ylab(y_label)                                       + \n  ggtitle(title)                                      +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# violin plot\n#-----------------------------------------------------------------\nx_label  <- ('\\n Season')\ny_label  <- ('Ticket Sales \\n')\ntitle    <- ('Distribution of Seasonal Ticket Sales')\nviolin_sales <-\n  ggplot2::ggplot(data = season_data, \n                  aes(x = factor(season), \n                      y = ticketSales))             +\n  geom_violin(fill = 'dodgerblue')                  +\n  geom_jitter(alpha = .35, height = 0, \n              width = .25, color = 'coral')         +\n  geom_rug(color = 'coral')                         +\n  scale_y_continuous(label = scales::comma)         +\n  xlab(x_label)                                     + \n  ylab(y_label)                                     + \n  ggtitle(title)                                    +\n  graphics_theme_1"},{"path":"chapter3.html","id":"line-and-tile-plots","chapter":"3 Data Exploration","heading":"3.4 Line and tile plots","text":"Line plots tile plots can used various ways. Line plots tend used demonstrate sort longitudinal trend. Tile plots can help visualize data multidimensional correlates features displayed. Line plots need sort grouping variable can occasionally frustrating work . general, put together way plots seen. following code produces plot figure 3.6.Line plots mainstay analysis. typically leveraged demonstrate change time series data points. pretty familiar . word warning can become busy. Perhaps plot better lines placed seperate graph.\nFigure 3.6: Line Tile Plots\nappears degree seasonality data. can see games relatively low attendance middle season. Perhaps school summer months helps bolster attendance. Baseball also doesn’t major sports competing months.look past sixtieth game notice 2024 surge attendance. Maybe playoff race. Line graphs also interesting notice trend seasonality might able decompose components using exponential smoothing even moving average. practice, found limited use techniques sports, something think aware .Tile plots (heatmaps) add level dimensionality data can extremely useful. next example little different. need aggregation data visualizing . ’ll use dplyr transformations found book. just makes life easier. different tend see languages python (unless ported). uses pipe %>% say words . ’ll seeing lot try data. simply selecting columns, grouping , creating aggregation. feels similar SQL. wonder ?… plot can seen figure 3.7.\nFigure 3.7: Tile Plots\ninterpret data? can clearly see Friday Saturday tend higher average ticket sales. shouldn’t surprise. also looks like Sundays March well. Beware! likely artifact reduced sample size. Overall, Heatmaps good job visualizing correlations, can misleading. careful always consider sampling issues.can also pump dplyr data directly ggplot. See .Let’s also take look variation heatmap called hex plot. can used slightly different ways. can see example hex plot figure 3.8.\nFigure 3.8: Hex Plots\ngraphic little different heatmap. demonstrates number games fall bin. tend use plots continuous variables x y axis interested density point. pay attention can perceive seasonality evident around game forty.","code":"\n#-----------------------------------------------------------------\n# Line plot\n#-----------------------------------------------------------------\nx_label <- ('\\n Game Number')\ny_label <- ('Ticket Sales \\n')\ntitle   <- ('Ticket Sales by Game Number')\nlegend  <- 'Season'\nline_sales <- \n  ggplot2::ggplot(data      = season_data, \n                  aes(x     = gameNumber,\n                      y     = ticketSales,\n                      color = factor(season)))             +\n  geom_line(size = .9)                                     +\n  scale_color_manual(legend, values = palette)             +\n  scale_y_continuous(label = scales::comma)                +\n  xlab(x_label)                                            + \n  ylab(y_label)                                            + \n  ggtitle(title)                                           +\n  graphics_theme_1 + theme(legend.position   = \"bottom\")\n#-----------------------------------------------------------------\n# Tile plot or heat map\n#-----------------------------------------------------------------\nx_label <- ('\\n Day of Week')\ny_label <- ('Month \\n')\ntitle   <- ('Ticket Sales by Day and Month')\n# compress data into an easier format\nsd_comp <- season_data                    %>% \n  select(dayOfWeek,month,ticketSales)     %>%\n  group_by(dayOfWeek,month)               %>%\n  summarise(avgSales = mean(ticketSales))\n\ntile_sales <- \n  ggplot2::ggplot(data     = sd_comp, \n                  aes(x    = dayOfWeek,\n                      y    = month,\n                      fill = avgSales))                       +\n  geom_tile()                                                 +\n  scale_fill_gradient(low = \"white\", high = \"dodgerblue\",\n                      name = 'Tickets',label = scales::comma) +\n  xlab(x_label)                                               + \n  ylab(y_label)                                               + \n  ggtitle(title)                                              +\n  graphics_theme_1\n\n\n  season_data                             %>% \n  select(dayOfWeek,month,ticketSales)     %>%\n  group_by(dayOfWeek,month)               %>%\n  summarise(avgSales = mean(ticketSales)) %>%\n\n  ggplot2::ggplot(aes(x    = dayOfWeek,\n                      y    = month,\n                      fill = avgSales))   +\n  geom_tile() \n#-----------------------------------------------------------------\n# Hexplots\n#-----------------------------------------------------------------\nx_label  <- ('\\n Game Number')\ny_label  <- ('Ticket Sales \\n')\ntitle   <- ('Ticket Sales by game')\n\nhex_sales <- \n  ggplot2::ggplot(data     = season_data, \n                  aes(x    = gameNumber,\n                      y    = ticketSales))                    +\n  geom_hex()                                                  +\n  scale_fill_gradient(low = \"dodgerblue\", high = \"coral\",\n                      name = 'Count',label = scales::comma)   +\n    scale_y_continuous(label = scales::comma)                 +\n  xlab(x_label)                                               + \n  ylab(y_label)                                               + \n  ggtitle(title)                                              "},{"path":"chapter3.html","id":"bar-plots","chapter":"3 Data Exploration","heading":"3.5 Bar plots","text":"want use pie chart, don’t. Bar plots almost always better choice. Visualizing relationships area simply easier bar plot. Bar plos probably popular way display data, find lacking. , really need train think distributions. Bar Plots counts. basic bar plot produced following code can seen figure 3.9.\nFigure 3.9: Proportion sales year weekend\nexample looking percentages tickets sold weekend vs. weekdays season. graph great job . can clearly see weekends make higher proportion sales 2024. also know weekdays make majority sales.can build variation plot position argument. Now can take look overall numbers. second bar plot can viewed figure 3.10.\nFigure 3.10: Barplot sales\ncan see 2022 2023 almost identical 2024 significantly ticket sales. type plot might use descriptive reporting. something accounting want see. don’t need know something happened, typically just need . true cases face. Make sure clearly explain .","code":"\n#-----------------------------------------------------------------\n# Bar plot version one\n#-----------------------------------------------------------------\nx_label  <- ('\\n Season')\ny_label  <- ('Proportion of Ticket Sales \\n')\ntitle    <- ('Proportion of Ticket Sales by DOW')\nbar_sales_pro <- \n  ggplot2::ggplot(data     = season_data, \n                  aes(y    = ticketSales,\n                      x    = season,\n                      fill = weekEnd))                   +\n  geom_bar(stat = 'identity',position = 'fill')          +\n  scale_fill_manual(values = palette, name = 'Weekend')  +\n  scale_y_continuous(label = scales::percent)            +\n  xlab(x_label)                                          + \n  ylab(y_label)                                          + \n  ggtitle(title)                                         +\n  graphics_theme_1 + theme(legend.position   = \"bottom\")\n#-----------------------------------------------------------------\n# Bar plot version two\n#-----------------------------------------------------------------\nx_label  <- ('\\n Season')\ny_label  <- ('Ticket Sales \\n')\ntitle    <- ('Ticket Sales by DOW')\nbar_sales <- \n  ggplot2::ggplot(data     = season_data, \n                  aes(y    = ticketSales,\n                      x    = season,\n                      fill = weekEnd))                  +\n  geom_bar(stat = 'identity', position = 'stack')       +\n  scale_fill_manual(values = palette, name = 'Weekend') +\n  scale_y_continuous(label = scales::comma)             +\n  xlab(x_label)                                         + \n  ylab(y_label)                                         + \n  ggtitle(title)                                        +\n  graphics_theme_1 + theme(legend.position   = \"bottom\")"},{"path":"chapter3.html","id":"a-final-word-on-graphics","chapter":"3 Data Exploration","heading":"3.6 A final word on graphics","text":"cautious represent data. just covered absolute rudiments creating graphs demonstrated commonly used instruments. Always think simplest way illustrate point trying make. Bar plots usually fine. like follow rules building graphs, don’t take word . Many people tell exactly things.Never use pie chart. difficult interpret bar plotDon’t use two Y axes. almost always better use two graphs differences scale units can confusing.Stick consistent color scheme.Stick consistent font schemeDon’t put much one graph. many colors shapes make graphs confusing. Always simplify.Additionally, didn’t cover dynamic plots. R provides interesting capabilities front. Shiny (Chang et al. 2022) allows build cool interactive web apps. also innumerable tools Tableau 30 Looker provide good business intelligence capability. Often times tools easier use quick ad hoc data exploration, lot utility putting everything code. automatically documents work can read . also gives readily available reference. find work can tend repetitive. use lot variations theme. code already written. just need adapt .","code":""},{"path":"chapter3.html","id":"summarizing-the-data","chapter":"3 Data Exploration","heading":"3.7 Summarizing the data","text":"section make heavy use R package already seen named dplyr (Wickham, François, et al. 2022). ’ll use dplyr easy read performant data sets ’ll encounter. ’ll also reference couple packages useful summarizing data psych (Revelle 2022). Summarizing data often accouterment graphs. don’t always , find often.Let’s create simple summary ticket sales day week using dplyr.Table 3.2: Mean ticket sales day weekdplyr wide assortment useful tools manipulating data. highly recommend leveraging whenever possible. edification, identical table can produced base R using function:many data manipulation packages data.table (Dowle Srinivasan 2021) plyr (Wickham 2022b). Let’s take look examples. frequently find looking quantiles. can use data set simple fences segmentation.case know 25% observations 26,126.5 tickets sold. can access components object brackets index. Many object R can accessed indexes. comes handy scripting.Turning data long wide format vise-versa something also common task. Perhaps like see Giants Baltimore need reformat data. little complex, demonstrates logically system operates. ’ll use dplyr tidyr (Wickham Girlich 2022) libraries achieve result. can perform tasks SQL, won’t intuitive user friendly . Learn use SQL vs. R vice-versa.Table 3.3: Median ticket sales day weekWe now table tells us exactly want know. practice probably need data long format often wide format. Let’s pretend want graph data. way ggplot works probably want convert data long format like example .Table 3.4: Median ticket sales day weekThis data much easier use ggplot. part, main operations performing. can much complex, just went main building blocks. understand just covered, surprised far can take .","code":"\n#-----------------------------------------------------------------\n# Creating a summary statistics table\n#-----------------------------------------------------------------\nlibrary(dplyr)\naverage_by_dow <- \nFOSBAAS::season_data                          %>% \n  group_by(dayOfWeek)                         %>% \n  summarise(AverageSales = mean(ticketSales))\n#-----------------------------------------------------------------\n# Creating a summary statistics table using 'by'\n#-----------------------------------------------------------------\nby(FOSBAAS::season_data$ticketSales,\n   FOSBAAS::season_data$dayOfWeek,function(x) mean(x))\n#> FOSBAAS::season_data$dayOfWeek: Fri\n#> [1] 38794.03\n#> --------------------------------------------- \n#> FOSBAAS::season_data$dayOfWeek: Mon\n#> [1] 28243.46\n#> --------------------------------------------- \n#> FOSBAAS::season_data$dayOfWeek: Sat\n#> [1] 38146.29\n#> --------------------------------------------- \n#> FOSBAAS::season_data$dayOfWeek: Sun\n#> [1] 29690.38\n#> --------------------------------------------- \n#> FOSBAAS::season_data$dayOfWeek: Thu\n#> [1] 28560.53\n#> --------------------------------------------- \n#> FOSBAAS::season_data$dayOfWeek: Tue\n#> [1] 28217.72\n#> --------------------------------------------- \n#> FOSBAAS::season_data$dayOfWeek: Wed\n#> [1] 28893.47\n#-----------------------------------------------------------------\n# Getting quantiles\n#-----------------------------------------------------------------\nquants <- quantile(FOSBAAS::season_data$ticketSales, \n                   probs = c(0,.10,.25,.5,.75,.9,1))\nquants\n#>      0%     10%     25%     50%     75%     90%    100% \n#> 19920.0 22797.6 26126.5 30956.0 35664.0 40816.6 45000.0\n#-----------------------------------------------------------------\n# Getting quantiles\n#-----------------------------------------------------------------\nquants[3]\n#>     25% \n#> 26126.5\n#-----------------------------------------------------------------\n# Converting to wide format\n#-----------------------------------------------------------------\nlibrary(dplyr)\nlibrary(tidyr)\nteam_dow <- \n  FOSBAAS::season_data                                   %>%\n  select(team,dayOfWeek,ticketSales)                     %>%\n  filter(team %in% c('SF','BAL'))                        %>%\n  group_by(team,dayOfWeek)                               %>%\n  summarise(medianSales = median(ticketSales),\n            games       = n())                           %>%\n  tidyr::pivot_wider(names_from  = team,\n                     values_from = c(medianSales,games)) %>%\n  mutate(difference = medianSales_BAL - medianSales_SF)  %>%\n  arrange(difference)                          \n#-----------------------------------------------------------------\n# Converting to long format\n#-----------------------------------------------------------------\nlibrary(dplyr)\nlibrary(tidyr)\nteam_dow_long <- \n  team_dow                                          %>%\n  select(dayOfWeek, medianSales_BAL,medianSales_SF) %>%\n  tidyr::pivot_longer(!dayOfWeek, \n                      names_to  = \"club\", \n                      values_to = \"medianSales\")\n                         "},{"path":"chapter3.html","id":"getting-statistical-information","chapter":"3 Data Exploration","heading":"3.7.1 Getting statistical information","text":"also thought might useful mention ways get statistical data without calculate . R excels several people extended ’s capabilities. ’ll show couple ones used . psych (Revelle 2022) library contains many useful functions manipulating summarizing data. ’ll use describe get basic statistical features data.Table 3.5: Summary StatisticsThis gives good overview structure ticket sales data looks like. ’s also good opportunity point one R’s quirks. psych package Hmisc (Harrell 2022) package function name (describe). even similar things. cautious dependencies can cause problems. load packages can overwrite . get around issue can always leverage full namespace: psych::describe instead describe.following example uses Hmisc package display summary statistics.can remove libraries environment using couple different commands: (unloadNamespace(\"Hmisc\") detach(\"package:Hmisc\")) Read documentation understand differences.Table 3.6: Summary Statistics using HmiscAs can see, somebody else done hard work . just need know look. Keep mind R packages high quality. software user-generated provided free. lots reputable packages, also lots poorly written. sometimes useful look provenance.","code":"\n#-----------------------------------------------------------------\n# Summary stats psych\n#-----------------------------------------------------------------\nlibrary(psych)\npsy_desc <- \n  t(data.frame(psych::describe(FOSBAAS::season_data$ticketSales)))\n#-----------------------------------------------------------------\n# Summary stats Hmisc\n#-----------------------------------------------------------------\nhmisc_desc <- (Hmisc::describe(FOSBAAS::season_data$ticketSales))\nhmisc_desc <- unlist(hmisc_desc$counts)\nhmisc_desc <- as.data.frame(hmisc_desc)"},{"path":"chapter3.html","id":"building-models-and-basic-statistics","chapter":"3 Data Exploration","heading":"3.7.2 Building models and basic statistics","text":"’ll apply rigor models progress ensuing chapters. However, want introduce couple concepts chapter. add complexity move actual projects.","code":""},{"path":"chapter3.html","id":"anova","chapter":"3 Data Exploration","heading":"3.7.2.1 ANOVA","text":"analysis variance commonly used tool gauge differences groups. going take cursory look one version ANOVA. lots different types ANOVA ’ll want put time understanding use . Additionally, ANOVA output equivalent regression output. ’ll use want understand differences groups. ways analyze differences groups students T test non-parametric methods. .following example one-way ANOVA covariate. want look differences ticket sales promotion day week. can begin building fequency table.group completely different representations indicate balanced design. matter? certainly . Test group differences using aov function.Table 3.7: ANOVA resultsWe’ll discuss designing experiments (DOE) later chapter. now, ’ll simply walk output. aren’t rigorous . Basically, P Value looks good F statistic isn’t super small. can move forward. Let’s take look statistics graph.\nFigure 3.11: Group means standard error promotion dow\nmodel output isn’t incredibly informative, graph gives lot information. model constructed can access parts dollar sign: mod$terms. lot information explore models begin making complex. case, can see sample issues.can also see certain promotions may effective others day week may impact effective promotion might absolute terms. Let’s apply Tukey test31 data. test help us compare pairs means based students t test. aren’t familiar tests, little research . can useful.Table 3.8: Tukey ComparisonsIn case, mean ticket sales bobbleheads concerts aren’t significantly different . mean? KPI ticket sales different marginal costs associated concerts bobbleheads less expensive option might preferable. Let’s visualize results.\nFigure 3.12: Group means standard error promotion dow\ninterval crosses zero, isn’t significant can potentially disregard information inconclusive.","code":"\n#-----------------------------------------------------------------\n# Build a frequency table\n#-----------------------------------------------------------------\ntable(FOSBAAS::season_data$promotion)\n#> \n#> bobblehead    concert       none      other \n#>         16          8        212          7\n#-----------------------------------------------------------------\n# One Way ANOVA\n#-----------------------------------------------------------------\nmod <- aov(ticketSales ~ promotion + dayOfWeek,\n           data = FOSBAAS::season_data)\n#summary(mod)\n#-----------------------------------------------------------------\n# Viewing group means\n#-----------------------------------------------------------------\nlibrary(dplyr)\ngraph_table <- FOSBAAS::season_data %>%\n               \n    select(promotion,ticketSales,dayOfWeek,daysSinceLastGame)  %>%\n    group_by(promotion,dayOfWeek)                              %>%\n    summarise(sales = mean(ticketSales),\n              daysSinceLastGame = mean(daysSinceLastGame),\n              N = n(),\n              sd = sd(ticketSales),\n              se = sd/sqrt(N))         \n\nx_label <- 'Day of Week'                                             \ny_label <- 'Mean ticket sales'                                            \ntitle   <- 'Group means and standard error: promos and sales'\nse <- \nggplot(graph_table, aes(y=sales, \n                        x=reorder(dayOfWeek,sales,mean), \n                        color=promotion))                     + \n    geom_errorbar(aes(ymin = sales-se, ymax = sales+se), \n                  width =.3,size = 1,\n                  position = position_dodge(0.25))            +\n    geom_point()                                              +\n    scale_color_manual(values = palette)                      +\n    scale_y_continuous(label = scales::comma)                 +\n    xlab(x_label)                                             + \n    ylab(y_label)                                             + \n    ggtitle(title)                                            +\n    graphics_theme_1 \n#-----------------------------------------------------------------\n# Tukey test\n#-----------------------------------------------------------------\ntu_test <- TukeyHSD(mod)\n#-----------------------------------------------------------------\n# Viewing group means\n#-----------------------------------------------------------------\nx_label <- 'Value'                                             \ny_label <- 'Promotion Comps'                                            \ntitle   <- '95% CI comps by promotion '\ntu_comp_graph <- \nggplot(tu_comp, aes(x=diff, \n                    y=promotion))                             + \n    geom_errorbar(aes(xmin = lwr, xmax = upr), \n                  width =.3,size = 1)                         +\n    geom_point()                                              +\n    scale_x_continuous(label = scales::comma)                 +\n    xlab(x_label)                                             + \n    ylab(y_label)                                             + \n    ggtitle(title)                                            +\n    geom_vline(xintercept = 0,color = 'red',lty = 2)          +\n    graphics_theme_1 "},{"path":"chapter3.html","id":"linear-regression","chapter":"3 Data Exploration","heading":"3.7.2.2 Linear Regression","text":"’ll go regression detail several later chapters. Functionally, output ANOVA.Table 3.9: Summary statisticsYou can use coefficients explain impact specific variables. many tickets month July worth? coefficient (5264.39) represents impact. July gives five-thousand ticket bump. can also use model make predictions. model completely overfit, doesn’t matter point. ’ll look ways make sure don’t overfit models subsequent chapters. Let’s put predictions graph.\nFigure 3.13: Prediction vs. acutal values\ncan see model pretty good job approximating sales. ’ll go complete examples later.","code":"\n#-----------------------------------------------------------------\n# Creating a simple linear model\n#-----------------------------------------------------------------\nln_mod <- lm(ticketSales ~ team+dayOfWeek+month+\n                           daysSinceLastGame+openingDay+promotion,\n                           data = FOSBAAS::season_data)\n\nln_mod \n#> \n#> Call:\n#> lm(formula = ticketSales ~ team + dayOfWeek + month + daysSinceLastGame + \n#>     openingDay + promotion, data = FOSBAAS::season_data)\n#> \n#> Coefficients:\n#>       (Intercept)            teamATL            teamBAL  \n#>          37993.73            -579.17            -612.86  \n#>           teamBOS            teamCHC            teamCIN  \n#>          10479.61            8873.01            -196.89  \n#>           teamCLE            teamCOL            teamCWS  \n#>          -1047.45            -488.43            1914.73  \n#>           teamDET            teamFLA            teamHOU  \n#>           -445.81           -1755.96            5301.38  \n#>           teamKAN            teamLAA            teamLAD  \n#>          -1729.86            4876.19           11798.00  \n#>           teamMIL            teamMIN            teamNYM  \n#>          -3877.18             330.59            4384.53  \n#>           teamNYY            teamOAK            teamPHI  \n#>           7362.71            1123.20            3590.36  \n#>            teamSD             teamSF             teamTB  \n#>          -1422.63            3995.62           -1543.36  \n#>           teamTEX            teamWAS       dayOfWeekMon  \n#>          -2309.79           -1107.37           -8998.56  \n#>      dayOfWeekSat       dayOfWeekSun       dayOfWeekThu  \n#>           -247.95           -8519.78           -8806.70  \n#>      dayOfWeekTue       dayOfWeekWed           monthAug  \n#>          -8724.49           -8947.95             -79.88  \n#>          monthJul           monthJun           monthMar  \n#>           5264.39            2986.79           -1522.76  \n#>          monthMay           monthOct           monthSep  \n#>            184.91           -1699.57           -1255.60  \n#> daysSinceLastGame     openingDayTRUE   promotionconcert  \n#>            279.20            3986.05            -929.53  \n#>     promotionnone     promotionother  \n#>          -4598.88           -2877.06\n#-----------------------------------------------------------------\n# Using the regression output\n#-----------------------------------------------------------------\nseasons <- FOSBAAS::season_data\nseasons$pred <- predict(ln_mod)\n\nx_label <- 'Actual Sales'                                             \ny_label <- 'Predicted Sales'                                            \ntitle   <- 'Actual Sales vs. Predictions'\nlegend   <- ('Season')\nsales_mod <- \nggplot(seasons, aes(x = ticketSales, \n                    y = pred,\n                    color = factor(season)))                  + \n    geom_point()                                              +\n   stat_smooth(method = 'lm', se = T)                      +\n    scale_color_manual(legend,values = palette)               +\n    scale_x_continuous(label = scales::comma)                 +\n    scale_y_continuous(label = scales::comma)                 +\n    xlab(x_label)                                             + \n    ylab(y_label)                                             + \n    ggtitle(title)                                            +\n    graphics_theme_1 "},{"path":"chapter3.html","id":"key-concepts-and-chapter-summary-2","chapter":"3 Data Exploration","heading":"3.8 Key concepts and chapter summary","text":"Exploring data fundamental component analysis. looking useful patterns might make sophisticated analysis possible. also looking things like missing data sparsity. Graphs typically easiest way accomplish goal. also best method communicating results.covered several key concepts chapter:important create consistent design language graphs. Pay attention colors.Certain graphs better representing specific data. often don’t need exotic plots convey information. ’ll use variations different themes.Summarizing data can done several ways. multiple tools can use reshape summarize data. can even implement methods relatively easily.R makes easy explore deploy wide variety statistical methods.interpreting communicating findings simplest explanation usually best. people care something. Let ask context.Think chapter reference. just went basic Business Intelligence analytics tools use data exploration. can also use Business Intelligence tools like Tableau add additional dimensions illustrations. want build custom websites R provides tools accomplish well. even use javascript python accomplish thing.","code":""},{"path":"chapter4.html","id":"chapter4","chapter":"4 Structuring Projects","heading":"4 Structuring Projects","text":"Framing projects overlooked component analytics exercises. projects often begin medias res. cobble data together start throwing models get result. Create basic wireframe project begin. ’ll help keep scope check force project-manage work. can loosely break analytics projects six parts, may sub-components:Defining measurable goal hypothesisData collection managementModeling dataEvaluating resultsCommunicating resultsDeploying resultsThis can often iterative process. numerous questions answer tools need, whether internal resources, return investment, long take, whether anybody use , whether done , , results, etc. process also helpful just beginning work. seasoned analyst good idea start project experience.basic premise always working one projects. create objectives based environmental considerations, build programs implement objectives, monitor performance. easy, process tends overlooked. Larger projects can smother die weight considerations (including political considerations) aren’t considered. good plan can help keep breathing.","code":""},{"path":"chapter4.html","id":"defining-goals","chapter":"4 Structuring Projects","heading":"4.1 Defining goals","text":"Defining goals can take time effort. ’ll likely need translate product operational tactic can understood leveraged functional unit business. example, Sales team may say want sell tickets need leads. marketing team may say want understand return marketing investment. translate statements practical solution? request contains several questions.Several management techniques look root causes. One aptly named “5 Whys” (Serrat 2017). subjects outside scope book. understand something devise plan. sounds obvious, consider critical component defining goal part discovery process. Let’s explore example:ticket sales manager says “need leads sell season tickets.” person asking? one several things, things may evolved person incentivized. always interesting think incentives guide behavior (see Freakonomics: rogue economist explores hidden side everything (Levitt 2005). sales manager saying?want make phone calls. number calls causes us sell tickets.current leads closing rate need hit goals.need leads keep guys working.put thoughts context motivates sales manager, simple statement insidious:ideas, going make guys work harder.want execute job. give want.shouldn’t viewed negative lens. jobs executing, can get tedious. Look task way escape monotony help sales team think strategically. Help escape mire.Let’s assume sales manager believes number calls directly correlates number tickets sold. think , something can investigate? simplistic method explore conclusion might see number phone calls correlates sales particular way. use formal informal methods, easiest thing put data tables graphs.","code":""},{"path":"chapter4.html","id":"identifying-goals","chapter":"4 Structuring Projects","heading":"4.1.1 Identifying goals","text":"data set FOSBAAS::aggregated_crm_data three fields, row represents interactions one customer. See ?FOSBAAS::aggregated_crm_data:repID corresponding specific repcalls represent number calls rep made specific personrevenue represents amount revenue generated specific customer.Table 4.1: Calls revenue repWhy salespeople effective? Perhaps someone called bought six expensive seats, skews results. call numbers correlated revenue? cor function give us correlation coefficient.Overall, calls weakly correlated revenue (higher number better). ?\nFigure 4.1: Boxplot revenue rep\ncan see boxplots figure 4.1 average sales customer reasonably equivalent. However, reps far fewer outliers. relatively small number sales drives differences see reps. can get summary stats representative using psych::describe.(ag_sales_data\\$revenue,ag_sales_data\\$repID).\nFigure 4.2: Cumulative revenue rep\ncan verify without looking actual stats looking jumps line graphs. also look statistics see anything else can understand. Let’s answer couple questions:specific reps efficient phone calls?specific reps efficient customers?\nFigure 4.3: Call failures\nOverall, rep stands less efficient, although differences top bottom. let’s take closer look distribution sales revenue. Instead density plot, ’ll use summary statistics compare highest lowest performers.Table 4.2: Aggregated sales data quantilesAt least 50% customers interacted resulted revenue. top 1% sales resulted $10,000.00 revenue, 25% customers spent less $2,924.00. Let’s isolate top bottom performers using describe.function.average sale top rep 50% higher lowest performer’s. Additionally, median sale greater zero, means success often. let’s look much generated reps upper end spectrum.top performer generated nine times much revenue top 1% spenders. couple tendency successful, answer reps better. now answered one . established?number phone calls per rep weakly correlates revenue.reps marginally effective others terms efficiency.reps sell many high-end deals others.else look ?experienced reps better closing sales?origin sales? sales called-?Perhaps females tend better luck given demographic distribution purchasers.seasonality impact figures? Perhaps reps began working different times?professional resellers hidden sales?number calls rep makes weakly correlated sales, goal shouldn’t reps make phone calls. also shouldn’t add sales reps unless sales rep’s experience another factor impacts sales. two tactics common solutions hear; however, technology rapidly changing mindset. example, artificial intelligence automates lead-warming, sales might -sells gauging establishing interest.point section establish goals, need objective justification goals . Determine driving desired outcomes settling tactic. sounds obvious, isn’t. reasons political. Look might incentivizing behavior.","code":"\n#-----------------------------------------------------------------\n# Aggregated CRM data\n#-----------------------------------------------------------------\nag_sales_data <- FOSBAAS::aggregated_crm_data\n\nlibrary(dplyr)\nagg_calls <- \n  ag_sales_data                     %>% \n  group_by(repID)                   %>%\n  summarise(calls   = sum(call),\n            revenue = sum(revenue)) %>%\n  mutate(revByCall = revenue/calls) %>%\n  arrange(desc(revByCall))\n#-----------------------------------------------------------------\n# Correlation coefficient\n#-----------------------------------------------------------------\ncor(agg_calls$calls,agg_calls$revenue)\n#> [1] 0.2589937\n#-----------------------------------------------------------------\n# Box plots of revenue by sales rep\n#-----------------------------------------------------------------\nx_label <- 'Rep ID'                                             \ny_label <- 'Revenue by sale'                                            \ntitle   <- 'Revenue by sale by rep'\nsales_box <- \nggplot(ag_sales_data, aes(y=revenue, \n                          x=factor(repID)))                   +\n    geom_boxplot(fill = palette[1])                           +\n    scale_color_manual(values = palette)                      +\n    scale_y_continuous(label = scales::dollar)                +\n    xlab(x_label)                                             + \n    ylab(y_label)                                             + \n    ggtitle(title)                                            +\n    graphics_theme_1                                          +\n    theme(axis.text.x  = element_text(angle = 90, size = 8, \n                                      vjust = 0, \n                                      color = \"grey10\"))\n#-----------------------------------------------------------------\n# Cumulative revenue by rep by customer\n#-----------------------------------------------------------------\nag_sales_line <- \nag_sales_data %>% group_by(repID) %>%\n                  mutate(cumSum = cumsum(revenue),\n                         observation = seq(1:500))\n\nx_label <- 'Customer'                                             \ny_label <- 'Revenue'                                            \ntitle   <- 'Revenue generated per rep by customer'\n\nsales_line <- \nggplot(ag_sales_line, aes(y     = cumSum, \n                          x     = factor(observation),\n                          group = repID,\n                          color = repID))                     +\n    geom_line()                                               +\n    scale_color_manual(values = palette,guide = FALSE)        +\n    scale_y_continuous(label = scales::dollar)                +\n    xlab(x_label)                                             + \n    ylab(y_label)                                             + \n    ggtitle(title)                                            +\n    graphics_theme_1                                          +\n    theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.ticks.x=element_blank(),\n        panel.grid.major  = element_line(colour = \"white\"),  \n        panel.grid.minor  = element_line(colour = \"grey80\"))\n#-----------------------------------------------------------------\n# Failed calls and customers by rep\n#-----------------------------------------------------------------\nfailures <- \n  ag_sales_data                      %>% \n  group_by(repID)                    %>%\n  filter(revenue == 0)               %>%\n  summarise(failedCalls = sum(call),\n            failedCusts = n())       %>%\n  tidyr::pivot_longer(!repID, \n                      names_to  = \"failure\", \n                      values_to = \"value\")\nx_label  <- ('\\n Rep')\ny_label  <- ('Count \\n')\ntitle    <- ('Failures by rep')\nbar_sales <- \n  ggplot2::ggplot(data     = failures, \n                  aes(y    = value, \n                      x    = reorder(repID,value,sum),\n                      fill = failure))                  +\n  geom_bar(stat = 'identity', position = 'dodge')       +\n  scale_fill_manual(values = palette, name = 'failure') +\n  scale_y_continuous(label = scales::comma)             +\n  xlab(x_label)                                         + \n  ylab(y_label)                                         + \n  ggtitle(title)                                        +\n  coord_flip()                                          +\n  graphics_theme_1                                      + \n  theme(legend.position   = \"bottom\")\n#-----------------------------------------------------------------\n# quantiles of sales\n#-----------------------------------------------------------------\nquants <- quantile(ag_sales_data$revenue,\n                   probs = c(.5,.75,.9,.95,.975,.99,1))\n#-----------------------------------------------------------------\n# description of sales\n#-----------------------------------------------------------------\nids <- c(\"0LK62LATB8E3\",\"AFA0Z9M2M4LQ\")\ndescriptives <- \npsych::describe.by(\n  ag_sales_data[which(ag_sales_data$repID %in% ids),]$revenue,\n  ag_sales_data[which(ag_sales_data$repID %in% ids),]$repID)\n#-----------------------------------------------------------------\n# description of sales\n#-----------------------------------------------------------------\ndescriptives$`0LK62LATB8E3`[c(3,4,5,6,9)]\n#>      mean      sd median trimmed      max\n#> X1 1400.9 2966.49      0 1036.41 54039.92\n#-----------------------------------------------------------------\n# description of sales\n#-----------------------------------------------------------------\ndescriptives$AFA0Z9M2M4LQ[c(3,4,5,6,9)]\n#>       mean     sd median trimmed      max\n#> X1 2404.23 6747.1 963.96 1417.14 58428.16\n#-----------------------------------------------------------------\n# quantiles of sales\n#-----------------------------------------------------------------\n\nag_sales_data                                          %>% \n  filter(repID %in% c('0LK62LATB8E3','AFA0Z9M2M4LQ'),\n         revenue >= quants[6])                         %>%\n  group_by(repID)                                      %>%\n  summarise(highRevenue  = sum(revenue),\n            countRevenue = n())\n#> # A tibble: 2 × 3\n#>   repID        highRevenue countRevenue\n#>   <chr>              <dbl>        <int>\n#> 1 0LK62LATB8E3      54040.            1\n#> 2 AFA0Z9M2M4LQ     453204.            9"},{"path":"chapter4.html","id":"collecting-data","chapter":"4 Structuring Projects","heading":"4.2 Collecting data","text":"lot understand data collection, always forced confront question data need acquire . data much easier obtain. instance, transactional data likely abound. access several years ticketing CRM data. However, may problems formatting consistency. place data collection headings:Transaction data ticketing, CRM, internal systemsTransaction data external systems (perhaps agency agreement something like concessions)Third-party data. might include data vendors Acxiom.Public data. includes sources Census data information legal records.Internal research data. category contains surveys initiatives competitive intelligence.also excellent time discuss much data may lack. professional sports team isn’t Google, Facebook, Amazon. companies can build analytics capabilities around data far sophisticated way possible along vertical. partner companies another third party like take advantage capabilities.Furthermore, tech companies walled-gardens. don’t like share. ’ll contend fact. Think critically use capabilities. spend money SEM32, ’ll want ask . market work? multiple channels consumers can exploit purchase tickets? easy skew attribution. knew type “Game Hens tickets” search bar, easy attribute sale activity. must ask attribute sale search-engine spend.Internal sources survey results, transaction data, interaction data phone calls, loyalty programs offer degree reliability. However, depending geography, legal environment may allow leverage data. numerous restrictions leveraging, storing, sharing data, :California Consumer Privacy Act 33The Illinois biometric information privacy Act 34The Can-Spam Act 35The -Call list 36As long live United States, laws impact can leverage first third-party data. list even comprehensive. GDRP37 (European privacy law) far-reaching influence can United States. future commerce undoubtedly make arena challenging navigate use biometrics becomes prolific.Additionally, data often needs accurate. third party data-brokers get data? Sometimes use bank names determine person male female, African American vs. Asian: Washington = African American, Wang = Asian, Lopez = LatinX. LatinX represents particular problem culture notion race. Third-party brokers (Axiom) source data multiple sources model components . Working third-party data can difficult frustrating unless deal massive datasets (uncommon sports).Internal research data may required specific research instruments :IDI (-depth interviews)Satisfaction surveysConjoint studiesFocus groupsBrand-tracking studiesSocial listeningThese sources can become stale rather quickly. can also expensive often require specialized software skill sets.Operational data may ready-made method data collection. example, let’s say Executive wants understand relationship line length concession stands relative attendance. get ? cameras installed .. tracks line length? ? ’ll collect manually, leveraging rubric can easily explained executed front-line staff. might data collection plan look like? determining collect data, thinking problem inside helpful. ’ll discuss concept depth chapter 9.main gist need focus couple things:questions trying answer?data need answer questions?process can sprawl concerned consistency. Public sources data also abound offer granularity needed useful directional reporting. example census data. extensive APIs access data, results may helpful long-term planning exercises.Data collection might also involve competitive intelligence. example visiting venues monitoring prices operational tactics. Competitive intelligence may also take form watching industries operate similar spaces. instance, can learn loyalty programs company like Starbucks? might discover aren’t appropriate .touched high-level concepts . Just understand data collection require combining .T. skill sets, research knowledge, critical thinking exercises. Getting data right spot take time.","code":""},{"path":"chapter4.html","id":"modeling-the-data","chapter":"4 Structuring Projects","heading":"4.3 Modeling the data","text":"Modeling data fun part working analytics. aren’t discussing modeling data database sense, although structuring data process component. also aren’t going actually modeling data . Instead, speak tools need. leverage least two languages (SQL programming language) lower levels. Increasingly modeling component data becoming straightforward part tech giants seek space. don’t know significant advantages using one tool . instance, advantage using Google’s tools Microsoft? advantage using Python R? answers may vary. cases, yes, cases, . time, ’ll see reliance code wane.Differences R Python may distinguish tool plan using. especially true look deploy results. R Python highly extensible. libraries almost everything, need write implementations likely minimal. need, using another language, C++, better. However, C++ significantly complicated many languages. (Practically speaking, R C since written )Many analytics projects Python use relatively small sets libraries. nice feature code one model look exactly like code another. sometimes true R, advantage Python users. Let’s look example.code hierarchically cluster data set Python might look something like following:Every method module sklearn (Pedregosa et al. 2011) resemble example. makes easy intuitive run several algorithms. instance, demonstration, wanted cluster Kmeans algorithm Python, code might look like .almost identical. Let’s compare code R code works tool. code hierarchically cluster data R follows.languages begin importing capabilities libraries. following code chunk uses kmeans algorithm. housed stats library works slightly differently hierarchical algorithm. R tends feel procedural Python.code chunk looks similar, sometimes case. R, like Python, thousands packages. However, Python, tends one correct way everything. ’s Pythonic way. hand, R wild west. Different algorithms often authors approach writing code dealing objects differently. Luckily, several developers R attempted solve problem wrapper packages work API libraries add additional functionality.can even run Python R using reticulate (Ushey, Allaire, Tang 2022) package. Packages caret (Kuhn 2022), mlr3 (Lang et al. 2022), tidymodels (Kuhn Wickham 2022) attempted create standard API many R functions. similar approach can taken regarding writing understanding code many functions. tradeoff often speed. Despite tradeoff, taking advantage one frameworks beneficial. make much easier cover steps modeling process. R Python learned one another. Pick tool get good . prefer R, admire Python. place, easy switch data analysis.modeling process follows four discrete (often iterative) steps. ’ll refer parts process chapter 5.Evaluating dataPreparing dataProcessing dataValidating outputYou’ll get better process gain experience, become intuitive. projects tend repetitive, can encounter several problems modeling data. Every problem doesn’t always manifest, issues always arise. ’ll see solve problems forthcoming chapters.","code":"#-----------------------------------------------------------------\n# Clustering algorithm applied in Python\n#-----------------------------------------------------------------\nfrom sklearn.cluster import AgglomerativeClustering\ndata = data.sample(n=1500)\ncluster = AgglomerativeClustering(n_clusters=6, \naffinity='euclidean', linkage='ward')  \ncl = py.DataFrame(cluster.fit_predict(data))#-----------------------------------------------------------------\n# kmeans algorithm applied in Python\n#-----------------------------------------------------------------\nfrom sklearn.cluster import KMeans\ndata = data.sample(n=1500)\ncluster = KMeans(n_clusters=6, random_state=0)\ncl = py.DataFrame(cluster.fit_predict(data))\n#-----------------------------------------------------------------\n# Hierarchical clustering algorithm applied in R\n#-----------------------------------------------------------------\nlibrary(stats)\nlibrary(cluster)\ndata         <- sample(data, 1500)\nmod_data     <- cluster::daisy(data)\ncl           <- stats::hclust(mod_data, method = \"ward.D\")\ncuts         <- cutree(cl, k = 6)\ndata$cluster <- cuts\n#-----------------------------------------------------------------\n# kmeans algorithm applied in R\n#-----------------------------------------------------------------\nlibrary(stats)\ndata         <- sample(data, 1500)\ncl           <- stats::kmeans(data, centers = 6)\ndata$cluster <- cl$cluster"},{"path":"chapter4.html","id":"evaldata","chapter":"4 Structuring Projects","heading":"4.3.1 Evaluating your data","text":"refers understanding looking . example, data structured, trying ? can occasionally one several built-features R. times, ’ll put thinking cap. Typical questions might include:questions trying answer data?attempted solve problem ? results?deeply held beliefs solution might ?data formatted?sparse data?current data? matter stale?data’s provenance? Can trust ?much missing, missing?Can use data?need data?data categorical, ordinal, numerical, mixed?dealing differences units scale?plan don’t find anything, likely possibility?Taking time get familiar data structure critical getting accurate results. , focus question hand. example current sales best predict future sales times year? might work better? Frame problem statement carefully.Furthermore, deeply held beliefs answer, may issues answer antithetical belief. reasons belief held. time people formed belief correct. Just use extra discretion determine another solution merit.","code":""},{"path":"chapter4.html","id":"preparing-your-data","chapter":"4 Structuring Projects","heading":"4.3.2 Preparing your data","text":"Preparing data takes time. ’ll go process detail chapter 5. missing data, consider imputation. dealing mixed data sets, must consider process . Ordinal data might require approach problem specific way. example might Holt-Winters 38 method estimating time-series data. particular method may also applicable underlying structure within data. preparing data, also critical consider two common questions face.deal missing data?methods plan applying data?Missingness huge problem. NAs, NANs, inf? sparse OK? answers may vary. Additionally, systematic reason data missing, results may invalid. Take warning; dealing missing data frustrating work.prepare data, always ask something repeated. recommend prepping data code. reason, always need repeat projects. Documenting ETL work crucial. start SQL push point doesn’t make sense. don’t duplicate information SQL. instance, won’t dummy-code data set SQL. leave analysis phase. judgment call. think can efficiently. Specific methods Latent Class Regression accept discrete data, transform numerical data discrete data can impact results.","code":""},{"path":"chapter4.html","id":"modelingdata","chapter":"4 Structuring Projects","heading":"4.3.2.1 Selecting the appropriate technique for analyzing your data","text":"Gone days expert use sound judgment select appropriate method processing data. Hardware software advances removed resource constraint delayed analytics revolution began early 2000s. future now. past, researcher may purchase time main-frame39 (may even know term means) therefore needed good idea technique want leverage solve problem. Analysts modern times can afford lazy. Modern analysts considerations regarding approach problem. See figure 4.4.\nFigure 4.4: Choosing right technique\nexaggerating little . Many techniques require high rigor validate results. However, much easier analysts twenty years ago. surplus methods become problem . Additionally, easy get rut. instance, like deep learning use every problem face. want use hammer, use fix everything. ’s variation something already discussed, might warrant repeated. Sometimes wrench make life easier even hammer work.","code":""},{"path":"chapter4.html","id":"processing-your-data","chapter":"4 Structuring Projects","heading":"4.3.3 Processing your data","text":"Begin looking simplest solution first. Stingy, parsimonious models tend forgiving friendly. Always look simplest solution. especially important interpretability also durability. straightforward solutions elegant easier deal . Always approach processing intelligent, procedural way. Let’s illustrate brief example:“ticket sales manager wants understand likely candidates system purchase season tickets. also want understand likely spend upgrade seats.”approach problem gathered cleaned data? classification problem related lead scoring. OLS regression tends best place start gold standard estimating numerical values, Logistic regression tends best place begin trying estimate classes. even particular forms, “multinominal logistic regression” (Ripley 2022), can estimate several classes, just binary classes.time ’ll learn specific techniques work better particular problems. Techniques may also provide almost identical results. example, ’ve found random forests gradient boosting deliver similar results. less interpretable, don’t require much rigor regression. Regression forces consider many issues, Heteroskedacity, Multicolinearity, Autocorrelation. always like use multiple techniques compare results. Getting support conclusion always good thing. Unless dealing vast amounts data, little penalty taking route. tools make easy.Keep mind results deployed. Regression represented mathematical formula. means results can computed almost instantly, model won’t change. Deep learning algorithms can adaptive produce unexpected results monitored. Many examples chat-bots developing racist 40 behaviors. Stick straightforward solution unless need higher degrees precision deployment requires specific approach. Don’t fall rut “problem; let’s bludgeon Deep Learning.”","code":""},{"path":"chapter4.html","id":"evaluating-your-results","chapter":"4 Structuring Projects","heading":"4.4 Evaluating your results","text":"Evaluating results can best worst part project. discover solution? Regardless whether solved problem, stage process give great information. don’t find solution may tell isn’t solution data. may also indicate problem nuanced needs sophisticated thought. Let’s examine example.marketing manager wants understand outreach efforts impact season ticket retention.collect data CRM system, number calls emails, attendance regarding non-sport events, etc. use data attempt estimate probability season ticket holders renew tickets. Unfortunately, outreach appears irrelevant (results insignificant model). also discover ticket usage tenure variables appear significant (ignoring macro factors economy team performance). mean shouldn’t call email clients? course , mean may need examine incentive client representatives closely. also need careful communicating type result. ’ll discuss next section.also technical component evaluation. Different models can compared one another ANOVA 41 BIC 42, AIC 43. example, estimate efficacy call campaign lift chart represented ROC 44 curve. OLS regression uses F statistic, P-values, R-squared values. Results can cross-validated compared control group holdout sample. Logistic regression models concerned Accuracy, precision, sensitivity, specificity, overdispersion. Many diagnostic features must evaluated ensure model want. Ultimately, ’ll always want compare results control group make iterative improvements. /B testing often best way accomplish goal something put production.Evaluating results exercise requires technical rigor domain knowledge. don’t find solution always learn something. Think carefully approach component modeling data. second time-consuming component project, significant influence whether project succeed fail.","code":""},{"path":"chapter4.html","id":"communicating-the-results","chapter":"4 Structuring Projects","heading":"4.5 Communicating the results","text":"Getting research hands Executives Managers need understand problem potential solution complicated sounds. Communicating results project may require esoteric techniques art. Every large management consulting firm frameworks around problem-solving framing results. can often borrow folks. Remember results can complex involve difficult concepts, isn’t always easy ELI5 45. credibility grows organization matures, process becomes manageable. Exorcising bias may impossible colleagues suffer confirmation bias. incredibly important understand.“often use reasoning find truth invent arguments support deep intuitive beliefs.”— Johnathan Haidt, “Happiness Hypothesis”quote Haidt (Haidt 2006) hammers home concept confirmation bias. results, case, may may winning argument. can frustrating phenomenon. hand, also merit. much experience problem? strong beliefs solution? Understanding someone might interpret solution (especially solution challenges preconceived notions) must considered. solution threaten someone, consider present . Even think words use recipients message perceive . someone perceive results threatening insulting? happens gas-lighted? Always take step back examine analysis fair. made difference another way? something doesn’t matter, don’t worry communicating .Additionally, best practices can follow. example, higher someone’s title , words bullet points include. lower title technical audience , graphs explanations include:Executives: Bullet points refer solution. Keep terse. Get point.Managers: Include specific explanations domain knowledge.Resist urge demonstrate domain knowledge. Typically, nobody cares something. don’t care Support Vectors, Eigenvalues, Deep Learning. Instead, care results , cases, got (especially contradicts beliefs). Instead, take collaborative approach demonstrate findings ask questions. ’ll gain credibility long term better success implementing solutions.R also provides great features helping communicate results findings. R Markdown (Allaire et al. 2020) makes creating publishable documents include code, graphics, mathematical formulas easy. book written using . discussed Shiny (Chang et al. 2022) earlier. tools make R excellent choice building documents dynamic web pages.","code":""},{"path":"chapter4.html","id":"deploying-the-results","chapter":"4 Structuring Projects","heading":"4.6 Deploying the results","text":"Deployment mean different things. One refers people, one relates systems.Communicating results (subject last section)Operationalizing output (automation component discussed figure 1.1.second part deploying results refers automating process putting solutions production. Multiple tools make possible. Although languages advantage R. Compiled languages C++ can much faster R full-featured sense, exceptions. SQL Server supported integration R 46 Python since 2016, although isn’t clear widespread use become. Google, Amazon, Microsoft building analytics frameworks cloud-based DBMSs. allow turn computing power make models run much quickly, assuming can take advantage technique multi-threading (R natively support). Thinking make something efficient probably already done . stage, R may used prototyping something else actual deployment.valuable?Forecasts ticket sales made daily minute update report.Customer lead scores dynamically updated throughout year based certain conditions.marketing campaign’s predicted vs. actual vs. expected results constantly evaluated ROI.Ticket Prices automatically updated tight interval optimize sales revenue.many applications. Additionally, always write wrapper (windows) within system runs script using simple batch script (notepad file saved .bat). following example runs R script called YourRScript.R.can write similar bash script Linux box. However, dealing complicated systems, may need developers automate process. , context available skill sets tell direction take.","code":"\n#-----------------------------------------------------------------\n# Batch script for automation\n#-----------------------------------------------------------------\n# REM This command will call an R Script from a program or \n# scheduling tool\n\n# \"C:\\Program Files\\R\\R-Version\\bin\\x64\\R.exe\" CMD BATCH \n#  --vanilla --slave \"\"C:\\locationOfScript\\YourRScript.R\""},{"path":"chapter4.html","id":"key-concepts-and-chapter-summary-3","chapter":"4 Structuring Projects","heading":"4.7 Key concepts and chapter summary","text":"structure problem critical component project management process. becomes important projects grow complexity require significant numbers people managed. project can benefit systematic approach consisting steps. typically break steps six components:Defining measurable goal hypothesisData collection managementModeling dataEvaluating resultsCommunicating resultsDeploying resultsYou’ll work steps whether mean . natural order projects take.Defining goal tends challenging part analytics project. also important. Even aren’t putting together formal hypothesis test, must still concerned scope creep. Please spend time step get right.Data collection management typically time-consuming component one projects. Getting quality data can take time effort. collect data , systematic . Document reproducibility.Modeling data fun part analytics project. lots options start simplest method first. deep learning prominent recently, isn’t always best tool. Interpretability can important . Put thought using one technique another.Evaluating results quantitative qualitative exercise. never just data. Think output context exercise give good smell test. results logical? Can take action ? relevant goal?“Simplicity ultimate sophistication.” quote attributed Leonardo Da Vinci. God knows came . Keep mind. Use bullet points. Keep results simple understand.Deploying results can mean multiple things. lots tools available put results action.","code":""},{"path":"chapter5.html","id":"chapter5","chapter":"5 Segmentation","heading":"5 Segmentation","text":"Appropriately segmenting data takes lot work. challenging complicated. finding patterns business data usually trivial, patterns may helpful. Often patterns may represent market structure rather exploitable feature. Additionally, improbable access rich data massive data platforms Facebook, Google, even league . means limited respects. Without partnerships, can’t even approach concept developing insight data way many companies. ’ll cover ideas chapter, two parts:Preparing data analysisAnalyzing interpreting dataThis spot book shifts tacit examples. walk fairly complete examples include code necessary, explanations output, interpretation deployment. ’ll cover techniques chapter based simulated data chapter 2. several things consider starting segmentation project. First, ’ll want ask questions:trying accomplish?output consumed?consume ?incremental value project?Number four vital limited time. another area focusing? Segmentation may leverage many different machine-learning techniques. chapter cover techniques found useful. Keep mind aren’t guaranteed find something useful. ? reasons:data badYou don’t enough dataYour technique inappropriateYou framed problem incorrectlyThere isn’t anything thereIf working consultant, find something. find something. working club, keep following statement mind:worst thing can happen data-science project think found something deploy solution isn’t valid.may actively harming organization also reducing credibility analytics. ’s OK can’t find something. Scale can also problem. much think incremental value work worth? models built around consumer behavior going maximize ticket revenue potential?chapter cover building segmentation scheme raw data. projects can become complex. ’ll also look different ways accomplishing goal. Perhaps need simple way differentiate two groups. may need full-scale psychographic profiles. ’ll cover basics products delivered. ’ll understand root segmentation scheme produced practice. cover portion variety, ’ll able identify consultant put together one frameworks build .","code":""},{"path":"chapter5.html","id":"building-a-segmentation-scheme","chapter":"5 Segmentation","heading":"5.1 Building a segmentation scheme","text":"example based demographic qualitative data survey. goal leverage simple scheme reasonable job explaining purchases tickets purchase . groups based demographics (discoverability), looking exploitable differences behavior. project successful, need follow four rules:Understand difference segments market structure. Groups differ behavior may segments.must able distinguish segments appeal selectively products channels.Validating segments require constant experimentation. willing capable experimenting data.Sales Marketing leadership must support remain involved segmentation.Sometimes segmentation schemes can diffuse. instance, people--necks helpful segment. covers large swath humans. segmentation schemes can irrelevant problem. sell dog food, women red hair may relevant segment. can also diffuse irrelevant. example might people brown eyes.basic goal find groups fans potential customers differ observable ways associated communicate, content preferences, marketing response. Generally, looking answer question: consumer, want?. understanding differences, hope make better strategic marketing decisions regarding pricing, market positioning,\nsales sponsorship opportunities, brand development. analysis two components:demographic segmentation derived data third party.set behavioral archetypes derived survey responses.demographic segments overlaid customer’s behavioral archetypes paint picture demographic broadly behaves. segments , specific . Abstracting fewer segments means edges segments tend fuzzy. Additionally, durability segments tested ensure validity.segmentation project successful can increase probability making correct decisions. isn’t magic bullet answer questions certainty. sports club, special considerations.don’t unlimited scale potentialYour sample sizes going relatively smallYou can’t continuously deploy tests way TicTok . projects ad hoc nature. useful marketers help understand market structure within fan base. allows minor tweaks help compound effectiveness multiple concurrent campaigns.Ultimately, success judged based creating testable hypotheses using information alter behavior one segments. also define success narrowly:create capability measure return marketing spend accurately?able increase sales revenue increasing conversion rates?able better determine product offerings?Can get relevant results surveys research projects?Regardless define success, segmentation always considered work progress. Segmentation remain static successful. Instead, must understand change behavior best suit customers’ expectations, channel capability, purchase behavior maximize opportunity.","code":""},{"path":"chapter5.html","id":"demographic-segmentation","chapter":"5 Segmentation","heading":"5.2 Demographic segmentation","text":"Demographic segmentation attractive many reasons. However, discoverability main reason. can useful certain circumstances, use also perilous. Steve Koonin (CEO Atlanta Hawks) famously stated Alpharetta Unicorn couldn’t save franchise (referring Atlanta Hawks). “Alpharetta Unicorn” :“55-year-old guy ’s going drive hour Alpharetta city three buddies go Hawks game. doesn’t exist. music, kiss cam, cheerleaders, shooting free car, bobbleheads … nothing going change .”— Steve KooninAlpharetta suburb Atlanta contrasts much Fulton County, GA, citizens (2021) tend less ethnically diverse (primarily white), right-leaning politically (mostly Republican) tend homogeneously affluent. outlying suburb, also geographically isolated.47 Steve’s response loaded several reasons. says Hawks brand incompatible marketing tactics product. may also taking shot another -market team. Either way, segment may may exist. may irrelevant specific situation, statement political stunt justify direction decided take sales marketing.Let’s begin taking look relatively simple data set. undoubtedly confronted much complex data career, context sports, data likely relatively straightforward simple. data sample FOSBAAS package entitled demographic_data.Let’s begin taking look column names.Table 5.1: structure demographic data setThis table 200,000 rows 12 columns corresponding standard demographic features. Latitude longitude already processed distance metric. ’ll pare data information think might helpful processing.","code":"\n#-----------------------------------------------------------------\n# Read data to use for segmentation\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\ndemo_data  <- FOSBAAS::demographic_data\n#-----------------------------------------------------------------\n# Take a look at the structure of the data\n#-----------------------------------------------------------------\nstr(demo_data)"},{"path":"chapter5.html","id":"exploring-the-demographic-data-set","chapter":"5 Segmentation","heading":"5.2.1 Exploring the demographic data set","text":"can sample variables think might helpful segmentation using select function dplyr.Looking first lines data, can see mixture discrete data, ethnicity, continuous data, age. relevant process data algorithm might use.(#tab:seg_dataset_str2)Demograhic datahhIncome modeled indicator. common run variable type purchased data sets. indexed value discrete beginning end. ’d need data dictionary explain numbers mean. case, larger number, higher income. Let’s start taking look numerical data sets. ’ll use graphics paradigm illustrated chapter 3.","code":"\n#-----------------------------------------------------------------\n# subset the data\n#-----------------------------------------------------------------\nlibrary(dplyr)\ndemo_data <- demo_data %>% select(custID,ethnicity,age,\n                                  maritalStatus,children,\n                                  hhIncome,distance,gender)"},{"path":"chapter5.html","id":"age-of-our-customers","chapter":"5 Segmentation","heading":"5.2.1.1 Age of our customers","text":"’ll begin exploratory data analysis. first, let’s build histogram categorical variable gender differentiate two groups.distribution bimodal48. describe data, know average age doesn’t make much sense. included gender see obvious differences men women sample. Let’s take closer look using describe function psych package(Revelle 2022).:Table 5.2: Males vs. FemalesWe can tell distribution skews older (mean 42.82 vs. median 46). Let’s also see significant differences women sample.Table 5.3: Descriptive statisticsAs can see, numbers almost identical. men women tend age.","code":"\n#-----------------------------------------------------------------\n# Histogram of the age of fans\n#-----------------------------------------------------------------\nx_label  <- ('\\n Age')\ny_label  <- ('Count \\n')\ntitle    <- ('Distribution of age (demographic)')\nhist_age <- \n  ggplot2::ggplot(data=demo_data,aes(x=age,fill=gender))  +\n  geom_histogram(binwidth = 2)                            +\n  scale_fill_manual(values = palette)                     +\n  geom_rug(color = 'coral')                               +\n  scale_x_continuous(label = scales::comma)               +\n  scale_y_continuous(label = scales::comma)               +\n  xlab(x_label)                                           + \n  ylab(y_label)                                           + \n  ggtitle(title)                                          +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Get summary statistics\n#-----------------------------------------------------------------\nlibrary(psych)\ndescript   <- psych::describe(demo_data$age)\ndescriptMF <- psych::describeBy(demo_data$age,demo_data$gender)"},{"path":"chapter5.html","id":"distance-from-facility","chapter":"5 Segmentation","heading":"5.2.1.2 Distance from facility","text":"Distance another interesting numerical data set. , distance significant depending think product. example, likely purchase season ticket, someone five miles venue five hundred? answer depends, general rule, season ticket holders live closer venues.distribution also bimodal. ? sample may season ticket holders percentage group. another major city around two-hundred miles away? Patterns like show looking ticket sales. simply accessible people nearby purchase utilize tickets. never imagine ballpark stadium rural area. Distance significant variable models. careful treat . measuring distance purchasers? ticket? differentiating fans companies?Looking data, can see skew numbers. example, median distance 66.47 miles, mean distance 110 miles. datasets can problematic regression may work favor terms identifying market structure. can also helpful thinking different marketing tactics OOH49.Table 5.4: Descriptive statistics distanceThe distribution distance much different age. median significantly smaller mean, indicates left skew distribution.","code":"\n#-----------------------------------------------------------------\n# Distance from our facility\n#-----------------------------------------------------------------\nlibrary(ggplot2)\nx_label   <- ('\\n Distance')\ny_label   <- ('Count \\n')\ntitle     <- ('Distribution of distance (demographic)')\nhist_dist <- \n  ggplot2::ggplot(data=demo_data,aes(x=distance))            +\n  geom_histogram(binwidth = 2,fill='dodgerblue')             +\n  geom_rug(color = 'coral')                                  +\n  scale_x_continuous(label = scales::comma)                  +\n  scale_y_continuous(label = scales::comma)                  +\n  xlab(x_label)                                              + \n  ylab(y_label)                                              + \n  ggtitle(title)                                             +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Use the psych package to generate summary statistics\n#-----------------------------------------------------------------\ndescript <- psych::describe(demo_data$distance)"},{"path":"chapter5.html","id":"household-income","chapter":"5 Segmentation","heading":"5.2.1.3 Household income","text":"Household income another continuous data set. also modeled, means likely scaled fashion. distribution multimodal, several peaks.analysis essential. Ultimately, seek behavioral differences attribute different features related discoverable group individuals. ’ll touch begin process data quantitative analysis. don’t currently reason believe difference person behaves based income. However, can probably make guesses may may merit concerning data.Table 5.5: Descriptive statistics incomeThere clear clusters groups data. Think critically one. higher household income mean disposable income? cases, yes, cases, . implicit signs disposable income use? Perhaps luxury cars pool ownership. point, people may tend spend income. People make money tend buy nicer cars bigger houses. asset investments like houses may increase wealth, may indicate disposable income. ever visited someone’s fabulous home noticed cheap furniture ?","code":"\n#-----------------------------------------------------------------\n# Histogram of income\n#-----------------------------------------------------------------\nx_label  <- ('\\n Household Income')\ny_label  <- ('Count \\n')\ntitle    <- ('Distribution of Income (demographic)')\nhist_income <- \n  ggplot2::ggplot(data=demo_data,aes(x=hhIncome)) +\n  geom_histogram(binwidth = 2,fill='dodgerblue')  +\n  geom_rug(color = 'coral')                       +\n  scale_x_continuous(label = scales::comma)       +\n  scale_y_continuous(label = scales::comma)       +\n  xlab(x_label)                                   + \n  ylab(y_label)                                   + \n  ggtitle(title)                                  +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# summary statistics of hhincome\n#-----------------------------------------------------------------\ndescription <- psych::describe(demo_data$hhIncome)"},{"path":"chapter5.html","id":"consololidating-the-numerical-analysis","chapter":"5 Segmentation","heading":"5.2.1.4 Consololidating the numerical analysis","text":"can get numbers manipulation. case, aren’t looking anomalous data NAs missing data.Table 5.6: Consolidated statisticsAs best practice, don’t things multiple times. example, can think way like view data, somebody else probably thought solved problem.","code":"\n#-----------------------------------------------------------------\n# Generate summary stats of multiple values\n#-----------------------------------------------------------------\nnumerical_cols <- names(dplyr::select_if(demo_data, is.numeric))\ndescription    <- psych::describe(demo_data[numerical_cols])"},{"path":"chapter5.html","id":"categorical-variables","chapter":"5 Segmentation","heading":"5.2.1.5 Categorical variables","text":"myriad ways analyze categorical data. ’ll go methods describe seeing. First, can create proportion table prop.table function determine discrete groups’ size.case, white fans represent 80% sample, African Americans represent 10%, Hispanics Asians representing another 10%.Table 5.7: Ethnicity proportionsAnalyzing ethnicity fraught difficulty. mean? identify differentiate Hispanics? seems nonsensical. Folks can get wrapped-around--axle . Sponsors, media, internal HR staff, local governments, everyone gets concerned ethnicity means doesn’t mean terms attendance. loaded word. analyst can make worse. Let’s consider joke heard. don’t know attribute :algorithm walks bar, bartender says, “?” algorithm looks around says, “’ll everyone else .”likely find exactly expect see . , ethnicity matter? Perhaps doesn’t point estimate. Longitudinally can think applications. meaningful ticket buyers tend live nearby, identify particular ethnicity? mean five ten years area grows?also typically useful look crosstabs data. example, let’s compare African Americans white fans.Table 5.8: Ethnicity proportionsWhen comparing groups, numbers relatively close. can also make generalizations.Many fans tend white. congruent population patterns?Wealth factors seem similarWhites tend slightly younger live closer parkWe can leverage analysis see -indexing within populations (assuming merit). factors explain purchases? Now done cursory analysis data, can begin processing modeling. Never overlook analyzing factor data set individually. Understanding data structure extremely useful work rest analysis.","code":"\n#-----------------------------------------------------------------\n# build a proportion table for categorical data\n#-----------------------------------------------------------------\ndescription      <- table(demo_data$ethnicity)\ndescription_prop <- prop.table(description)\n#-----------------------------------------------------------------\n# use describeBy to generate statistics tabulated by a factor\n#-----------------------------------------------------------------\nnumerical_cols <- names(dplyr::select_if(demo_data, is.numeric))\ndescription    <- psych::describeBy(demo_data[numerical_cols], \n                                    demo_data$ethnicity)\ndeth <- rbind(description$aa[,c(2,3,4,5,6)], \n              description$w[,c(2,3,4,5,6)])"},{"path":"chapter5.html","id":"preparing-the-data-for-modeling","chapter":"5 Segmentation","heading":"5.2.2 Preparing the data for modeling","text":"Unfortunately, ’ll spend lot time prepping data can operate . ’ll discuss major problems can . several confronted frequently:Missing data (NA,NAN,Inf)Sparse dataWe’ll briefly touch imputation give thoughts ’s appropriate. Imputation enormous subject far outside scope book.","code":""},{"path":"chapter5.html","id":"dealing-with-missing-data","chapter":"5 Segmentation","heading":"5.2.3 Dealing with missing data","text":"Missing data almost always problem, different ways can handled. One consideration data missing. reasons data missing can bias results analysis. Bias frequent problem surveys. instance, survey long, people tend finish portion. data “missing completely random.” technical term, ways deal , don’t worry . Just aware issues.Imputing data (dealing missing data) can incredibly complex. First, need understand much data missing. Several packages deal missing data. can also write simple function help identify common culprits. Let’s take look data see missing anything.Table 5.9: Missing valuesNothing missing! isn’t surprising since created data set. Let’s introduce missing data explore vital subject. randomly assign cells NA based random number generated cell dataframe.function randomly makes 2% fields NA.Now can apply simple function determine missing values. successful.Table 5.10: New missing valuesNow need something missing values. small amount data missing, may OK delete rows. much small amount? don’t know. Let’s look row determine values missing. ’ll use simple loop count missing fields row column. different ways .four-thousand entries missing attribute. Since know overlap, may great idea delete rows. many rows missing data? can use complete cases function determine many rows represent complete cases.Table 5.11: Complete casesAbout thirty thousand rows missing data. enough build suitable model. know? hard fast rules. sampled 10,000 rows data, distributions field likely look complete data set. really looking . case reach arbitrarily small number (perhaps 400-700 people). people call law large numbers. just expression. law. However, might think data missing. data typically better, get penalized process time.Let’s try couple different techniques impute missing data. ethnicity, know vast majority (80%) known values “w.” Therefore, can replace NAs “w” assume close 80% accurate. easy accomplish following code.Age little trickier. know distribution multimodal. Taking common value might reasonable. However, also take mean median. Additionally, factors within data set, children income, may help predict age degree accuracy. technique best? don’t know. However, represents two percent entries. choose suboptimal path, damage minimal. case, lazy use mean age.Using averages interesting looked entire data set, find know individuals average age. known issue designing everything airplane cockpits gloves. also simulate distribution randomly assign values distribution. However, technique also issues. , just aware think critically . can reasonably justify approach, good enough.know 58% fans married. approach one? pattern marriage random? Assuming individual married 58% time randomly pick one, accurate simply saying everyone married? can’t tell . pull Bayes’ theorem devise various ways estimate likelihood person married. Since deal 2% data, use simple imputation random guesses discrete probability.imputed three variables can see tedious exercise can . However, several packages can help automate tasks.Table 5.12: Customer data setWe can approach children gender much way categorical data.Let’s take slightly different approach rest data. Customer Ids missing, going delete rows. , need able link segment constituents individual utility. addition, want able compare demographic attributes actual behaviors.","code":"\n#-----------------------------------------------------------------\n# Write a function to generate stats on missing data\n#-----------------------------------------------------------------\nf_missing_data <- function(dF){\n\nna  <- sum(apply(dF,2,function(x) is.na(x)))\nnan <- sum(apply(dF,2,function(x) is.nan(x)))\ninf <- sum(apply(dF,2,function(x) is.infinite(x)))\n\nmissing        <- as.data.frame(rbind(na,nan,inf))\nnames(missing) <- 'Values'\nreturn(missing)\n}\n\nmissing <- f_missing_data(demo_data)\n#-----------------------------------------------------------------\n# Randomly make some values in the dataframe NA\n#-----------------------------------------------------------------\nset.seed(755)\ndemo_data[] <- \napply(demo_data,1:2, function(x) ifelse(runif(1,0,1) > .98,NA,x))\n#-----------------------------------------------------------------\n# Print the number of missing values for each column\n#-----------------------------------------------------------------\nfor(i in names(demo_data)){\n  print(paste(sum(is.na(demo_data[,i])),names(demo_data[i])))\n}\n#> [1] \"3973 custID\"\n#> [1] \"3997 ethnicity\"\n#> [1] \"4042 age\"\n#> [1] \"4111 maritalStatus\"\n#> [1] \"4021 children\"\n#> [1] \"3993 hhIncome\"\n#> [1] \"4044 distance\"\n#> [1] \"4030 gender\"\n# You can get the same data with this statement:\n\n# sapply(1:length(names(demo_data)), function(x)\n#        paste(sum(is.na(demo_data[,x])),names(demo_data[x])))\n\n# A best practice is to use apply functions in R when possible\n#-----------------------------------------------------------------\n# Only keep rows with no NAs in any column\n#-----------------------------------------------------------------\ncomplete_cases <- nrow(demo_data[complete.cases(demo_data),])\nknitr::kable(nrow(demo_data[complete.cases(demo_data),]),caption = 'Complete cases',\n             align = 'c',format = \"markdown\",padding = 0)\n#-----------------------------------------------------------------\n# Simple imputation. Make any NAs 'w'\n#-----------------------------------------------------------------\ndemo_data$ethnicity[is.na(demo_data$ethnicity)] <- 'w'\n#-----------------------------------------------------------------\n# Make any missing age values the mean of the overall age\n#-----------------------------------------------------------------\nmA <- \n  round(mean(as.numeric(as.character(na.omit(demo_data$age)))),0)\ndemo_data$age[is.na(demo_data$age)] <- mA\n#-----------------------------------------------------------------\n# Impute values based on a proportion\n#-----------------------------------------------------------------\nl <- \n  length(demo_data$maritalStatus[is.na(demo_data$maritalStatus)])\ndemo_data$maritalStatus[is.na(demo_data$maritalStatus)] <- \n  sample(c('m','s'), size=l, prob=c(.6,.4), replace=TRUE)\n#-----------------------------------------------------------------\n# Impute remaining values based on a proportion\n#-----------------------------------------------------------------\n#prop.table(table(demo_data$children))\n#prop.table(table(demo_data$gender))\n\nl <- length(demo_data$children[is.na(demo_data$children)])\ndemo_data$children[is.na(demo_data$children)] <- \nsample(c('n','y'), size=l, prob=c(.52,.48), replace=TRUE)\n\nl <- length(demo_data$gender[is.na(demo_data$gender)])\ndemo_data$gender[is.na(demo_data$gender)] <- \nsample(c('m','f'), size=l, prob=c(.5,.5), replace=TRUE)"},{"path":"chapter5.html","id":"imputating-with-a-package","chapter":"5 Segmentation","heading":"5.2.3.0.1 Imputating with a package","text":"several packages imputing numerical data. One commonly used ones mice (van Buuren Groothuis-Oudshoorn 2021). easy use works relatively quickly smaller datasets. Several parameters can adjusted mice function. See documentation explain indicate. instance, used predictive mean matching method. However, dozen options . may appropriate others specific conditions.Let’s go ahead delete rows customerid missing. Use na.omit greedily remove rows column NA . can subtle, na.omit get job done case.Now can run function look missing values .Table 5.13: New missing valuesPerfect. expert imputation, now get gist. Correcting data missing values tedious critical component exercises. doesn’t tend one best way, methods often judgment calls. example, 40% data missing, perform similar exercise just saw? may depend data missing. point think context goal.","code":"\n#-----------------------------------------------------------------\n# Impute numerical values using the mice package\n#-----------------------------------------------------------------\nlibrary(mice)\ndemo_data$distance <- as.numeric(demo_data$distance)\nimp_frame          <- as.data.frame(cbind(demo_data$hhIncome,\n                           demo_data$distance))\nimp_frame[]        <- apply(imp_frame,2,function(x) as.numeric(x))\n\nimputed_Data       <- mice(imp_frame, m=5, maxit = 10,\n                           method = 'pmm', seed = 755,\n                           printFlag = F)\nnew_Data           <- complete(imputed_Data,2)\ndemo_data$hhIncome <- new_Data$V1\ndemo_data$distance <- new_Data$V2"},{"path":"chapter5.html","id":"discretizing-our-numerical-variables","chapter":"5 Segmentation","heading":"5.2.3.1 Discretizing our numerical variables","text":"Many operations (latent class regression) require discreet data. art part exercise. However, several ways approach . ’ll couple different ways. ’ll start creating couple appropriate data structures segmentation methods.Let’s begin creating new data frame work . ’ll use data frames tibbles interchangeably. yet speak tibbles. Tibbles Wickham construct working tidyverse. functionally act like data frames features generally make little friendlier work . However, specific legacy packages require data frames. one peculiarities R won’t find Python. can also frustrating.","code":"\n#-----------------------------------------------------------------\n# Create a new dataframe to work with\n#-----------------------------------------------------------------\ndemo_data_discrete <- tibble(custID = demo_data$custID)"},{"path":"chapter5.html","id":"dummy-coding-our-categorical-variables","chapter":"5 Segmentation","heading":"5.2.3.2 Dummy coding our categorical variables","text":"must treat Categorical variables particular way useful many segmentation exercises. begin, need discretize numerical variables. ’ll function apply numerical data sets. following function represents choice ’ll make. determine break continuous data discrete groups? case, use familiar generational marketing groups. package lubridate (Spinu, Grolemund, Wickham 2021) used make working dates easier. Dates can incredibly frustrating , along regular expressions50, among reviled parts working data.Programmers thrive criticizing sort function. often algorithmic methods simplifying sort operation. switch function can sometimes used R languages. cares aren’t dealing massive data sets? readable get job done. SQL joins efficient -statement, generally good direction begin larger data sets.prefer individual’s birth year age. helps keep data evergreen. Keep mind build surveys. Now discrete value representing specific generations.Distance represents another nuanced problem. select create breaks? Distance can often approximated using zip code centroids Haversine 51 formula. can also use Google’s APIs calculate latitude longitude based address estimate drive time. important certain urban suburban areas.case, already calculated distance. ’ll use quantiles place 60% customers one four buckets.can discretize household income way. case, ’ll use three buckets. Remember categories , columns final data set. can problematic get many.Finally, ’ll add variables dataframe.Table 5.14: discretized dataNow discrete variables, can take preparation step dummy-code variables. , several ways . psych package function use. another frustrating thing R. Python, must implement encoder way every time.Table 5.15: Dummy coded dataNow dataframe consists categorical data; ’ll create one mixed numerical dummy-coded categorical data.Table 5.16: Combined data frameOur new dataframe mixes discrete continuous variables format consumable wide array machine learning tools. column numeric. take even , example work now.","code":"\n#-----------------------------------------------------------------\n# Calculate birth year and assign it to a generation\n#-----------------------------------------------------------------\nlibrary(lubridate)\nbirthYear <- lubridate::year(Sys.Date()) - \n             as.numeric(demo_data$age)\n\nf_seg_generation_def <- function(birth_year){\n  \n  if(birth_year >= 1910 & birth_year <= 1924){'gen_Greatest'}\n   else if (birth_year >= 1925 & birth_year <= 1945){'gen_Silent'}\n    else if (birth_year >= 1946 & birth_year <= 1964){'gen_Boomer'}    \n     else if (birth_year >= 1965 & birth_year <= 1979){'gen_X'}     \n      else if (birth_year >= 1980 & birth_year <= 1994){'gen_Mill'}       \n       else if (birth_year >= 1995 & birth_year <= 2012){'gen_Y'}    \n        else if (birth_year >= 2013 & birth_year <= 2020){'gen_Alpha'}      \n         else{'Unknown'}\n}\ndemo_data_discrete$generation <- \n  sapply(birthYear,f_seg_generation_def)\n#-----------------------------------------------------------------\n# Assign a discrete value to distance\n#-----------------------------------------------------------------\nquantile_list <- quantile(demo_data$distance,\n                          probs = c(.1,.2,.3,.4,.5,.6,.7,.8,.9))\n\nf_seg_dist_def <- function(dist,q){\n  \n  if(dist <= 30.67){'dist_Primary'}\n    else if (dist > 30.67 & dist <= 51.23){'dist_Secondary'}\n      else if (dist > 41.23 & dist <= 106.07){'dist_Tertiary'}    \n        else{'dist_Quaternary'}        \n}\n\ndemo_data_discrete$marketLoc <- \n  sapply(demo_data$distance,f_seg_dist_def)\n#-----------------------------------------------------------------\n# Assign a discrete value to Household Income\n#-----------------------------------------------------------------\nhhIncomeQuantiles <- quantile(demo_data$hhIncome,\n                              probs = c(.25,.75))\n\nf_seg_hhInc_def <- function(hhInc, quant = hhIncomeQuantiles){\n  \n  if(hhInc <= hhIncomeQuantiles[[1]]){\"income_low\"}\n    else if(hhInc >= hhIncomeQuantiles[[2]]){\"income_high\"}\n      else{\"income_med\"}\n}\n\ndemo_data_discrete$income <-\n  sapply(demo_data$hhIncome,f_seg_hhInc_def)\n#-----------------------------------------------------------------\n# Add columns to demo_data\n#-----------------------------------------------------------------\ndemo_data_discrete <- demo_data_discrete    %>% \n  mutate(ethnicity = demo_data$ethnicity,\n         married   = demo_data$maritalStatus,\n         gender    = demo_data$gender)\n#-----------------------------------------------------------------\n# Dummy code (One Hot encoding) for model consumption\n#-----------------------------------------------------------------\ndummy_coded_vars <- \n  apply(demo_data_discrete[,2:7],2, \n        function(x) psych::dummy.code(factor(x)))\n\nmod_data_discrete <- \n  cbind.data.frame(dummy_coded_vars[1:length(dummy_coded_vars)])\n\nrow.names(mod_data_discrete) <- demo_data_discrete$custID\n#-----------------------------------------------------------------\n# Combine data frames\n#-----------------------------------------------------------------\nmod_data_numeric <- demo_data[,c('age','distance','hhIncome')]\nmod_data_numeric <- cbind.data.frame(mod_data_numeric,\n                                     mod_data_discrete[,13:20]) "},{"path":"chapter5.html","id":"hierarchical-clustering","chapter":"5 Segmentation","heading":"5.2.4 Hierarchical clustering","text":"Hierarchical clustering seductive results easily illustrated. However, drawback technique may tell already know simple models. precise explanation , “constraint structure hierarchical corresponds fact , although subsets can include one another, intersect.” (Ian H. Witten 2011) technique also problematic larger data sets.Let’s take look example. ’ll start creating dissimilarity matrix.can now use dissimilarity matrix cluster data. Keep mind dissimilarity matrices get large fast. Take look , understand . often sampling data approximate large data sets vital. big data problems simply small data problems disguise. case, tested twenty-five individuals.Several groups fans tend cluster together. However, cuts can happen predictable ways using technique. byproduct categorical data.","code":"\n#-----------------------------------------------------------------\n# Prepared data for hierarchical clustering\n#-----------------------------------------------------------------\nlibrary(dplyr)\nlibrary(cluster)\nset.seed(755)\nmod_data_samp     <- mod_data_numeric %>% dplyr::sample_n(25)\nmod_data_samp$age <- as.numeric(mod_data_samp$age)\n# Create a dissimilarity matrix\nmod_dist <- cluster::daisy(mod_data_samp)\n#-----------------------------------------------------------------\n# Apply the hierarchical clustering method\n#-----------------------------------------------------------------\nmod_HC <- stats::hclust(mod_dist, method = \"centroid\")\n#-----------------------------------------------------------------\n# Apply the hierarchical clustering method\n#-----------------------------------------------------------------\nplot(mod_HC)"},{"path":"chapter5.html","id":"latent-class-regression","chapter":"5 Segmentation","heading":"5.2.5 Latent class regression","text":"Latent class regression powerful technique produces durable clusters. However, leverages discrete data, continuous data must discredited dummy-coded use. ’ve already dummy-coded data, ’ll process LCR function accepts . another quirks R. allows happen. case, ’ll make zeros = 2. can apply function simple ifelse function.Table 5.17: Data coded LCR functionA helpful trick simplify function feeds clustering function. make function easier read add parameters.going choose five classes include data. formal methods determining number classes include, remember probably abstract relatively groups classes valuable understandable. can also problematic since behaviors exhibited group can become diffuse.can also save model importing applying back new data future.Now can look see many groups created machine-learning algorithm.Table 5.18: Frequency table segmentsOur groups relatively large range. example, smallest group contains 10,000 people, largest contains 56,000 individuals.can now create list ids class apply original data set.Table 5.19: LC model resultsOnce joined, can create exciting crosstabs look group differences.Distance generally similar, one group slightly away average.Table 5.20: Distance segmentAge much enjoyable look . Three distinct groups manifest arrange latent classes.Table 5.21: Age segmentHousehold income also well differentiated three distinct groups.Table 5.22: HHI segmentGender two groups male female three evenly split.Table 5.23: Gender segmentEthnicity reasonably well distributed across three groups.one little complex. can use heat map visualize relationships. Using log function fill argument exponentially diminish differences colors. valuable technique. Keep mind. Logarithms great compressing data challenging visualize distribution.One segment made entirely white individuals, one segment white individuals. segments split proportionally across ethnic group. now? reason believe groups represent segments individuals? even represent market structure? still need finish. groups spend money average? respond advertising different ways? ’ll want answer questions. Another way approach problem segment people measurable behaviors look look-alike groups. common way leverage sort analysis.demonstrate combine one segmentation approach previous analysis create psychographic segments.","code":"\n#-----------------------------------------------------------------\n# Latent Class regression\n#-----------------------------------------------------------------\nmod_data_LC   <- mod_data_discrete\nmod_data_LC[] <- apply(mod_data_discrete, 2, \n                       function(x) ifelse(x == 1 ,1 ,2))\n#-----------------------------------------------------------------\n# Latent Class regression formula\n#-----------------------------------------------------------------\nformat  <- paste(names(mod_data_LC), collapse = \",\")\nformula <- \n  with(mod_data_LC,\n       cbind(generation.gen_X,generation.gen_Mill,\n             generation.gen_Boomer,generation.gen_Y,\n             generation.gen_Silent,marketLoc.dist_Quaternary,\n             marketLoc.dist_Tertiary,marketLoc.dist_Secondary,\n             marketLoc.dist_Primary,income.income_med,\n             income.income_low,income.income_high,\n             ethnicity.w,ethnicity.aa,ethnicity.h,\n             ethnicity.a,married.m,married.s,gender.f,\n             gender.m)~1)\n#-----------------------------------------------------------------\n# Latent Class regression\n#-----------------------------------------------------------------\nrequire(poLCA)\nset.seed(363)\nseg_LCR_5 <- poLCA::poLCA(formula, data = mod_data_LC, \n                          nclass = 5, verbose = F)\nsave(seg_LCR_5, file=\"CH5_LCR_5_Model.Rdata\")\n#-----------------------------------------------------------------\n# Evaluating the results\n#-----------------------------------------------------------------\ntable(seg_LCR_5$predclass)\n#-----------------------------------------------------------------\n# Attaching to the data frame\n#-----------------------------------------------------------------\nmod_results          <- \n  as.data.frame(matrix(nrow = nrow(mod_data_LC), ncol = 0))\nmod_results$custID   <- row.names(mod_data_LC)\nmod_results$lcrClass <- seg_LCR_5$predclass\n#-----------------------------------------------------------------\n# View segments\n#-----------------------------------------------------------------\nhead(mod_results)\n#-----------------------------------------------------------------\n# Attaching to the data frame\n#-----------------------------------------------------------------\ndemo_data_segments <- \n  dplyr::left_join(demo_data,mod_results, by = \"custID\")\n#-----------------------------------------------------------------\n# Distance cross tab\n#-----------------------------------------------------------------\nlibrary(dplyr)\ndemo_seg_distance <- demo_data_segments %>%\n  group_by(lcrClass)                    %>% \n  summarise(avgDist = mean(distance),\n            medDist = median(distance))\n                  \n#-----------------------------------------------------------------\n# Age cross tab\n#-----------------------------------------------------------------\ndemo_data_segments$age <- as.numeric(demo_data_segments$age)\n\ndemo_seg_age <- demo_data_segments             %>%\n                group_by(lcrClass)             %>% \n                summarise(avgAge = mean(age),\n                          medAge = median(age))\n                    \n#-----------------------------------------------------------------\n# Household income cross tab\n#-----------------------------------------------------------------\ndemo_seg_HHI <- demo_data_segments                        %>%\n                     group_by(lcrClass)                   %>% \n                     summarise(avgHHI = mean(hhIncome),\n                               medHHI = median(hhIncome))\n                    \n#-----------------------------------------------------------------\n# Gender cross tab\n#-----------------------------------------------------------------\ndemo_seg_gender <- \n  demo_data_segments         %>%\n  group_by(lcrClass)         %>% \n  count(gender)              %>%\n  tidyr::pivot_wider(names_from = gender, \n                     values_from = n)\n                     \n#-----------------------------------------------------------------\n# Ethnicity tab\n#-----------------------------------------------------------------\ndemo_seg_ethnicity <- \n  demo_data_segments    %>%\n  group_by(lcrClass)    %>% \n  count(ethnicity)      %>%\n  tidyr::pivot_wider(names_from  = ethnicity, \n                     values_from = n)\n                    \n#-----------------------------------------------------------------\n#  Visualize ethnicity\n#-----------------------------------------------------------------\nlibrary(ggplot2)\ndemo_seg_ethnicity_l <- \n  demo_data_segments %>%\n  group_by(lcrClass) %>% \n  count(ethnicity)      \n\ndemo_seg_ethnicity_l <- as.data.frame(demo_seg_ethnicity_l)\n\nx_label  <- ('\\n Ethnicity')\ny_label  <- ('Class \\n')\ntitle   <- ('Ethnicity by class')\ntile_segment <- ggplot2::ggplot(data   = demo_seg_ethnicity_l, \n                              aes(x    = ethnicity,\n                                  y    = lcrClass,\n                                  fill = log(n)))               +\n  geom_tile()                                                   +\n  scale_fill_gradient(low=\"white\", high=\"dodgerblue\",\n                      name = 'log(Fans)',label = scales::comma) +\n  xlab(x_label)                                                 + \n  ylab(y_label)                                                 + \n  ggtitle(title)                                                +\n  graphics_theme_1"},{"path":"chapter5.html","id":"a-benefits-sought-approach-to-segmentation","chapter":"5 Segmentation","heading":"5.3 A benefits sought approach to segmentation","text":"Behavioral segmentation can take many forms. ’ll focus activities fans may consider attending game. Taking benefits-sought approach segmentation interesting gives specific insight might position marketing offers particular group. downside discoverability reduced.’ll link forms build psychographic profiles. can go much deeper. instance, can ask many questions various activities. ’ll cover chapter research, chapter 9.","code":""},{"path":"chapter5.html","id":"factoranalysis","chapter":"5 Segmentation","heading":"5.4 Using factor analysis to segment","text":"Factor analysis data reduction technique similar principle components analysis. However, ’ll use slightly differently compress stated behaviors use patterns create specific archetypes.can load survey data using fa_survey_data data set FOSBAAS.Table 5.24: Survey dataThis data set based asking individuals like ballpark. list predefined activities, fans asked rank 0 10. Fans also asked attended game.data wasn’t created person mind. can assign customer_ids data set simulate real people taking survey. ’ll also need process data perform factor analysis. use scale function normalize data. yet discuss , can important mixed data sets. Ensure data placed dataframe instead tibble.Now data prepared analysis, need consider many factors create. formulaic way determine number factors. ’ll use nScree function nFactors (Raiche Magis 2020) package understand many factors appropriate. ways visualize data. Try eigen(cor(survey_sc)) function access eigenvalues.Table 5.25: Number FactorsNow know many factors appropriate, can run factor analysis. First, ’ll use GPArotation package (Bernaards Jennrich 2022) add rotation argument factanal function stats (R-stats?) package. help us visualize data.’ll restructure data prepare visualization.Now data processed format can graph, can look .Certain activities tend group factors. data? First, need name groups approachable. art. Let’s give terrible try:Factor 1: Activities tend oriented toward game. Let’s call Avid fans. Creative.Factor 2: Tailgating posting social media. Socializers. group also tends affinity children’s activities.Factor 3. Drink beer, sample food, Eat park food. Foodies.Factor 4: Seems interested park. ’ll need go deeper . Parkies.Factor 5: Doesn’t distinct groupings. StrangersConsultants marketing backgrounds begin crafting stories groups. supplemental analysis necessary get granular behavioral profiles. won’t take fake exercise far. get point.One problems using sort analysis impossible cast segments across groups taken survey. Additionally, someone belong different segments different days. example, father bring children game one day go friends another. Belonginess can mutable. However, richer data data sample, might able predict segment. can also create richer profiles utilizing demographic segments created earlier chapter. Let’s begin building simple model see factors can used predict reason someone came game. combine data see can find.’ll use function nnet (Ripley 2022) package -fit model. aren’t rigorous just curious . ’ll use multinom function try predict classes.surprisingly, can’t. However, reasons created entirely randomly, wouldn’t expect see good predictions . predict reasons, expect tight 45-degree color band middle graph. terrible result like real life? evaluate data another way. dead-end. happens frequently.","code":"\n#-----------------------------------------------------------------\n# access the survey data\n#-----------------------------------------------------------------\nsurvey <- FOSBAAS::fa_survey_data\n#-----------------------------------------------------------------\n# access the survey data\n#-----------------------------------------------------------------\nnFacts <- nFactors::nScree(survey_sc)\n#-----------------------------------------------------------------\n# run a factor analysis\n#-----------------------------------------------------------------\nlibrary(GPArotation)\nsurvey_sc_ob <- factanal(survey_sc,\n                         factors  = 5,\n                         rotation = \"oblimin\",\n                         scores   = \"Bartlett\")\n#-----------------------------------------------------------------\n# Restructure the data\n#-----------------------------------------------------------------\nsurvey_sc_ob_scores <- as.data.frame(unlist(survey_sc_ob$scores))\nsurvey_sc_ob_scores$Reason <- survey$ReasonForAttending\n\nfa_data <- unclass(survey_sc_ob$loadings)\nfa_data  <- as.data.frame(fa_data)\nfa_data$Selection <- row.names(fa_data)\n\nlibrary(tidyr)\nfa_data_p <-  \n  fa_data %>% tidyr::pivot_longer(!Selection, \n                                   names_to  = \"Factor\", \n                                   values_to = \"Loading\")\n\nfa_data_p$Order <- \n  reorder(fa_data_p$Selection,fa_data_p$Loading,sum)\n#-----------------------------------------------------------------\n# Visualize the factors\n#-----------------------------------------------------------------\nrequire(ggplot2)\nfactor_table <- \nggplot(fa_data_p, aes(Factor, Order))                       +\n  geom_tile(aes(fill = Loading))                            + \n  geom_text(aes(label = round(Loading, 1)),color='grey40')  +\n  scale_fill_gradient(low      = \"white\", \n                      high     = \"dodgerblue\", \n                      space    = \"Lab\",\n                      na.value = \"grey50\", \n                      guide    = \"colourbar\")               +\n  xlab('\\n Factor')                                         + \n  ylab('Activity\\n')                                        + \n  ggtitle('What do fans do at games? \\n')                   +\n  graphics_theme_1                                          +\n  theme(legend.position=\"none\")                             +\n  theme(axis.text.y = element_text(angle = 0, \n                                   size = 8, vjust = 0, \n                                   color = \"grey10\"))\n#-----------------------------------------------------------------\n# predict classes\n#-----------------------------------------------------------------\nlibrary(nnet)\nmod_glm <-   nnet::multinom(Reason ~ .,\n                            data = survey_sc_ob_scores,\n                            linout = FALSE)\n#> # weights:  98 (78 variable)\n#> initial  value 26390.573296 \n#> iter  10 value 26368.581604\n#> iter  20 value 26367.453867\n#> iter  30 value 26362.202858\n#> iter  40 value 26361.592507\n#> iter  50 value 26360.892606\n#> final  value 26360.870907 \n#> converged\n\npred_survey_sc_ob_scores <- predict(mod_glm , \n                            newdata=survey_sc_ob_scores,\n                            type='class')\n\ntile_data <- \nas.data.frame(table(survey_sc_ob_scores$Reason,\n                    pred_survey_sc_ob_scores))\n#-----------------------------------------------------------------\n# Visualize the factors\n#-----------------------------------------------------------------\nrequire(ggplot2)\n\nx_label  <- ('\\n Actual response')\ny_label  <- ('Attend predict \\n')\ntitle   <- ('Prediction of attendance reason')\n\ntile_class <- ggplot2::ggplot(data = hex_data, \n                              aes(x    = Var1,\n                                  y    = pred_survey_sc_ob_scores,\n                                  fill = Freq))        +\n  geom_tile()                               +\n  scale_fill_gradient(low = \"dodgerblue\", high = \"coral\",\n                      name = 'Count',label = scales::comma)   +\n  xlab(x_label)                                               + \n  ylab(y_label)                                               + \n  ggtitle(title)                                              +\n  graphics_theme_1                                            +\n  theme(axis.text.x = element_text(angle = 90, \n                                   size = 8, vjust = 0, \n                                   color = \"grey10\"),\n        axis.text.y = element_text(angle = 0, \n                                   size = 8, vjust = 0, \n                                   color = \"grey10\"))"},{"path":"chapter5.html","id":"casting-segments-onto-other-groups","chapter":"5 Segmentation","heading":"5.4.1 Casting segments onto other groups","text":"Can use demographic factors estimate someone’s factor group? , easy cast segments onto broader groups people. Let’s assign survey respondents individuals customer table.Now combined analyses, can look patterns within data. First, let’s recode data approachable. Next, let’s use switch function. switch function excellent, languages . Thanks, R!Now ready look profiles slightly sophisticatedly.","code":"\n#-----------------------------------------------------------------\n# Apply classes to individuals\n#-----------------------------------------------------------------\nids <- sample(demo_data_segments$custID,10000)\nsurvey_sc_ob_scores$Class <- \n  colnames(survey_sc_ob_scores)[max.col(survey_sc_ob_scores[,1:5],\n                                        ties.method=\"first\")]\nsurvey_sc_ob_scores$custID <- ids\ncombined_classes <- dplyr::left_join(demo_data_segments,\n                                     survey_sc_ob_scores,\n                                     by=\"custID\")         %>%\n                                     na.omit()\ncombined_classes <- as.data.frame(combined_classes)\n#-----------------------------------------------------------------\n# recode the factors and classes\n#-----------------------------------------------------------------\ncombined_classes$f_seg_name <- sapply(combined_classes$Class, \n                 function(x) switch(x,\"Factor1\" = \"Avid fan\",\n                                      \"Factor2\" = \"Socializers\",\n                                      \"Factor3\" = \"Foodies\",\n                                      \"Factor4\" = \"Parkies\",\n                                      \"Factor5\" = \"Strangers\"))\n\ncombined_classes$d_seg_name <- sapply(combined_classes$lcrClass, \n                 function(x) switch(x,'1' = \"Young and Broke\",\n                                      '2' = \"Marty Male\",\n                                      '3' = \"Fionna Female\",\n                                      '4' = \"Diffuse Diane\",\n                                      '5' = \"Nearby Ned\"))"},{"path":"chapter5.html","id":"psychographic-segmentation","chapter":"5 Segmentation","heading":"5.5 Psychographic segmentation","text":"Psychographic augmentations tend abstract, although simplify combination behavioral demographic schemes. see schemes often. segments alliterative names “Jets fan Jeff” “Torpid Tina.” can use behavioral demographic segmentation schemes build psychographic schemes.can see demographic segments factor segments cluster specific groups. can now analyze groups create detailed psychographic profiles.Now can look differences groups. data contrived, can extract interesting insights .factored segments aren’t influenced age distance.Clusters well differentiated distance age.case, -male female segments tend index older segments live average distance park. hand, Young Broke segment tends live nearby. gave Nearby Ned ironic name since tends live furthest park., just getting started. example produced demonstrate tedious segmentation can . complete analysis much exhausting repetitive. covered basic steps.","code":"\n#-----------------------------------------------------------------\n# Visualize the results\n#-----------------------------------------------------------------\ncc <- combined_classes                %>%\n      group_by(f_seg_name,d_seg_name) %>% \n      count()      \n\nx_label  <- ('\\n Factor Segments')\ny_label  <- ('Demographic Segment \\n')\ntitle   <- ('Segment Comparrison')\ntile_segment <- \n  ggplot2::ggplot(data = cc, \n                  aes(x = f_seg_name,\n                      y = d_seg_name,\n                      fill = n))                              +\n  geom_tile()                                                 +\n  scale_fill_gradient(low=\"white\", high=\"dodgerblue\",\n                      name = 'Count',label = scales::comma)   +\n  xlab(x_label)                                               + \n  ylab(y_label)                                               + \n  ggtitle(title)                                              +\n  graphics_theme_1                                            +\n    theme(axis.text.x = element_text(angle = 0, \n                                   size = 8, vjust = 0, \n                                   color = \"grey10\"),\n        axis.text.y = element_text(angle = 0, \n                                   size = 8, vjust = 0, \n                                   color = \"grey10\"))\n#-----------------------------------------------------------------\n# Scale the results and average\n#-----------------------------------------------------------------\ncombined_classes$age_s <- \n  scale(as.numeric(combined_classes$age))\ncombined_classes$distance_s <- \n  scale(as.numeric(combined_classes$distance))\n\ncc <- \n  combined_classes                %>%\n  group_by(f_seg_name,d_seg_name) %>% \n  summarise(age = mean(age_s), \n  distance = mean(distance_s)) \n#-----------------------------------------------------------------\n# Observe differences between the segments\n#-----------------------------------------------------------------\nx_label  <- ('\\n Scaled Avg Age')\ny_label  <- ('Scaled Avg Distance \\n')\ntitle    <- ('Segments by distance and age')\n\npoint_segment <- \n  ggplot2::ggplot(data      = cc, \n                  aes(x     = age,\n                      y     = distance,\n                      color = f_seg_name,\n                      shape = d_seg_name))                +\n  geom_point(size = 3)                                    +\n  scale_color_manual(values = palette)                    +\n  geom_hline(yintercept = median(cc$distance), lty = 2)   +\n  geom_vline(xintercept = median(cc$age), lty = 2)        +\n  xlab(x_label)                                           + \n  ylab(y_label)                                           + \n  ggtitle(title)                                          +\n  graphics_theme_1"},{"path":"chapter5.html","id":"other-segmentation-methods","chapter":"5 Segmentation","heading":"5.6 Other segmentation methods","text":"hundreds methods segment data. Generally, start simplest method work . means using one rule . Think Males Females. many tools use:K-means clusteringKNNExpectation maximizationsimple schemesNo one approach best. Nate Silver describes Hedgehog Fox book “Signal Noise.” (Silver 2012) case forecasting, want fox. ’ll need combine practical knowledge analytic process. purely numerical approach unlikely yield best results.","code":""},{"path":"chapter5.html","id":"key-concepts-and-chapter-summary-4","chapter":"5 Segmentation","heading":"5.7 Key concepts and chapter summary","text":"Segmenting data art science. also frustrating, repetitive, demanding. didn’t cover simple segmentation schemes like age ranges generations. However, excellent place start typically. covered lot material chapter tried hit many high marks tangible examples. covered:Data exploration demographic approach segmentationData preparation imputationAn hierarchical approach segmentationSegmentation schemes using latent class regression demographicsA benefits sought approach utilizing factor analysisCombining schemes build psychographic profilesThere many ways segment customers potential customers, can creative . key uncover something beyond market structure. looking exploitable differences groups help spend limited marketing dollars efficiently.demographic segmentation merits, may helpful . However, simply accessible place start.Preparing data analysis tedious task better suited someone else. part project time-consuming. bears heavily outcome, get correct.Hierarchical approaches segmentation utility cases can often tell already know.Latent class regression tends produce durable segments excellent second step looking simple segmentation schemes.Benefits-sought approaches segmentation often good utility less discoverable methods. Factor analysis excellent method simplify complex data sets.can ensemble methods create richer profiles.","code":""},{"path":"chapter6.html","id":"chapter6","chapter":"6 Pricing and Forecasting","heading":"6 Pricing and Forecasting","text":"Like segmentation, writing entire book forecasting pricing tickets easy due breadth subjects. innumerable considerations, determining willingness pay, taking arbitrage account, cannibalizing sales, margins, marketing channel, product suite, brand considerations, competing internal goals. chapter also immediately follows examples segmenting market. order deliberate. Pricing dynamic; certain circumstances, products may worth less money. However, pricing sometimes deliberate exercise. Consider quote found Robert Phillips’ book “Revenue Pricing Optimization” (Phillips 2005)“many cases, prices charged customers result large number uncoordinate arbitrary decisions.”— Robert L. PhillipsI agree Mr. Phillips’ sentiment. pricing fundamental component marketing mix, isn’t always well understood interacts promotions marketing efforts (’ll explore little chapter 9). Additionally, always brand considerations consider. Marketers often regress blunt instrument lower prices move tickets. tactic can counterproductive long term. team isn’t good, lower prices less impact team performing well. However, less impetus lower prices team performs well.Pricing impacts brand. Since season ticket holders critical risk mitigation, anything eroding value scrutinized. chapter take simplified example can set prices discuss rationale behind thinking. , use real examples demonstrate done practice. However, going knee-deep ocean. Pricing can challenging complicated exercise.","code":""},{"path":"chapter6.html","id":"understanding-your-inventory","chapter":"6 Pricing and Forecasting","heading":"6.1 Understanding your inventory","text":"value tickets? Let’s think inventory might look. multiple ticket classes, individuals may cross numerous classes. Additionally, certain conditions make sales inventory fluid. instance, goal maximize revenue, selling many season tickets possible may reduce revenue potential sold discount. tipping point. pieces inventory may inefficient use certain games. instance, may want use something group discounts opening day high demand obviates need sell discounted ticket classes. Typical ticketing inventory includes:Season ticketsGroup ticketsSmall plansSingle-game ticketssubscription ticketsAdditionally, tickets may obtained several channels. Understanding channels critical pricing. Think airlines. exert significant control channels, enabling incredibly fine-grain control pricing tactics. Typical ticketing channels include:Primary channels\nDirectly team league\nconsignment mechanism\nticketing system Ticketmaster\nDirectly team leagueThrough consignment mechanismThrough ticketing system TicketmasterSecondary channels\nSeatGeek\nStubHub\nSeatGeekStubHubSecondary channels even nuanced professional ticket brokers may many tickets. tickets may may exist liquid market allows prices float level. Ticket brokers represent unique problem. Let’s discuss ticket brokers .Ticket brokers buy sell tickets events. sports environment, two primary strategies. However, can hedge bets purchasing multiple events teams different sports. arbitrage opportunities might also exist, parking passes. two main strategies :Scale purchase cheap inventoryPurchase good inventory maintain strong marginsOption number one can risky. However, varies market. purchase lots inexpensive inventory make money marquee events margins high demand significantly outstrips supply (Think big games weekend prime opponents playoff games). Option two complicated. likely takes time makes visible team. Many clubs now employ revenue-sharing opportunities brokers use sell single-game tickets secondary market. Clubs may also leverage secondary channels flatten markets.Selling brokers mitigates risk get money can used operations early. hand, amount money dealing large 52. However, tradeoff. longer control sales channels. Imagine purchase airline tickets secondary channels. airlines able price effectively dynamically?Additionally, customer service issue. longer resides club. happens ticket doesn’t work? fix ? service issue becomes brand problem. Recently, quick proliferation digital tickets made problem less relevant, persist. also technology component. ticketing platform allow disallow? many technical problems likely solved future, haven’t eliminated.Adding complexity sales channels inventory classes price levels associated specific locations:Premium areas might amenities free F&B, better seats, club access, -seat service, etc.seats may shaded areasDistance field court may matter experiential perceptual perspectiveThe home field side park may attractive buyersThe setting sun may make seats less appealingSome people may want sit area near specific food offerings ingress point close preferred parking location.Scoreboard sightlines may issueAll factors may considered, certainly . Product stratification critical maximizing revenue opportunity across customer expectations.determine willingness--pay? historical data based people paid tickets past events. best sales levels associated prices ticket class specific location. data might much harder capture. stadium arena might dozens even hundreds different price levels. cannibalization must considered. may need learn price increase decrease one section might demand another. cross-price elasticity demand seating locations isn’t trivial question. can also deploy exciting techniques help mitigate cannibalization, liner optimization53.Another confounding factor inventory levels. know much inventory available? might see sold seven seats dugout section, sold Brewers Saturday night July. However, might know inventory hold unavailable sale. Additionally, might know many tickets sale secondary market. aren’t even considering marketing efforts promote game.Inventory control critical part pricing exercise. shouldn’t overlooked. Incentives sell specific asset classes sales team can lead sub-optimal inventory control protocols. instance, sales team may hold inventory particular games help hit group goals. conducting pricing exercise, ensure understand inventory may allocated.","code":""},{"path":"chapter6.html","id":"understanding-pricing-mechanisms","chapter":"6 Pricing and Forecasting","heading":"6.2 Understanding pricing mechanisms","text":"pricing tickets, may faced unresolvable goal.want sell available tickets highest price possible optimize revenueIs selling ten tickets $100 $1,000 revenue equivalent selling 100 tickets $10 $1,000 revenue? answer may different depending ask. going look price-response functions get quantitative perspective. Price response functions fun work , analytics perspective, enjoy deploying type analysis. Price response functions handy, pointed “Pricing Segmentation Analytics.” (Tudor Bodea 2012)helpful feature price-response functions , estimated, can used determine price sensitivity product demand change response change price. - Furguson BodeaWhat price-response function look like practice.? Let’s say sell tickets build chart results:table tells many tickets sold price level. figure 6.1 represents data:\nFigure 6.1: Linear price response function\nlinear model probably won’t great job forecasting sales different price levels. also encounter diminishing returns point continue raise prices. graph also missing critical information. Let’s assume prices represent one stadium section refer single-game tickets. ’ll ignore don’t know tickets sold, channel sold , available inventory. Let’s also briefly talk fitting distributions price response functions. many different ways, look relatively simple example. following examples taken “Pricing Segmentation Analytics.” (Tudor Bodea 2012)Typical examples price response functions include:Linear models:\\[\\begin{equation}\n\\ {d(p)} = {D} + {m}*{b}\n\\end{equation}\\]Exponential models:\\[\\begin{equation}\n\\ d(p) = C*p^\\epsilon\n\\end{equation}\\]Logit models:\\[\\begin{equation}\n\\ {d(p)} = \\frac{C*e^{+b*p}}{1 + e^{+b*p}}\n\\end{equation}\\]Let’s build simple model approximates data better straight line. can see linear model won’t fit data well curved one. also doesn’t look constant like logit curve (two models likely encounter). let’s try simple polynomial.\nFigure 6.2: Polynomial price response function\ncurve approximates line relatively well. go exercise finding line produces best fit, isn’t necessary stage. Now happy simple polynomial approximates curve reasonably, ? R provides several ways evaluate fits. following example, feed polynomial function lm (linear model) function R. can write function f_get_Sales can use return values line figure 6.2. ’ll use coefficients complete function.poorly written function hard-coded. output also identical using predict function. ’ll look briefly apply function data. best always generalize sorts functions possible. using hard-coded function understand happening. Try using following function compare output.apply function, overfit price response function can use forecast sales different price levels. Since concept may unfamiliar , ’ll take minute explain going . fit <- lm(sales$sales~poly(sales$price,2,raw=TRUE)) using lm function (linear model) fit line data points. aren’t necessarily looking best fit example. function f_get_sales() takes coefficients generated model stored fit builds equation can use plug price values equation give us sales values (y) terms price (x). might look scary, just middle-school algebra.equation function sales <- coef(fit)[1] + (coef(fit)[2]*new_price + (coef(fit)[3] * new_price^2)) nothing rearranged linear equation second-degree exponent. linear equation equation fit line figure 6.1 take familiar form:\\[\\begin{equation}\n\\ {y} = {m}{x} + {b}\n\\end{equation}\\]done rearrange linear equation add second-degree exponent. math now looks like :\\[\\begin{equation}\n\\ {f(x)} = b + {m_1}{x} + {m_2}{x^2}\n\\end{equation}\\]conceptualize linear regression simple linear equation additional feature manipulates slope line. simplest way think .\\[\\begin{equation}\n\\ ({m_1}{x} + {m_2}{x^2}) = {m}{x}\n\\end{equation}\\]Now can look estimates applying function original values.Table 6.1: Estimates applied old salesAdditionally, can feed numbers directly function. example, many sales expect $26 price level?Table 6.2: Estimates applied new salesThere tricks can use equation. highest point curve represents maximum number tickets sold particular price level. find price number sales point? raw equation:\\[\\begin{equation}\n\\ {f(x)} = -737.366 + {52.934}{x} + {-0.906}{x^2}\n\\end{equation}\\]deep get math book. ’ll see tricky stuff chapter operations, difficult get . First, calculate derivative function. since ’ve taken calculus, use math engine Wolframalpha 54.\\[\\begin{equation}\n\\ \\frac{dy}{dx} ({-737.366} + {52.934}{x} - {0.906}{x^2}) = {52.934} - {1.812}{x}\n\\end{equation}\\]Setting derivative = 0 solving equation give price level highest point curve:\\[\\begin{equation}\n\\ f'(0) = {52.934} - {1.812}{x};\n\\end{equation}\\]Now solve equation:\\[\\begin{equation}\n{26467}/{906} = {29.21}\n\\end{equation}\\]highest number sales happen price $29.21. can plug number back original equation get y value.\\[\\begin{equation}\n\\ f(29.21) = -737.366 + {52.934}{29.21} + {-0.906}{29.21^2} = 35.8151054\n\\end{equation}\\]$29.21, model predicts 35.8151054 tickets sold. Let’s look figure 6.3 try interpretation. side note, want jerk refuse use basic calculus, write binary search function traverses function along range values outputs minimums maximums. Since typically deal whole numbers limited range, approach acceptable.\nFigure 6.3: Finding local maximum\ngreat. identified maximum number tickets sold price level range. revenue look like specific prices according model?Table 6.3: Forecasted revenue different price levelsImagine must set prices product just modeled. ? know revenue-maximizing number lives. tell entire story? Suppose sell expensive tickets every price level. cumulative revenue look like case?Let’s use loop instead apply function defined function.Table 6.4: Revenue maximizing pricesThis interesting way look problem. assume sell every ticket costs price set, setting prices $25 make revenue. However, sold every ticket forecasted amounts, earn revenue: 8495.9703213. simplistic example demonstrates power variable dynamic pricing. also considerations. set prices , might appear $25 price level best. earn ticket revenue sell tickets. However, simply due price elasticity assumptions made. season-ticket-holder price $29? now brand considerations. inventory limited, makes decisions slightly discrete.","code":"\n#-----------------------------------------------------------------\n# The Price response function\n#-----------------------------------------------------------------\nlibrary(tidyverse)\n# Build a simple data set\nsales <- tibble::tibble(\n  sales = c(20,30,35,43,35,8,2,0),\n  price = c(25,28,29,30,31,34,35,36)\n)\n\nx_label  <- ('\\n Price')\ny_label  <- ('Ticket Sales \\n')\ntitle    <- ('Ticket Sales by Price')\nline_sales <- \n  ggplot2::ggplot(data  = sales, \n                  aes(x = price,\n                      y = sales))                 +\n  geom_point(size = 2.5,color = 'mediumseagreen') +\n  scale_x_continuous(label = scales::dollar)      +\n  xlab(x_label)                                   + \n  ylab(y_label)                                   + \n  ggtitle(title)                                  +\n  graphics_theme_1                                +\n  geom_line(color = \"mediumseagreen\")             +\n  geom_smooth(method = 'lm') \n#-----------------------------------------------------------------\n# The Price response function\n#-----------------------------------------------------------------\nx_label  <- ('\\n Price')\ny_label  <- ('Ticket Sales \\n')\ntitle    <- ('Ticket Sales by Price')\nline_sales_poly <- \n  ggplot2::ggplot(data = sales, \n                  aes(x = price,\n                      y = sales))                 +\n  geom_point(size = 2.5,color = 'mediumseagreen') +\n  scale_x_continuous(label = scales::dollar)      +\n  xlab(x_label)                                   + \n  ylab(y_label)                                   + \n  ggtitle(title)                                  +\n  graphics_theme_1                                +\n  geom_line(color = \"mediumseagreen\")             +          \n  stat_smooth(method = \"lm\",color = 'coral', \n              formula = y ~ x + poly(x, 2)-1) \n#-----------------------------------------------------------------\n# Function to return sales based on price\n#-----------------------------------------------------------------\nfit <- lm(sales$sales~poly(sales$price,2,raw=TRUE))\n\n f_get_sales <- function(new_price){\n sales <- coef(fit)[1] + \n         (coef(fit)[2]*new_price + (coef(fit)[3] * new_price^2))\n return(sales)\n }\n \n#-----------------------------------------------------------------\n# Function to return values from a polynomial fit\n#-----------------------------------------------------------------\nf_get_poly_fit <- function(new_var,dist_fit){\n\n  len_poly   <- length(dist_fit$coefficients)\n  exponents  <- seq(1:(len_poly-1))\n  value_list <- list()\n\n  for(i in 1:length(exponents)){\n    value_list[i] <- coef(dist_fit)[i+1] * new_var^exponents[i]\n  }\n  sum(do.call(sum, value_list),coef(dist_fit)[1])\n}\n#-----------------------------------------------------------------\n# Use f_get_sales to get modeled demand at each price level\n#-----------------------------------------------------------------\nold_prices      <- c(25,28,29,30,31,34,35,36)\nestimated_sales <- sapply(old_prices, function(x) f_get_sales(x))\n\n# Either of these statements will produce the same output\n\n# predict(fit,new_data = old_prices) \n# sapply(old_prices, function(x) f_get_poly_fit(x,fit))\n#-----------------------------------------------------------------\n# Use f_get_sales to get modeled demand at each price level\n#-----------------------------------------------------------------\nestimated_sales_new <- f_get_sales(26)\n#-----------------------------------------------------------------\n# Demonstrate the highest point on the curve\n#-----------------------------------------------------------------\nx_label  <- ('\\n Price')\ny_label  <- ('Ticket Sales \\n')\ntitle   <- ('Ticket Sales by Price')\nline_sales_polyb <- \n  ggplot2::ggplot(data  = sales, \n                  aes(x = price,\n                      y = sales))                     +\n  geom_point(size = 2.5,color = 'mediumseagreen')     +\n  scale_x_continuous(label = scales::dollar)          +\n  xlab(x_label)                                       + \n  ylab(y_label)                                       + \n  ggtitle(title)                                      +\n  graphics_theme_1                                    +\n  geom_line(color = \"mediumseagreen\")                 +                                         \n  stat_smooth(method = \"lm\", color = 'coral',\n              formula = y ~ x + poly(x, 2)-1, se = F) +\n  geom_hline(yintercept = 35.8151054, lty = 2)        +\n  geom_vline(xintercept = 29.21, lty = 2)\n#-----------------------------------------------------------------\n# Look for optimum price levels\n#-----------------------------------------------------------------\nestimated_prices <- seq(from = 25, to = 35, by = 1)\nestimated_sales <- sapply(estimated_prices,\n                          function(x) f_get_sales(x))\n\nestimated_revenue <- tibble::tibble(\n  \n  sales      = estimated_sales,\n  price      = estimated_prices,\n  revenue    = estimated_sales * estimated_prices,\n  totalSales = rev(cumsum(rev(sales)))\n)\n#-----------------------------------------------------------------\n# Look for optimum price levels\n#-----------------------------------------------------------------\nx <- 1\nrevenue <- list()\nwhile(x <= nrow(estimated_revenue)){\n  \n revenue[x] <- estimated_revenue[x,2] * estimated_revenue[x,4]\n x <- x + 1\n}\nestimated_revenue$totalRevenue <- unlist(revenue)"},{"path":"chapter6.html","id":"basics-of-forecasting","chapter":"6 Pricing and Forecasting","heading":"6.3 Basics of forecasting","text":"Forecasting fundamental setting prices, many different ways . However, subject can also get sophisticated. ’ll focus couple basic approaches forecasting confronted working club. , demonstrate approach problems, focus think . First, let’s create sample data. can also find data FOSBAAS package using FOSBAAS::past_season_data. can use functions build different conditions data set like simulate different situations.Now let’s take look data set created.Table 6.5: past seasons dataIn section, dive regression little rigorous fashion. , highly recommend purchasing book regression, current favorite “R Companion Applied Regression.” (John Fox 2019) John Fox Sanford Weisberg. convenient doesn’t dwell theory. also lots statistical terms need familiar gain proficiency regression. Often, important understand limitations tool using. non-exhaustive list terms might run across might include:Hypothesis testing55Normality56Independence57linearity58Validation59Homoskedacity vs Heteroskedacity60Autocorrelation61Multicolinearity62Interaction terms63Omitted Variable Bias64Transformations65Endogeneity66An unfortunate fact working sports data living non-parametric world rank order might important. critical underlying assumptions data need met. specific ways deal problems. However, clinical study, need go every test ensure model durable. hit high points move . need make sure aware deep ocean gets. won’t get heads.said, can approach forecasting several ways, two main types forecasting approaches typically deploy:Top-forecastsBottom-forecastsA top-forecast typically consists macro-level factors team payroll projected wins. forecasts help compare potential across entire league since can understand main components advance. Bottom-forecasts can vary granularity consist specific information marketing schedule specifics around scheduling game attractiveness specific market. example, might mean rivalry games attractive, can build model. Let’s start making simple bottom-forecast.First, let’s create treatment control group evaluate model’s efficacy. future chapters, ’ll perform tasks couple different packages go -depth applying modeling techniques data rigorously. following lines code create training test data set. example uses standard R functions.earlier examples, overfit models. didn’t split data testing training data. even partition data calibration set, ’ll stick straightforward training test set now.Now can build simple linear model training data set. select following values? Selecting values model can involved. now, let’s choose values look reasonable. Variable selection can involved, creating parsimonious models (simplest model captures variance) best practice. Also, keep mind dealing mixed data sets. regression examples find deal clean numerical data normally distributed. rarely case wild.following sections won’t exhaustive give crash course must look . also use realistic example find working club.standard summary model looks like :looking output? First, ’ll need hit books. ’ll need become familiar statistics speak.Estimate: coefficients equation produced regressionStd. Error: standard error represents average distance estimates fall regression line.t value: coefficient divided standard errorPr(>|t|): p value. tell variable significant. sure estimate falls within confidence interval?Residual standard error: larger number, less likely model helpful.Multiple R-squared: much variation explained modelF-statistic: model valid? Lower numbers badAbout 60% variance ticket sales explained simple model. good? especially consider standard error. anything can improve ? Let’s look regression diagnostics help us evaluate results. Additionally, can run plot(ln_mod_bu) get standard diagnostic plots.cut statistics brevity. summaries tend look like :Table 6.6: Summary stats modelThis OLS regression, certain assumptions need made underlying data. aren’t satisfied, model may problems. car package (Fox, Weisberg, Price 2022) great tools help evaluate models. ’ll use assess model .","code":"\n#-----------------------------------------------------------------\n# Generate past season's data\n#-----------------------------------------------------------------\nseason_2022 <- \n  FOSBAAS::f_build_season(seed1 = 3000, season_year = 2022,\n  seed2 = 714, num_games = 81, seed3 = 366, num_bbh = 5,\n  num_con = 3, num_oth = 5, seed4 = 309, seed5  = 25,\n  mean_sales = 29000, sd_sales = 3500\n )\n\nseason_2023 <- \n  FOSBAAS::f_build_season(seed1 = 755, season_year = 2023,\n  seed2 = 4256, num_games = 81, seed3 = 54, num_bbh = 6,\n  num_con = 4, num_oth = 7, seed4 = 309, seed5  = 25,\n  mean_sales = 30500, sd_sales = 3000\n )\n\nseason_2024 <- \n  FOSBAAS::f_build_season(seed1 = 2892, season_year = 2024,\n  seed2 = 714, num_games = 81, seed3 = 366, num_bbh = 9,\n  num_con = 2, num_oth = 6, seed4 = 6856, seed5  = 2892,\n  mean_sales = 32300, sd_sales = 2900\n )\n\npast_season <- rbind(season_2022,season_2023,season_2024)\n#-----------------------------------------------------------------\n# Build test and training set\n#-----------------------------------------------------------------\nsamp <- round(0.05 * nrow(past_season),0)\n\nset.seed(715)\nrows  <- sample(seq_len(nrow(past_season)), \n                        size = samp)\ntrain <- past_season[-rows, ]\ntest  <- past_season[rows, ]\n#-----------------------------------------------------------------\n# Linear model for ticket sales\n#-----------------------------------------------------------------\nln_mod_bu <- lm(ticketSales ~ promotion + daysSinceLastGame + \n                  schoolInOut + weekEnd + team , data = train)\nln_mod_bu_sum <- \ntibble::tibble(\n  st_error     = unlist(summary(ln_mod_bu)[6]),\n  r_square     = unlist(summary(ln_mod_bu)[8]),\n  adj_r_square = unlist(summary(ln_mod_bu)[9]),\n  f_stat       = unlist(summary(ln_mod_bu)$fstatistic[1]))#> \n#> Call:\n#> lm(formula = ticketSales ~ promotion + daysSinceLastGame + schoolInOut + \n#>     weekEnd + team, data = train)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -10994.7  -2468.7    126.9   2504.1   8969.3 \n#> \n#> Coefficients:\n#>                   Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)       33357.39    2929.22  11.388  < 2e-16 ***\n#> promotionconcert    127.09    1657.31   0.077 0.938953    \n#> promotionnone     -5358.02     988.93  -5.418 1.71e-07 ***\n#> promotionother    -3721.18    1375.10  -2.706 0.007390 ** \n#> daysSinceLastGame   288.13      42.69   6.750 1.53e-10 ***\n#> schoolInOutTRUE    4608.16    1037.41   4.442 1.47e-05 ***\n#> weekEndTRUE        6397.82     610.61  10.478  < 2e-16 ***\n#> teamATL            3135.03    3000.93   1.045 0.297417    \n#> teamBAL            4382.63    2925.33   1.498 0.135651    \n#> teamBOS           12751.52    3594.69   3.547 0.000484 ***\n#> teamCHC           13193.89    3565.58   3.700 0.000278 ***\n#> teamCIN            3659.22    3226.57   1.134 0.258101    \n#> teamCLE            2201.94    2906.84   0.758 0.449632    \n#> teamCOL            3057.71    3285.54   0.931 0.353142    \n#> teamCWS            3906.19    2985.27   1.308 0.192195    \n#> teamDET            3202.46    3267.54   0.980 0.328217    \n#> teamFLA            2833.12    2924.04   0.969 0.333750    \n#> teamHOU           10209.56    2947.13   3.464 0.000649 ***\n#> teamKAN            5246.46    3732.75   1.406 0.161403    \n#> teamLAA            9102.40    2888.86   3.151 0.001875 ** \n#> teamLAD           12150.54    3198.52   3.799 0.000192 ***\n#> teamMIN            1625.41    3708.86   0.438 0.661672    \n#> teamNYM            9879.64    3030.31   3.260 0.001306 ** \n#> teamOAK            1877.06    3741.10   0.502 0.616397    \n#> teamPHI            9450.28    3949.78   2.393 0.017645 *  \n#> teamSF             8185.73    3741.10   2.188 0.029813 *  \n#> teamTB             3212.49    3191.25   1.007 0.315305    \n#> teamTEX            4595.50    3069.58   1.497 0.135925    \n#> teamWAS            1036.66    3036.91   0.341 0.733192    \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3900 on 202 degrees of freedom\n#> Multiple R-squared:  0.6632, Adjusted R-squared:  0.6166 \n#> F-statistic: 14.21 on 28 and 202 DF,  p-value: < 2.2e-16"},{"path":"chapter6.html","id":"outliers-and-unusual-data-points","chapter":"6 Pricing and Forecasting","heading":"6.3.1 Outliers and unusual data points","text":"Linear regression sensitive outliers. Therefore, looking data points fall well outside average range always best. instance, games may rained . Maybe new promotion worked well. Something create unique data point two. Sometimes appropriate remove .appears one outlier:\nTable 6.7: Outliers\nQQ plot help ensure underlying assumptions around normality satisfied. demonstrates “95% confidence envelope.”part, points fall line. points stray , can see outlier (48) bottom. Extreme points can highly influence linear regression models. ’ll end removing point rerunning model.can run influence plot combine several graphs can see plot(ln_mod_bu) .bigger circle, larger Cooks Distance. Cooks distance mathematical definition think way identify influential points data.Several points outsized influence model. evaluate individually. may OK remove model values appear influenced exogenous factor account . might points influential? Perhaps rained dates depressed ticket sales. might reasonable exclude dates analysis.Table 6.8: Influential ObservationsLet’s remove observations.Table 6.9: Summary stats clean modelWe get modest improvement removing influential values. else can ? can also look interaction terms transform response. multiple ways handle . Additionally, may need transform data. case, don’t see pronounced curvature data.can also transform variables regression directly.Table 6.10: Summary stats log modelThis makes model worse. also makes challenging interpret. phenomenon isn’t uncommon data find working sports. aren’t dealing new medicines. dealing dollars cents. don’t precise. However, precision helps! Often business, best can. understand limitations done, ’ll still good shape comes time decide.Let’s go ahead apply model testing data set. First, well appear approximate ticket sales practice? Next, ’ll use predict function apply predictions test data observe close estimates fall reality.Table 6.11: Mean error percentageOn average, 2% mark. good? Maybe. depends trying accomplish. Let’s take look error graph.Overall, error relatively low. However, can see average error low, point estimates can vary considerably. Can apply model new data? believe . However, look predictors might improve model. Removing season tickets may help. Unfortunately, away season, blunt estimates tend . won’t gambling odds, may promotional schedule. need pull insignificant teams model able use make forecast. ’ll guess. ’ll often settle “good enough.”top-approach forecasting sales analyzing prices helpful certain circumstances many applications. However, following example cover bottom-approach.","code":"\n#-----------------------------------------------------------------\n# Outliers test\n#-----------------------------------------------------------------\nlibrary(car)\noutliers <- car::outlierTest(ln_mod_bu)\n#-----------------------------------------------------------------\n# QQPlot\n#-----------------------------------------------------------------\nqq <- car::qqPlot(ln_mod_bu, main=\"QQ Plot\")\n#-----------------------------------------------------------------\n# Influence plot\n#-----------------------------------------------------------------\nip <- car::influencePlot(ln_mod_bu, id.method=\"identify\", \n              main=\"Influence plot for our linear model \")\n#-----------------------------------------------------------------\n#  Remove outliers and view summary\n#-----------------------------------------------------------------\npast_season_clean <- past_season[-c(37,48,53,129,142,143),]\n\nln_mod_clean <- lm(ticketSales ~ promotion + daysSinceLastGame + \n                  schoolInOut + weekEnd + team , \n                  data = past_season_clean)\n#-----------------------------------------------------------------\n# Save our model\n# save(ln_mod_clean, file=\"ch6_ln_mod_clean.rda\")\n#-----------------------------------------------------------------\nln_mod_clean_sum <- \ntibble::tibble(\n  st_error     = unlist(summary(ln_mod_clean)[6]),\n  r_square     = unlist(summary(ln_mod_clean)[8]),\n  adj_r_square = unlist(summary(ln_mod_clean)[9]),\n  f_stat       = unlist(summary(ln_mod_clean)$fstatistic[1]))\n#-----------------------------------------------------------------\n# Transforming the data\n#-----------------------------------------------------------------\nsummary(pw_mod <- car::powerTransform(ln_mod_clean))\n#> bcPower Transformation to Normality \n#>    Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd\n#> Y1    3.0383        3.04       2.2665       3.8101\n#> \n#> Likelihood ratio test that transformation parameter is equal to 0\n#>  (log transformation)\n#>                            LRT df       pval\n#> LR test, lambda = (0) 71.19639  1 < 2.22e-16\n#> \n#> Likelihood ratio test that no transformation is needed\n#>                            LRT df       pval\n#> LR test, lambda = (1) 30.40168  1 3.5122e-08\n#-----------------------------------------------------------------\n# Directly transforming the data\n#-----------------------------------------------------------------\nln_mod_log <- \n  lm(log(ticketSales) ~ promotion + daysSinceLastGame + \n                        schoolInOut + weekEnd + team , \n                        data = past_season_clean)\nln_mod_log_sum <- \ntibble::tibble(\n  st_error     = unlist(summary(ln_mod_log)[6]),\n  r_square     = unlist(summary(ln_mod_log)[8]),\n  adj_r_square = unlist(summary(ln_mod_log)[9]),\n  f_stat       = unlist(summary(ln_mod_log)$fstatistic[1])\n)\n#-----------------------------------------------------------------\n# Apply predictions to our test data set. \n#-----------------------------------------------------------------\ntest$pred_tickets <- predict(ln_mod_clean,\n                             newdata = test)\ntest$percDiff <- \n  (test$ticketSales - test$pred_tickets)/test$ticketSales\nmean_test <- mean(test$percDiff)\n#-----------------------------------------------------------------\n# Apply predictions to our test data set. \n#-----------------------------------------------------------------\ntest_line <- test\ntest_line$order <- seq(1:nrow(test))\ntest_line <-   tidyr::pivot_longer(test_line,\n                      cols = c('ticketSales','pred_tickets'),\n                      values_transform = list(val = as.character))\nx_label  <- ('\\n Selected Game')\ny_label  <- ('Ticket Sales \\n')\ntitle   <- ('Ticket forecasts vs. Actuals')\nline_est <- \n  ggplot2::ggplot(data      = test_line, \n                  aes(x     = order,\n                      y     = value,\n                      color = name))             +\n  geom_point(size = 2.5)                         +\n  geom_line()                                    +\n  scale_color_manual(values = palette)           +\n  scale_y_continuous(label = scales::comma)      +\n  xlab(x_label)                                  + \n  ylab(y_label)                                  + \n  ggtitle(title)                                 +\n  graphics_theme_1"},{"path":"chapter6.html","id":"analyzing-a-schedule-and-ranking-games","chapter":"6 Pricing and Forecasting","heading":"6.3.2 Analyzing a schedule and ranking games","text":"much impact schedule team’s financial success? factors considered? sport like professional baseball, ticket sales crucial component revenue generation. Almost every additional revenue stream derived sales. Let’s take look schedule data begin frame problem. like take ensemble approach problems. case, thing two different ways. First, evaluate games based attractiveness fans. bottom-approach use secondary market purchases estimate game’s attractiveness.","code":""},{"path":"chapter6.html","id":"utilizing-transaction-data","chapter":"6 Pricing and Forecasting","heading":"6.3.2.1 Utilizing transaction data","text":"always, let’s begin understanding underlying data structure.Table 6.12: Secondary market data structureTable 6.13: Secondary market data structureNow can look game see many transactions happened spent. can join table original season data table see can use regression accurately predict games commanded higher prices secondary market. data wasn’t created linked, alter . ’re going results look realistic.’ll create scaled ticket sales coefficient add mean secondary price.Now let’s build model data.Table 6.14: Linear model summary statisticsThis model works well. explains 90% variance secondary market prices. course, model overfitted, get point. Let’s assume put rigor example get application.","code":"\n#-----------------------------------------------------------------\n# Secondary market data, manifest, and sales data\n#-----------------------------------------------------------------\nsm  <- FOSBAAS::secondary_data\nman <- FOSBAAS::manifest_data\nsea <- FOSBAAS::season_data\nsea$gameID <- seq(1:nrow(sea))\n#-----------------------------------------------------------------\n# Secondary market data, manifest, and sales data\n#-----------------------------------------------------------------\nhead(sm)[c(1,2,7,9)]\n#-----------------------------------------------------------------\n# Secondary sales by section\n#-----------------------------------------------------------------\nsm_man <- left_join(sm,man, by = 'seatID')\n\navg_price_comps <- \n  sm_man %>% dplyr::select(gameID,price,singlePrice,tickets) %>%\n             na.omit()                                       %>%\n             dplyr::group_by(gameID)                         %>%\n             dplyr::summarise(meanSec   = mean(price),\n                       meanPri   = mean(singlePrice),\n                       tickets   = sum(tickets))\n#-----------------------------------------------------------------\n# Adjust secondary to reflect primary\n#-----------------------------------------------------------------\nsea_adj  <- left_join(sea,avg_price_comps, by = \"gameID\") \nadj_coef <- scale(sea_adj$ticketSales)\nsea_adj$meanSecAdj <- sea_adj$meanSec + (adj_coef * 10)\n#-----------------------------------------------------------------\n# Adjust secondary to reflect primary\n#-----------------------------------------------------------------\nln_mod_sec <- \n  lm(meanSecAdj ~ promotion + daysSinceLastGame + \n                        schoolInOut + weekEnd + team , \n                        data = sea_adj)\nln_mod_sec_sum <- \ntibble::tibble(\n  st_error     = unlist(summary(ln_mod_sec)[6]),\n  r_square     = unlist(summary(ln_mod_sec)[8]),\n  adj_r_square = unlist(summary(ln_mod_sec)[9]),\n  f_stat       = unlist(summary(ln_mod_sec)$fstatistic[1])\n)"},{"path":"chapter6.html","id":"applying-our-models-to-a-new-data-set","chapter":"6 Pricing and Forecasting","heading":"6.3.3 Applying our models to a new data set","text":"Now model constructed, apply new data? ? First, let’s evaluate model’s results create event score can used think promotion schedule. can .new data uses seed2 = 714. type View(f_build_season), can see seed2 controls teams selected schedule. happens use different seed2? might get factor levels didn’t anticipate. model unable make prediction values. ? easiest thing remove offending factor levels. also make analogs substitute like-factor.Table 6.15: 2025 season dataNow predictions around ticket sales, can build event scores . simple part.Table 6.16: 2025 season event scoresWe can see groupings games similar levels attractiveness. Although many games appear attractive others, may also priced higher. Based demand attractiveness, can progressively price games highest anticipated demand receiving higher prices.Let’s cluster like games together based event scores. ’ll use kmeans function group games. ’ll see kmeans getting used quite bit. relatively simple understand good job discriminating numerical data sets.Now let’s look summary stats groups.Table 6.17: Summary stats clusterWe now several groups games can evaluate. can use groups various ways. One way look games need help. Let’s see promotion impacted sales. ’ll chapter 8. now data set gives good idea attractive game every game. However, pricing much complex practice. Reference pricing plays part might price. main idea price games based attractiveness. using attractiveness proxy demand.can improve bottom-forecast? can begin layer data onto . data looking ? several pieces data can use:number season tickets sold anticipate sellingAvailable inventory game allocatedprices paid volume ticket classYou likely imperfect conditions setting prices. Additionally, model compelling. improve model? practice, prefer ensemble approach. can use data access liquid market, like secondary market, many transactions different prices.","code":"\n#-----------------------------------------------------------------\n# Create data for a new season\n#-----------------------------------------------------------------\nseason_2025 <- \n  FOSBAAS::f_build_season(seed1 = 755, season_year = 2025,\n  seed2 = 714, num_games = 81, seed3 = 366, num_bbh = 5,\n  num_con = 3, num_oth = 7, seed4 = 366, seed5  = 1,\n  mean_sales = 0, sd_sales = 0\n )\n#-----------------------------------------------------------------\n# Apply model output to new data set\n#-----------------------------------------------------------------\nseason_2025$predTickets <- predict(ln_mod_clean,\n                             newdata = season_2025)\nseason_2025$predPrices  <- predict(ln_mod_sec,\n                             newdata = season_2025)\n#-----------------------------------------------------------------\n# Build event scores\n#-----------------------------------------------------------------\nseason_2025$eventScoreA <- \n  as.vector(scale(season_2025$predTickets) * 100)\nseason_2025$eventScoreB <- \n  as.vector(scale(season_2025$predPrices) * 100)\nseason_2025$eventScore  <- \n   season_2025$eventScoreA + season_2025$eventScoreB\n\nseason_2025 <- season_2025[order(-season_2025$eventScore),]\n#-----------------------------------------------------------------\n# Observe differences in event scores\n#-----------------------------------------------------------------\nlibrary(ggplot2)\nseason_2025$order <- seq(1:nrow(season_2025))\n\nx_label  <- ('\\n Game')\ny_label  <- ('Event Scores \\n')\ntitle   <- ('Ordered event scores')\nline_est_es <- \n  ggplot2::ggplot(data  = season_2025, \n                  aes(x = order,\n                      y = eventScore))             +\n  geom_point(size = 1.3,color = 'dodgerblue')      +\n  geom_line()                                      +\n  scale_color_manual(values = palette)             +\n  scale_y_continuous(label = scales::comma)        +\n  xlab(x_label)                                    + \n  ylab(y_label)                                    + \n  ggtitle(title)                                   +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Kmeans clustering on event scores\n#-----------------------------------------------------------------\nset.seed(715)\nclusters <- kmeans(season_2025$eventScore,6)\nseason_2025$cluster <- clusters$cluster\n#write.csv(season_2025,'season_2025.csv',row.names = F)\n#-----------------------------------------------------------------\n# Summary statistics\n#-----------------------------------------------------------------\nlibrary(dplyr)\nseason_summary <- season_2025                            %>% \n                  group_by(cluster)                      %>%\n                  summarise(mean   = mean(eventScore),\n                            median = median(eventScore),\n                            sd     = sd(eventScore),\n                            n      = n())                %>%\n                  arrange(desc(mean))\nknitr::kable(season_summary,caption = \"Summary stats by cluster\",\n             align = 'c',format = \"markdown\",padding = 0)"},{"path":"chapter6.html","id":"leveraging-qualitative-data","chapter":"6 Pricing and Forecasting","heading":"6.4 Leveraging qualitative data","text":"Leveraging surveys pricing support may may utility. two principal methodologies deployed practice:Van Westendorp analysisConjointBoth techniques supported survey tools Qualtrics. Conjoint analysis can complex, understanding well outside scope book. Let’s look Van Westendorp survey talk can helpful.First, need create survey data.Table 6.18: Survey resultsLike many statistical tools find, package analyzing Van Westendorp data. Let’s start manually. ’ll little prep work. First, let’s build new dataframe slightly different terminology.Ecdf function Hmisc package give coordinates cumulative distribution. ’ll use reshape2 (Wickham 2020) package melt data frame. package helps pivot data, results identical using tools tidyr package already seen.Now ready view graph.can also use pricesensitivitymeter package produce analysis quickly.price_sensitivity object makes easy explore data.Table 6.19: Price sensitivity metricsYou can also produce plot already created. plotting mechanism leverages ggplot2 can override look. now qualitative data price sensitivity. doesn’t represent willingness pay, help us conceptualize range prices might appropriate.","code":"\n#-----------------------------------------------------------------\n# Create Van Westendorp survey data\n#-----------------------------------------------------------------\nvw_data <- data.frame(matrix(nrow = 1000, ncol = 6))\nnames(vw_data) <- c('DugoutSeats', 'PriceExpectation', \n                    'TooExpensive', 'TooCheap', \n                    'WayTooCheap', 'WayTooExpensive')\nset.seed(715)\nvw_data[,1] <- 'DugoutSeats'\nvw_data[,2] <- round(rnorm(1000,100,10),0)\nvw_data[,3] <- round(rnorm(1000,130,20),0)\nvw_data[,4] <- round(rnorm(1000,60,15),0)\nvw_data[,5] <- round(rnorm(1000,50,10),0)\nvw_data[,6] <- round(rnorm(1000,160,20),0)\n#-----------------------------------------------------------------\n# Empirical cumulative distribution function\n#-----------------------------------------------------------------\nlibrary(Hmisc)\ndat <- data.frame(\n    \"toocheap\"     = vw_data$WayTooCheap,\n    \"notbargain\"   = vw_data$TooExpensive,\n    \"notexpensive\" = vw_data$TooCheap,\n    \"tooexpensive\" = vw_data$WayTooExpensive\n)\na <- Ecdf(dat$toocheap,what=\"1-F\",pl=F)$y[-1]\nb <- Ecdf(dat$notbargain, pl=F)$y[-1]\nc <- Ecdf(dat$notexpensive,what = \"1-F\", pl=F)$y[-1]\nd <- Ecdf(dat$tooexpensive,pl=F)$y[-1]\n#-----------------------------------------------------------------\n# Build data set for creating graphic\n#-----------------------------------------------------------------\nlibrary(reshape2)\necdf1 <- data.frame(\n  \"variable\"  = rep(\"toocheap\",length(a)),\n  \"ecdf\"      = a,\n   \"value\"    = sort(unique(dat$toocheap)))\necdf2 <- data.frame(\n  \"variable\"  = rep(\"notbargain\",length(b)),\n  \"ecdf\"      = b,\n  \"value\"     = sort(unique(dat$notbargain),decreasing = T))\necdf3 <- data.frame(\n  \"variable\"  = rep(\"notexpensive\",length(c)),\n   \"ecdf\"     = c,\n   \"value\"    = sort(unique(dat$notexpensive),decreasing = T))\necdf4 <- data.frame(\n  \"variable\"  = rep(\"tooexpensive\",length(d)),\n  \"ecdf\"      = d,\n  \"value\"     = sort(unique(dat$tooexpensive)))\ndat2 <- rbind(ecdf1,ecdf2,ecdf3,ecdf4)\ndat  <- melt(dat)\ndat  <- merge(dat,dat2,by=c(\"variable\",\"value\"))\n#-----------------------------------------------------------------\n# Graph the results\n#-----------------------------------------------------------------\nrequire(ggplot2)\nrequire(scales)\nrequire(RColorBrewer)\n\nPaired     <- RColorBrewer::brewer.pal(4,\"Paired\")\n\ng_xlab     <- '\\n prices'\ng_ylab     <- 'Responses  \\n'\ng_title    <- 'Dugout Price Value Perception\\n'\n\nvw_gaphic <- \nggplot(dat, aes(value, ecdf, color=variable))     + \n      annotate(\"rect\", xmin = 55, xmax = 146, \n                     ymin = 0,  ymax = 1,\n             alpha = .4, fill = 'coral')          +\n    geom_line(size = 1.2) +\n    scale_color_manual(values = Paired,\n                       name = 'Value Perception') + \n    xlab(g_xlab)                                  + \n    ylab(g_ylab)                                  + \n    ggtitle(g_title)                              + \n    scale_y_continuous(labels = percent)          +\n    scale_x_continuous(labels = dollar)           +\n    coord_cartesian(xlim = c(0,220),ylim= c(0,1)) +\n    geom_hline(yintercept = .5,\n               lty=4,\n               alpha = .5)                        +\n    graphics_theme_1\n#-----------------------------------------------------------------\n# Duplicate work with a library\n#-----------------------------------------------------------------\nlibrary(pricesensitivitymeter)\nprice_sensitivity <- psm_analysis(\n                           toocheap        = \"WayTooCheap\",\n                           cheap           = \"TooCheap\",\n                           expensive       = \"TooExpensive\",\n                           tooexpensive    = \"WayTooExpensive\",\n                           data            = vw_data,\n                           validate        = TRUE)\n#-----------------------------------------------------------------\n# Explore price sensitivity data\n#-----------------------------------------------------------------\nps <- \n  tibble::tibble(lower_price = price_sensitivity$pricerange_lower,\n                 upper_price = price_sensitivity$pricerange_upper)"},{"path":"chapter6.html","id":"implementing-revenue-management-strategies-in-sports","chapter":"6 Pricing and Forecasting","heading":"6.5 Implementing revenue management strategies in sports","text":"techniques becoming increasingly commoditized. mechanisms forecasting pricing well understood widely deployed. Additionally, ticketing platforms markets already working together enable instantaneously deploying changes across multiple markets. already covered primary considerations, didn’t talk :Brand consumer equitySetting initial pricesAutomatically adjusting pricesControlling cannibalizationPublic relations perceptionThe perceived fairness raising pricesLowering prices can negatively impact perception brand equity long term. Concepts prospect theory explain changes utility asymmetric gains losses. (Tudor Bodea 2012) someone feels missed pricing policy, deleterious impacts. Additionally, price changes may perceived fair. Keep behavioral economics mind deploy pricing strategy.Initial prices challenging set, reference prices potent mechanisms. develop new product (premium seating area), consider value proposition carefully. -market competitive intelligence may useful . However, don’t rely pro forma.evaluate prices deploy ? Setting internal mechanism sales, marketing, operations best. Pricing one single-vital sales revenue influencers. ’ll want right people table. ’ll also want make sure right technology deployed. price get market? updated website? policies much little specific tickets may cost?Additionally, inventory control protocols may influence decisions. raise prices one section, impact demand another? fundamental component pricing. price levels linked. can explain links systems equations. multiple ways approach problems.Finally, people respond prices? depend number factors demand, success, markets. Exploiting opportunities sometimes appropriate. Qualitative research may help. can deploy conjoint surveys research tools help gauge fans respond products changes price. Taking ensemble approach pricing recommended approach.","code":""},{"path":"chapter6.html","id":"key-concepts-and-chapter-summary-5","chapter":"6 Pricing and Forecasting","heading":"6.6 Key concepts and chapter summary","text":"Getting pricing right key component integrated marketing mix strategy. Pricing several quantitative qualitative features. covered several concepts:Inventory controlMechanical price controlsMore -depth regressionForecasting schedule analysisEvent scoringQualitative researchWe skimmed broad subject, know, understand basics complex subject.Inventory can complex, people may perceive inventory different ways. Understanding differences inventory essential building discreet-choice model.mechanics determining prices tend rely mathematical models. models can adjusted promote sales maximize revenue generation. goals may inclusive exclusive.Regression can rigorous. confident results, ’ll need spend time methodically evaluating model. Get book regression make sure understand enough use correctly.Schedules can play outsized role revenue generation. walked evaluate schedule. useful marketing wants build promotions games less demand.covered use Van Westendorp analysis help us understand people perceive inventory prices.","code":""},{"path":"chapter7.html","id":"chapter7","chapter":"7 Lead Scoring","heading":"7 Lead Scoring","text":"someone told pay $50,000 sold four season tickets someone next twenty-four hours? ? tickets less $50,000, always buy tickets collect surplus. increase likelihood sold four season tickets?look lapsed purchasersI look abandoned carts websiteI call individuals yet renew season ticketsI beg family friends purchaseI call ticket broker ask buy ticketsThere lots tactics deploy make sure picking low-hanging fruit. However, unlikely ever get point pick telephone begin calling phone numbers. can approach problem analytically?Lead scoring fundamental sales campaigns. can qualify leads many ways, goal always . ordering leads efficiently sales efforts can maintain greater efficiency. Warm leads critical, someone yet interact brand, likely less efficient use time. Working marketing always reminds one-hundred--thirty-year-old quote:“Half money spend advertising wasted; trouble don’t know half.”— John WanamakerLead scoring assist evaluating direct sales efforts help maximize ROI context. However, several considerations. First, assess lead?Customer Lifetime Value?likelihood purchasing sales cycleRecent, frequency, monetary valueAccessing lead also judgment call predicated sales marketing teams incentivized. Always consider compensation packages. Compensation designed incentivize behavior. However, may need optimized achieve organizational goals.","code":""},{"path":"chapter7.html","id":"recency-frequency-and-monetary-value","chapter":"7 Lead Scoring","heading":"7.1 Recency, frequency, and monetary value","text":"RFM scores often described poor-man’s analytic technique. basic premise score sales candidates along three dimensions build lists comprising cohorts highest aggregate scores. might work practice?Let’s assemble ad hoc data set demonstrate RFM scores might work.added three columns customer file:long last interaction happened daysHow many interactions happenedHow much spent customerTable 7.1: Prepped RFM dataThere many ways apply scores . take similar approach score events add one extra step. RFM scores traditionally built one five, five highest.Now can apply function data set. generalize . also use apply function. sort exercise, aren’t concerned extreme efficiency. want demonstrate mechanics. ’ll use lists loops accomplish task. repeat exercise often, better practices daisy chaining loops. Try copy paste three times. ’ll make errors.Now customer RFM score can used build campaigns. First, let’s subset group campaign. ’ll select customers interacted recently, frequently tended spend .Table 7.2: Top prospects campaignRFM scores can helpful segmentation schemes lead scoring. However, better ways accomplish goal. Lead scoring close get discussing Customer Relationship Management (CRM) book. also fundamental component CRM. following examples take sophisticated example use analytics framework make regression machine learning easier.","code":"\n#-----------------------------------------------------------------\n# RFM data\n#-----------------------------------------------------------------\nlibrary(dplyr)\ndemo_data <- FOSBAAS::demographic_data[,c(1,4)]\nset.seed(44)\ndemo_data <- demo_data %>%\nmutate(\nlastInteraction = abs(round(rnorm(nrow(demo_data),50,30),0)),\ninteractionsYTD = abs(round(rnorm(nrow(demo_data),10,5),0)),\nlifetimeSpend   = abs(round(rnorm(nrow(demo_data),10000,7000),0))\n)\n#-----------------------------------------------------------------\n# Function to calculate FRM scores\n#-----------------------------------------------------------------\ndemo_data$Recency       <- -scale(demo_data$lastInteraction)\ndemo_data$Frequency     <- scale(demo_data$interactionsYTD)\ndemo_data$MonetaryValue <- scale(demo_data$lifetimeSpend)\n# Produce quantiles for each scaled value\nr_quant <- unname(quantile(demo_data$Recency,\n                           probs = c(.2,.4,.6,.8)))\nf_quant <- unname(quantile(demo_data$Recency,\n                           probs = c(.2,.4,.6,.8)))\nm_quant <- unname(quantile(demo_data$Recency,\n                           probs = c(.2,.4,.6,.8)))\n# Function to evaluate RFM score\nf_create_rfm <- function(quantList,number){\n  \n  if(number <= quantList[[1]]){'1'}\n    else if(number <= quantList[[2]]){'2'}\n      else if(number <= quantList[[3]]){'3'}\n        else if(number <= quantList[[4]]){'4'}\n          else{'5'}\n}\n#-----------------------------------------------------------------\n# Create final RFM values\n#-----------------------------------------------------------------\nvalue <- list()\nj     <- 1\nfor(i in demo_data$Recency){\n  value[j] <- f_create_rfm(r_quant,i)\n  j <- j + 1\n}\ndemo_data$r_val <- unlist(value)\n#-----------------------------------------------------------------\nvalue <- list()\nj     <- 1\nfor(i in demo_data$Frequency){\n  value[j] <- f_create_rfm(f_quant,i)\n  j <- j + 1\n}\ndemo_data$f_val <- unlist(value)\n#-----------------------------------------------------------------\nvalue <- list()\nj     <- 1\nfor(i in demo_data$MonetaryValue){\n  value[j] <- f_create_rfm(m_quant,i)\n  j <- j + 1\n}\ndemo_data$m_val <- unlist(value)\n#-----------------------------------------------------------------\n\ndemo_data$RFM <- paste(demo_data$r_val,\n                       demo_data$f_val,\n                       demo_data$m_val, sep = '')\n#-----------------------------------------------------------------\n# Create final RFM values\n#-----------------------------------------------------------------\ntop_prospects <- subset(demo_data,demo_data$RFM == '555')"},{"path":"chapter7.html","id":"scoring-season-ticket-holders-on-their-likelihood-to-renew","chapter":"7 Lead Scoring","heading":"7.2 Scoring season ticket holders on their likelihood to renew","text":"advancements hardware software, lead scoring become commodity. can leverage several techniques, including random forests, gradient boosting, logistic regression, even deep learning tool Tensorflow, without getting penalized time costs. Take minute appreciate fact. amazing. Amazon, Google, others improving process AWS GCP. example demonstrate machine learning applied real-world problems. ’ll frame problem around common question asked every year every club sports.ticket sales service manager wants understand accounts less likely renew season tickets.’ll use mlr3 library (Bischl et al. 2021) demonstrate several machine-learning algorithms. package mlr3 similar caret (Kuhn 2022), refactored tidymodels (Kuhn Wickham 2022). Caret great library sought create unifying API scores R libraries. ’ll look tidymodels next chapter. Mlr3 reminiscent scikit-learn Python. like using Python analysis, mlr3 feel familiar.can call functions directly, library makes process much easier. lack consistent frameworks R one language’s biggest drawbacks. Additionally, mlr3 excellent documentation 67.Using framework also comes issues.’ll learn , adds complexity task.Errors (especially beta versions) can frustrating troubleshootThey can work slowly libraries used taskHowever, ’ll better leveraging framework long run. frameworks make essential tasks machine learning much systematic repeatable, especially comes benchmarking.Let’s also discuss package data.table (Dowle Srinivasan 2021) . data.table powerful tool underlies many popular packages. many uses, can confusing dplyr (Wickham, François, et al. 2022). Nevertheless, one pillars holds much R universe. using Python, datatable remind pandas (team 2020). working large data sets, datatable helpful learn.","code":""},{"path":"chapter7.html","id":"implementing-a-lead-scoring-project","chapter":"7 Lead Scoring","heading":"7.2.1 Implementing a lead-scoring project","text":"random forest excellent tool classification can forecast two classes. Logistic regression typically used binary classes (renewed, renew) probably first step. However, forms logistic regression handle multi-class problems. ’ll take look tools. practice, random forest handles wide variety issues face club well.Missing data orphan cases can make tasks extremely frustrating. guarantee model converge. stated earlier time spent organizing data. ’ll much happier put hours getting data proper spot. fun part exercises modeling. However, ends part spend shortest time working . hope high worth lows .Additionally, follow process outlined chapter 4. ’ll demonstrate isn’t managerial B.S. Structuring projects critical working teams distributed fashion. Let’s remind basic steps want follow:Define measurable goal hypothesisData collectionModel dataEvaluate resultsCommunicate resultsDeploying results","code":""},{"path":"chapter7.html","id":"defining-our-goal","chapter":"7 Lead Scoring","heading":"7.2.1.1 Defining our goal","text":"going use multiple years season ticket holder renewal data. also problem statement:need identify season ticket accounts less likely renew.output score can used compare customers one another. ’ll need look features predict whether someone likely renew tickets. feature might ticket usage tenure. don’t know.rub can’t help improve prospects someone might renew. levers can pull? Building helpful model predicts renewals finance one thing, trying impact renewals sales. ’ll need think carefully insight gather. may find something didn’t know looking . perspective, success can gauged several ways.","code":""},{"path":"chapter7.html","id":"understanding-the-data-set","chapter":"7 Lead Scoring","heading":"7.2.1.2 Understanding the data set","text":"data set includes several features, including account, whether renewed, several traits related season tickets. data located FOSBASS package. can see data created section 2.2.data set contains several years data. looks like something find club, behave appropriately. Let’s take look structure data:Table 7.3: Dataset evaluating likelihood renewThis data relatively simple. several columns appear helpful. data set also already clean, let’s refer section 4.3.1 chapter 4 ensure cover bases. know data good shape, still deal significant issues face running operation data.sort analysis mechanism helpful solve problem?data structured formatted can use ?missing values systematic issues cause problem?Question one easy. can refer chart section 4.4. example calculating future numerical values.question two, different analysis techniques require formats. can see couple columns categorical. Let’s go ahead create two data sets. ’ll dummy code categorical data one data set contains numerical values.Table 7.4: Numeric data set regressionFor question three, can little preparation standard data set. ’ll eliminate columns won’t use turn response variable factor. know isn’t missing data, skip part. already covered deal .terms prep work, ready go. chapter designed cover data modeling detail, optional anyway. also didn’t worry data collection. data might interesting ?Additionally, like briefly cover topic deserves attention give. Maps. Geography essential sales, marketing, corporate sponsorship groups. context, ’ll see geography impact sales marketing strategy chapter.","code":"\n#-----------------------------------------------------------------\n# access renewal data\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\nlibrary(dplyr)\nmod_data <- FOSBAAS::customer_renewals\n#-----------------------------------------------------------------\n# Dummy code and alter data frame for all numeric input\n#-----------------------------------------------------------------\nd1 <- as.data.frame(psych::dummy.code(mod_data$corporate))\nd2 <- as.data.frame(psych::dummy.code(mod_data$planType))\n\nmod_data_numeric <- dplyr::select(mod_data,ticketUsage,\n                                        tenure,spend,tickets,\n                                        distance,renewed) %>%\n                    dplyr::bind_cols(d1,d2)\n#-----------------------------------------------------------------\n# Prepare the data for analysis\n#-----------------------------------------------------------------\nmod_data$renewed   <- factor(mod_data$renewed)\nmod_data$accountID <- NULL\nmod_data$season    <- NULL"},{"path":"chapter7.html","id":"understanding-geography-and-building-maps","chapter":"7 Lead Scoring","heading":"7.2.1.2.1 Understanding geography and building maps","text":"Geography plays prominent role selling tickets sporting events. importance role especially true sports games, baseball. Coupling digital physical assets club may make less critical holistically. However, geography remain crucial point consideration ticket sales.R isn’t best way , easy accomplish analysis platform. GIS large field outside book’s scope. However, R multiple libraries devoted maps, good choice looking something high-quality graphics. map created using stamen maps68, open-source tool can used various projects.maps better, easy execute can give much insight get sophisticated product ArcGIS 69. R also makes easy access google APIs, allowing Geocode addresses perform many interesting tasks geographic data. can find multitude informative demos online.Using county names can create maps look like following. one took lot data wrangling used maps (Brownrigg 2021) package, output looks nice. underlying data identical.","code":"\n#-----------------------------------------------------------------\n# Using R to visualize geographic data\n#-----------------------------------------------------------------\nlibrary(ggmap)\nlibrary(ggplot2)\n\ndemos <- FOSBAAS::demographic_data\ndemos <- demos[sample(nrow(demos), 5000), ]\n\nmap_data <- subset(demos,demos$longitude >= -125 & \n                   demos$longitude <= -67 &\n                   demos$latitude >= 25.75 & \n                   demos$latitude <= 49)\nus <- c(left = -91, bottom = 32, right = -80, top = 38)\nmap <- get_stamenmap(us, zoom = 6, maptype = \"toner-lite\") %>% \n       ggmap() \n\ngeographic_vis <- \n  map +\n  geom_point(data = map_data, \n             mapping = aes(x = longitude, y = latitude),\n             size = .2,alpha = .5, color= 'dodgerblue')\n#-----------------------------------------------------------------\n# Using R to visualize geographic data\n#-----------------------------------------------------------------\nlibrary(maps)\nlibrary(tidyr)\n\nfips <- \nmaps::county.fips %>%\n  as_tibble       %>% \n  extract(polyname, \n          c(\"region\", \"subregion\"), \"^([^,]+),([^,]+)$\") \n\ncounty_data <- \nggplot2::map_data(\"county\") %>% \n  left_join(fips)           %>%\nmutate(county = paste(region,subregion, sep = ',')) \n\n\ncustomers <- left_join(customers,county_data, by = 'county')\n\ncounts    <- customers %>% group_by(county) %>%\n                           summarise(accounts = n())\n\ncounty_data <- county_data %>% left_join(counts,by='county') \n\nstate_data           <- map_data(\"state\") \nmain_color           <- 'steelblue4'\ncounty_data$accounts <- ifelse(is.na(county_data$accounts) == T,\n                               1,\n                               county_data$accounts)\n\n# adjust number of accounts per county\ncorrection <- county_data %>% select(county,accounts)                       %>%\n                              group_by(county)                              %>%\n                              mutate(adjustment = n())                      %>%\n                              unique()                                      %>%\n                              summarise(adj_accounts = accounts/adjustment) %>%\n                              select(county,adj_accounts)\n\ncorrection$adj_accounts <- ifelse(correction$adj_accounts <= 1,\n                                  1,\n                                  correction$adj_accounts)\n\ncounty_data <- county_data %>% left_join(correction, by = 'county')\n\nregion_map <- \n      \n  county_data %>% \n  ggplot(aes(long, lat, group = group))                                 +\n  geom_polygon(aes(fill=log10((adj_accounts))), \n               color=main_color,size = 0)                               +\n  geom_polygon(data = state_data ,color=\"white\",\n               size = .01,fill = 'transparent')                         +\n  coord_map(xlim = c(-91,-79), ylim = c(31,38.5))                       +\n #scale_fill_viridis(option=\"inferno\")                                  + \n  scale_fill_gradient(low = main_color ,high = 'white')                 +\n  labs( x = '', y ='',\n        title = \"Nashville ticket purchasers\",\n        subtitle = \"Total accounts by county\",\n        caption  = \"purchaser density\")                                 +\n   theme(plot.caption = element_text(hjust = 0, \n                                     face= \"italic\",color = \"grey90\"),\n         plot.title.position = \"plot\", \n         plot.caption.position =  \"plot\",\n         plot.subtitle = element_text(color = \"grey90\"))                +\n  graphics_theme_1                                                      +\n  theme(rect = element_rect(fill = main_color ),\n        axis.text.x  = element_blank(),  \n        axis.text.y  = element_blank(),\n        panel.grid.major  = element_line(colour = main_color ),  \n        panel.grid.minor  = element_line(colour = main_color ), \n        panel.background  = element_rect(fill = main_color , \n                                         colour = main_color ), \n        plot.background   = element_rect(fill = main_color , \n                                         colour = main_color ),\n        legend.position=\"none\")"},{"path":"chapter7.html","id":"model","chapter":"7 Lead Scoring","heading":"7.2.1.3 Model the data","text":"use mlr3 framework , essential understand need use one frameworks. can call functions directly. mlr3 descends mlr, (Bischl et al. 2021) built modern framework. ’ll want begin downloading library poking around 70.downloading packages need, can go ahead get started task modeling data. want determine likely specific season ticket holder renew. Make sure data set doesn’t contain characters. ’ll need dummy code change factors.classification problem, mlr3 follows specific pattern building model. data placed object called task:input straightforward. name object, tell data use, give target column, result. case, “r” indicates account renewed past.task, want select learner wish use. MLR works several libraries. case, like apply random forest data use ranger (Wright, Wager, Probst 2022) package. Ranger powerful library includes lots tools can leverage build random forest models. can add parameters ranger function library following code chunk.can now build test training data set. reminder, also use three-way data partition calibration data set. However, optional. ’ll see better way following chapter.Training model simple. First, ’ll use learner object point task rows identified training data set.learner contains several valuable pieces data generated. can access different ways.Table 7.5: Learner outputThis model doesn’t great job judging prediction error. Let’s take look holdout sample:several ways consider model’s accuracy, interpretation can confusing. first thing look confusion matrix. confusion matrix simple way gauge well model predictions performed known values.confusion matrix demonstrates often model correct incorrect response. case, model performing better. serve better random guess? many metrics can extract model.case, model 0.8208345 percent accurate. , first, let’s visualize results.simple models, mlr3::autoplot() function includes several graphs built ggplot2, means can apply themes . first, let’s explore data little . can access predictions command learner_ranger_rf\\(model\\)predictions. probabilities may helpful depending .can compare models one another couple different ways. compare random forest model model predicts classifier. doesn’t use parameters fed model.classification error better random forest, significant amount. classification errors resampling rerunning models.can also see generate lift model looking ROC curve.data? can now use model predict likely person renew ticket. However, can see model doesn’t perform well like. options terms proceeding:Tune model parameters try improve itTry different modelGet dataLet’s try couple different models attempt tune models improve accuracy.","code":"\n#-----------------------------------------------------------------\n# Download and install the mlr3 libraries\n#-----------------------------------------------------------------\nlibrary(\"mlr3\")         #  install.packages(\"mlr3viz\")\nlibrary(\"mlr3learners\") #  install.packages(\"mlr3learners\")\nlibrary(\"mlr3viz\")      #  install.packages(\"mlr3viz\")\nlibrary(\"mlr3tuning\")   #  install.packages(\"mlr3tuning\")\nlibrary(\"paradox\")      #  install.packages(\"paradox\")\n#-----------------------------------------------------------------\n# Recode response to a factor\n#-----------------------------------------------------------------\nmod_data_numeric$renewed <- factor(mod_data_numeric$renewed)\n#-----------------------------------------------------------------\n# Build task\n#-----------------------------------------------------------------\ntask_mod_data <- TaskClassif$new(id       = \"task_renew\", \n                                 backend  = mod_data_numeric, \n                                 target   = \"renewed\", \n                                 positive = \"r\")\n# Add a task to the task dictionary\nmlr_tasks$add(\"task_renew\", task_mod_data)\n#-----------------------------------------------------------------\n# Define learner\n#-----------------------------------------------------------------\nlearner_ranger_rf <- lrn(\"classif.ranger\",\n                         predict_type = \"prob\",\n                         mtry         = 3,\n                         num.trees    = 500)\n# Check parameters with this: learner_ranger_rf$param_set$ids()\n# look at a list of learners: mlr3::mlr_learners\n#-----------------------------------------------------------------\n# Build test and training data set\n#-----------------------------------------------------------------\nset.seed(44)\ntrain_mod_data <- sample(task_mod_data$nrow, \n                         0.75 * task_mod_data$nrow)\ntest_mod_data  <- setdiff(seq_len(task_mod_data$nrow), \n                          train_mod_data)\n#-----------------------------------------------------------------\n# Train the model\n#-----------------------------------------------------------------\nlearner_ranger_rf$train(task    = task_mod_data, \n                        row_ids = train_mod_data)\n#-----------------------------------------------------------------\n# Inspect the results\n#-----------------------------------------------------------------\n# print(learner_ranger_rf$model)\nlearner_output <- \ntibble::tibble(\n    numTrees  = unname(learner_ranger_rf$model$num.trees),\n    trys      = unname(learner_ranger_rf$model$mtry),\n    samples   = unname(learner_ranger_rf$model$num.samples),\n    error     = unname(learner_ranger_rf$model$prediction.error)\n)\n#-----------------------------------------------------------------\n# Evaluate holdout sample\n#-----------------------------------------------------------------\nprediction <- learner_ranger_rf$predict(task_mod_data, \n                                        row_ids = test_mod_data)\n#-----------------------------------------------------------------\n# Confusion matrix\n#-----------------------------------------------------------------\nprediction$confusion\n#>         truth\n#> response    r   nr\n#>       r  2662  508\n#>       nr  106  151\n#-----------------------------------------------------------------\n# Evaluate holdout sample\n#-----------------------------------------------------------------\nmeasure = msr(\"classif.acc\")\nprediction$score(measure)\n#> classif.acc \n#>   0.8208345\n#-----------------------------------------------------------------\n# Access model predictions\n#-----------------------------------------------------------------\nprobs <- as.data.frame(learner_ranger_rf$model$predictions)\nhead(probs)\n#>           r         nr\n#> 1 0.5376683 0.46233166\n#> 2 0.9015481 0.09845190\n#> 3 0.9252880 0.07471201\n#> 4 0.9603436 0.03965638\n#> 5 0.9370835 0.06291653\n#> 6 0.9028345 0.09716551\n#-----------------------------------------------------------------\n# Evaluate holdout sample\n#-----------------------------------------------------------------\n\ntasks        <- tsks(c(\"task_renew\"))\nlearner      <- lrns(c(\"classif.featureless\",\"classif.rpart\"),\n                     predict_type = \"prob\")\nresampling   <- rsmps(\"cv\")\nobject       <- benchmark(benchmark_grid(tasks, \n                                         learner, \n                                         resampling))\n# Use head(fortify(object)) to see the ce for the resamples\n#-----------------------------------------------------------------\n# Boxplot of classification error\n#-----------------------------------------------------------------\nbplot <- \nautoplot(object)   +\n  graphics_theme_1 +\n  geom_boxplot(fill = 'dodgerblue') \n#-----------------------------------------------------------------\n# ROC curve for random forest model\n#-----------------------------------------------------------------\nroc_model <- \nautoplot(object$filter(task_ids = \"task_renew\"), \n         type = \"roc\") +\n  graphics_theme_1 +\n  scale_color_manual(values = palette)"},{"path":"chapter7.html","id":"resampling","chapter":"7 Lead Scoring","heading":"7.2.1.3.1 Resampling","text":"Conceptually, last model validated one training set data one test set data. Occasionally ’ll see three-way partition data train, test, calibration set, often just train test set randomly sampled entire data set. many cases, OK, give confidence model warranted. Resampling strategies help calibrate model give confidence accuracy. can visualize mean resampling figure 7.1.\nFigure 7.1: Resampling concept\nstandard method find 10-fold cross-validation. learning procedure executed ten times different training sets (overlap), ten error estimates averaged yield overall error estimate (Ian H. Witten 2011). Leveraging framework mlr3 makes efforts (resampling tuning) much manageable. Let’s look example cross-validation regarding already constructed model.task learner objects identical saw previous sections.However, need add resampling object:’ll need instantiate resampling strategy. means going create .can now call resample. take longer seen building model multiple data sets.now sure way evaluate model.classification error improved. great! improve ?","code":"\n#-----------------------------------------------------------------\n# Rebuild model\n#-----------------------------------------------------------------\ntask_mod_data <- TaskClassif$new(id       = \"task_renew\", \n                                 backend  = mod_data_numeric, \n                                 target   = \"renewed\", \n                                 positive = \"r\")\n\nlearner_ranger_rf$train(task_mod_data, row_ids = train_mod_data)\n#-----------------------------------------------------------------\n# Add a resampling parameter\n#-----------------------------------------------------------------\nresampling_mod_data  <- rsmp(\"cv\")\n#-----------------------------------------------------------------\n# Rebuild model\n#-----------------------------------------------------------------\nresampling_mod_data$instantiate(task_mod_data)\nresampling_mod_data$iters\n#> [1] 10\n#-----------------------------------------------------------------\n# We can now call the resample object\n#-----------------------------------------------------------------\nresamp <- resample(task_mod_data, \n                   learner_ranger_rf, \n                   resampling_mod_data, \n                   store_models = TRUE)\n#> INFO  [09:56:34.816] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/10)\n#> INFO  [09:56:38.699] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 2/10)\n#> INFO  [09:56:42.644] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 3/10)\n#> INFO  [09:56:46.535] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 4/10)\n#> INFO  [09:56:50.122] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 5/10)\n#> INFO  [09:56:54.030] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 6/10)\n#> INFO  [09:56:58.063] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 7/10)\n#> INFO  [09:57:01.543] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 8/10)\n#> INFO  [09:57:05.144] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 9/10)\n#> INFO  [09:57:09.051] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 10/10)\n#-----------------------------------------------------------------\n# Rebuild model\n#-----------------------------------------------------------------\nresamp$aggregate(msr(\"classif.ce\"))\n#> classif.ce \n#>  0.1794095\n# Look at scores from the models: resamp$score(msr(\"classif.ce\"))"},{"path":"chapter7.html","id":"optimizing-your-model","chapter":"7 Lead Scoring","heading":"7.2.1.3.2 Optimizing your model","text":"Many algorithms parameters can alter performance model. instance, random forest can different numbers trees applied model. tuning algorithm appropriately?Let’s look different parameters available ranger package. 29, look first .Let’s select couple parameters tuneWe also consider resampling strategy. Since cross-validated results demonstrated similar error rates, let’s use holdout sample. , ’ll use classification error measure.Let’s combine everything tune instance pass new criteria model.Now determine search different parameters search. First, ’ll use random search. random search means levels randomly searched within set parameters.Now can take look best result.best results came minimum node size {r tune_instance$result_learner_param_vals$min.node.size max depth 4.Tuning improved model. 0.0097881 percent. can apply model test data observe results.three-part data partition useful. use calibrate results . first, let’s look new predictions look.new model fair job discriminating renewed renew.certainly improved accuracy. Now let’s compare model produced random forest one built different algorithm.","code":"\n#-----------------------------------------------------------------\n# Parameter set\n#-----------------------------------------------------------------\nparam_set <- as.data.frame(learner_ranger_rf$param_set$ids())\nhead(param_set)\n#>   learner_ranger_rf$param_set$ids()\n#> 1                             alpha\n#> 2            always.split.variables\n#> 3                     class.weights\n#> 4                           holdout\n#> 5                        importance\n#> 6                        keep.inbag\n#-----------------------------------------------------------------\n# Tuning parameters\n#-----------------------------------------------------------------\ntune_rf_params <- ParamSet$new(list(\n  ParamInt$new(\"min.node.size\", lower = 10, upper = 200),\n  ParamInt$new(\"max.depth\",     lower = 2,  upper = 20),\n  ParamInt$new(\"num.trees\",     lower = 500,  upper = 600)\n))\n#-----------------------------------------------------------------\n# set resampling and eval parameters\n#-----------------------------------------------------------------\nresamp_strat     <- rsmp(\"holdout\")\nmeasure_mod_data <- msr(\"classif.ce\")\nevals_10         <- trm(\"evals\", n_evals = 10)\n#-----------------------------------------------------------------\n# Build a tuning instance\n#-----------------------------------------------------------------\ntune_instance <- TuningInstanceSingleCrit$new(\n  task         = task_mod_data,\n  learner      = learner_ranger_rf,\n  resampling   = resamp_strat,\n  measure      = measure_mod_data,\n  search_space = tune_rf_params,\n  terminator   = evals_10\n)\n#-----------------------------------------------------------------\n# Randomly select options within tuning min and max\n#-----------------------------------------------------------------\ntuner_rf = tnr(\"random_search\")\n#-----------------------------------------------------------------\n# Run the models\n#-----------------------------------------------------------------\ntuner <- tuner_rf$optimize(tune_instance)\n#> INFO  [09:57:24.054] [bbotk] Starting to optimize 3 parameter(s) with '<OptimizerRandomSearch>' and '<TerminatorEvals> [n_evals=10, k=0]'\n#> INFO  [09:57:24.068] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [09:57:24.104] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [09:57:24.109] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [09:57:26.038] [mlr3] Finished benchmark\n#> INFO  [09:57:26.161] [bbotk] Result of batch 1:\n#> INFO  [09:57:26.163] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [09:57:26.163] [bbotk]            123        18       542   0.175093        0\n#> INFO  [09:57:26.163] [bbotk]  errors runtime_learners\n#> INFO  [09:57:26.163] [bbotk]       0             1.91\n#> INFO  [09:57:26.163] [bbotk]                                 uhash\n#> INFO  [09:57:26.163] [bbotk]  05488eaa-eeb9-44a2-bba5-5cf18aaa84f7\n#> INFO  [09:57:26.166] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [09:57:26.183] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [09:57:26.188] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [09:57:27.848] [mlr3] Finished benchmark\n#> INFO  [09:57:27.869] [bbotk] Result of batch 2:\n#> INFO  [09:57:27.871] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [09:57:27.871] [bbotk]            200        17       507  0.1726855        0\n#> INFO  [09:57:27.871] [bbotk]  errors runtime_learners\n#> INFO  [09:57:27.871] [bbotk]       0             1.64\n#> INFO  [09:57:27.871] [bbotk]                                 uhash\n#> INFO  [09:57:27.871] [bbotk]  c6a73688-fec4-4a27-a4a7-97385e8e7fa4\n#> INFO  [09:57:27.873] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [09:57:27.890] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [09:57:27.894] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [09:57:29.308] [mlr3] Finished benchmark\n#> INFO  [09:57:29.326] [bbotk] Result of batch 3:\n#> INFO  [09:57:29.327] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [09:57:29.327] [bbotk]            120        11       508  0.1739987        0\n#> INFO  [09:57:29.327] [bbotk]  errors runtime_learners\n#> INFO  [09:57:29.327] [bbotk]       0             1.41\n#> INFO  [09:57:29.327] [bbotk]                                 uhash\n#> INFO  [09:57:29.327] [bbotk]  ab2b1a52-511b-4d1f-b843-e15848fe8860\n#> INFO  [09:57:29.330] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [09:57:29.346] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [09:57:29.350] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [09:57:31.086] [mlr3] Finished benchmark\n#> INFO  [09:57:31.108] [bbotk] Result of batch 4:\n#> INFO  [09:57:31.109] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [09:57:31.109] [bbotk]             79        11       520  0.1755308        0\n#> INFO  [09:57:31.109] [bbotk]  errors runtime_learners\n#> INFO  [09:57:31.109] [bbotk]       0             1.73\n#> INFO  [09:57:31.109] [bbotk]                                 uhash\n#> INFO  [09:57:31.109] [bbotk]  133dbdb2-5f14-4c09-9761-d1c3d3e845d3\n#> INFO  [09:57:31.112] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [09:57:31.131] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [09:57:31.135] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [09:57:33.018] [mlr3] Finished benchmark\n#> INFO  [09:57:33.039] [bbotk] Result of batch 5:\n#> INFO  [09:57:33.040] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [09:57:33.040] [bbotk]            125        11       594  0.1739987        0\n#> INFO  [09:57:33.040] [bbotk]  errors runtime_learners\n#> INFO  [09:57:33.040] [bbotk]       0             1.86\n#> INFO  [09:57:33.040] [bbotk]                                 uhash\n#> INFO  [09:57:33.040] [bbotk]  b740795f-538f-4751-b113-0520fc6cac11\n#> INFO  [09:57:33.043] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [09:57:33.059] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [09:57:34.251] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [09:57:36.513] [mlr3] Finished benchmark\n#> INFO  [09:57:36.543] [bbotk] Result of batch 6:\n#> INFO  [09:57:36.544] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [09:57:36.544] [bbotk]            109        16       597  0.1748742        0\n#> INFO  [09:57:36.544] [bbotk]  errors runtime_learners\n#> INFO  [09:57:36.544] [bbotk]       0             2.25\n#> INFO  [09:57:36.544] [bbotk]                                 uhash\n#> INFO  [09:57:36.544] [bbotk]  0b7811b3-168d-4b93-8584-ef65cbfeeb32\n#> INFO  [09:57:36.547] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [09:57:36.566] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [09:57:36.570] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [09:57:37.508] [mlr3] Finished benchmark\n#> INFO  [09:57:37.534] [bbotk] Result of batch 7:\n#> INFO  [09:57:37.535] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [09:57:37.535] [bbotk]            124         4       580  0.1696214        0\n#> INFO  [09:57:37.535] [bbotk]  errors runtime_learners\n#> INFO  [09:57:37.535] [bbotk]       0             0.93\n#> INFO  [09:57:37.535] [bbotk]                                 uhash\n#> INFO  [09:57:37.535] [bbotk]  2db45362-fab3-4700-81ec-2e9f9fa28bc5\n#> INFO  [09:57:37.538] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [09:57:37.555] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [09:57:37.560] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [09:57:40.384] [mlr3] Finished benchmark\n#> INFO  [09:57:40.407] [bbotk] Result of batch 8:\n#> INFO  [09:57:40.408] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [09:57:40.408] [bbotk]             16        17       560  0.1792515        0\n#> INFO  [09:57:40.408] [bbotk]  errors runtime_learners\n#> INFO  [09:57:40.408] [bbotk]       0             2.81\n#> INFO  [09:57:40.408] [bbotk]                                 uhash\n#> INFO  [09:57:40.408] [bbotk]  f3d496b9-5230-4630-a96a-543fe31c3127\n#> INFO  [09:57:40.410] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [09:57:40.426] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [09:57:40.430] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [09:57:41.367] [mlr3] Finished benchmark\n#> INFO  [09:57:41.399] [bbotk] Result of batch 9:\n#> INFO  [09:57:41.401] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [09:57:41.401] [bbotk]             97         5       520  0.1696214        0\n#> INFO  [09:57:41.401] [bbotk]  errors runtime_learners\n#> INFO  [09:57:41.401] [bbotk]       0             0.94\n#> INFO  [09:57:41.401] [bbotk]                                 uhash\n#> INFO  [09:57:41.401] [bbotk]  a88c1e71-d8f7-445f-83d5-b60d0da8e850\n#> INFO  [09:57:41.404] [bbotk] Evaluating 1 configuration(s)\n#> INFO  [09:57:41.423] [mlr3] Running benchmark with 1 resampling iterations\n#> INFO  [09:57:41.428] [mlr3] Applying learner 'classif.ranger' on task 'task_renew' (iter 1/1)\n#> INFO  [09:57:46.447] [mlr3] Finished benchmark\n#> INFO  [09:57:46.470] [bbotk] Result of batch 10:\n#> INFO  [09:57:46.471] [bbotk]  min.node.size max.depth num.trees classif.ce warnings\n#> INFO  [09:57:46.471] [bbotk]             42        20       560  0.1772817        0\n#> INFO  [09:57:46.471] [bbotk]  errors runtime_learners\n#> INFO  [09:57:46.471] [bbotk]       0             5.03\n#> INFO  [09:57:46.471] [bbotk]                                 uhash\n#> INFO  [09:57:46.471] [bbotk]  59e4c429-02c2-472b-93a7-01915420499e\n#> INFO  [09:57:46.478] [bbotk] Finished optimizing after 10 evaluation(s)\n#> INFO  [09:57:46.478] [bbotk] Result:\n#> INFO  [09:57:46.479] [bbotk]  min.node.size max.depth num.trees learner_param_vals\n#> INFO  [09:57:46.479] [bbotk]            124         4       580          <list[5]>\n#> INFO  [09:57:46.479] [bbotk]   x_domain classif.ce\n#> INFO  [09:57:46.479] [bbotk]  <list[3]>  0.1696214\n#-----------------------------------------------------------------\n# Get the best parameters\n#-----------------------------------------------------------------\nbest_params <- tune_instance$result_learner_param_vals\n#-----------------------------------------------------------------\n# Observe new classification error\n#-----------------------------------------------------------------\ntune_instance$result_y\n#> classif.ce \n#>  0.1696214\n#-----------------------------------------------------------------\n# Rerun model\n#-----------------------------------------------------------------\nlearner_ranger_rf$param_set$values = \n  tune_instance$result_learner_param_vals\nlearner_ranger_rf$train(task_mod_data)\n#-----------------------------------------------------------------\n# Build a tuning instance\n#-----------------------------------------------------------------\nprediction_tuned <- learner_ranger_rf$predict(task_mod_data, \n                                        row_ids = test_mod_data)\n#-----------------------------------------------------------------\n# Observe confusion matrix\n#-----------------------------------------------------------------\nprediction_tuned$confusion\n#>         truth\n#> response    r   nr\n#>       r  2714  504\n#>       nr   54  155\n#-----------------------------------------------------------------\n# Observe optimized classification\n#-----------------------------------------------------------------\nmeasure = msr(\"classif.acc\")\nprediction_tuned$score(measure)\n#> classif.acc \n#>   0.8371754"},{"path":"chapter7.html","id":"comparing-the-results-of-different-model-types","chapter":"7 Lead Scoring","heading":"7.2.1.4 Comparing the results of different model types","text":"know model constructed best model problem? ’ll try different ones. likely little success problems altering classification system. go demonstration purposes.can choose available learners following command. know looking classification problems:Many algorithms accept certain data types. Numerical types always safe. ’ll use data set using test another algorithm.Now can build new task consisting group learners. ’ll use gradient-boosting algorithm, random forest, naive Bayes algorithm.can take look available measures following command:can view measures algorithm following command:xgboost model performed slightly better random forest. use instead.","code":"\n#-----------------------------------------------------------------\n# Observe all learners\n#-----------------------------------------------------------------\nmlr3::mlr_learners\n#> <DictionaryLearner> with 27 stored values\n#> Keys: classif.cv_glmnet, classif.debug,\n#>   classif.featureless, classif.glmnet, classif.kknn,\n#>   classif.lda, classif.log_reg, classif.multinom,\n#>   classif.naive_bayes, classif.nnet, classif.qda,\n#>   classif.ranger, classif.rpart, classif.svm,\n#>   classif.xgboost, regr.cv_glmnet, regr.debug,\n#>   regr.featureless, regr.glmnet, regr.kknn, regr.km,\n#>   regr.lm, regr.nnet, regr.ranger, regr.rpart,\n#>   regr.svm, regr.xgboost\n#-----------------------------------------------------------------\n# Observe all learners\n#-----------------------------------------------------------------\n\ntask_mod_data_num <- TaskClassif$new(id   = \"task_bench\", \n                                 backend  = mod_data_numeric, \n                                 target   = \"renewed\", \n                                 positive = \"r\")\n\nlearner_num <- \n  list(lrn(\"classif.xgboost\", predict_type = \"prob\"), \n  lrn(\"classif.ranger\", predict_type = \"prob\"))\n\nset.seed(44)\ntrain_mod_data_num <- \n  sample(task_mod_data_num$nrow, 0.75 * task_mod_data_num$nrow)\ntest_mod_data_num  <- \n  setdiff(seq_len(task_mod_data_num$nrow), train_mod_data_num)\n#-----------------------------------------------------------------\n# Build new task and learner\n#-----------------------------------------------------------------\ndesign_bnch <- benchmark_grid(\ntask_bnch        <- TaskClassif$new(id  = \"task_class2\", \n                               backend  = mod_data_numeric, \n                               target   = \"renewed\", \n                               positive = \"r\"),\nlearners_bnch    <- \n  list(lrn(\"classif.xgboost\", predict_type = \"prob\"), \n       lrn(\"classif.ranger\",  predict_type = \"prob\"),\n       lrn(\"classif.naive_bayes\", predict_type = \"prob\" )),\nresamplings_bnch <- rsmp(\"holdout\")\n)\n#-----------------------------------------------------------------\n# benchmark our designs\n#-----------------------------------------------------------------\nbmr = benchmark(design_bnch)\n#> INFO  [09:57:48.053] [mlr3] Running benchmark with 3 resampling iterations\n#> INFO  [09:57:48.057] [mlr3] Applying learner 'classif.xgboost' on task 'task_class2' (iter 1/1)\n#> INFO  [09:57:48.119] [mlr3] Applying learner 'classif.ranger' on task 'task_class2' (iter 1/1)\n#> INFO  [09:57:53.158] [mlr3] Applying learner 'classif.naive_bayes' on task 'task_class2' (iter 1/1)\n#> INFO  [09:57:53.586] [mlr3] Finished benchmark\n#-----------------------------------------------------------------\n# Observe available measures\n#-----------------------------------------------------------------\nmlr3::mlr_measures\n#> <DictionaryMeasure> with 62 stored values\n#> Keys: aic, bic, classif.acc, classif.auc,\n#>   classif.bacc, classif.bbrier, classif.ce,\n#>   classif.costs, classif.dor, classif.fbeta,\n#>   classif.fdr, classif.fn, classif.fnr, classif.fomr,\n#>   classif.fp, classif.fpr, classif.logloss,\n#>   classif.mauc_au1p, classif.mauc_au1u,\n#>   classif.mauc_aunp, classif.mauc_aunu,\n#>   classif.mbrier, classif.mcc, classif.npv,\n#>   classif.ppv, classif.prauc, classif.precision,\n#>   classif.recall, classif.sensitivity,\n#>   classif.specificity, classif.tn, classif.tnr,\n#>   classif.tp, classif.tpr, debug, oob_error,\n#>   regr.bias, regr.ktau, regr.mae, regr.mape,\n#>   regr.maxae, regr.medae, regr.medse, regr.mse,\n#>   regr.msle, regr.pbias, regr.rae, regr.rmse,\n#>   regr.rmsle, regr.rrse, regr.rse, regr.rsq,\n#>   regr.sae, regr.smape, regr.srho, regr.sse,\n#>   selected_features, sim.jaccard, sim.phi, time_both,\n#>   time_predict, time_train\n#-----------------------------------------------------------------\n# Compare the models\n#-----------------------------------------------------------------\nmeasures = list(\n  msr(\"classif.auc\", id = \"auc\"),\n  msr(\"classif.ce\", id = \"ce_train\")\n)\nmeasure_list <- as.data.frame(bmr$score(measures))\nmeasure_list[,c(6,11,12)]\n#>            learner_id       auc  ce_train\n#> 1     classif.xgboost 0.6953685 0.1744364\n#> 2      classif.ranger 0.6795682 0.1792515\n#> 3 classif.naive_bayes 0.6735490 0.2061720"},{"path":"chapter7.html","id":"manually-calling-a-logistic-regression-model","chapter":"7 Lead Scoring","heading":"7.2.1.5 Manually calling a logistic regression model","text":"Logistic regression form regression “builds linear model based transformed target variable.” (Ian H. Witten 2011) target variable 1 0. Applied problem, renew renew. Let’s call basic logistic regression model fun.Table 7.6: Summary stats logistic modelThis output looks slightly different regression outputs seen. won’t cover logistic regression , wanted inform . performs well great place start predict two possible outcomes. Additionally, best careful models. estimated pseudoRSquared, multiple ways . Try function pscl::pR2(mod) (Zeileis, Kleiber, Jackman 2008) see different methods.","code":"\n#-----------------------------------------------------------------\n# Compare the models\n#-----------------------------------------------------------------\nmod <- glm(renewed ~ ticketUsage + tenure + \n           spend + distance,\n           data = mod_data_numeric,\n           family = binomial(link = \"logit\")\n           )\nmod_sum <- \ntibble::tibble(\n  deviance      = unlist(summary(mod)$deviance),\n  null.deviance = unlist(summary(mod)$null.deviance),\n  aic           = unlist(summary(mod)$aic),\n  df.residual   = unlist(summary(mod)$df.residual),\n  pseudoR2      = 1 - mod$deviance / mod$null.deviance\n)"},{"path":"chapter7.html","id":"measuring-performance","chapter":"7 Lead Scoring","heading":"7.2.2 Measuring performance","text":"Measuring performance can complex. measures can technical confusing. using holdout samples, can ignore many degree. hand, sampled calibrated correctly, using “ce” classification error visual inspection confusion matrix often enough. Let’s build plot classification error give us additional insight happening.Let’s look density plot renewal scores classes.ROC curve demonstrates targeting certain groups can efficient.","code":"\n#-----------------------------------------------------------------\n# Compare the models\n#-----------------------------------------------------------------\ncalc_rates <- learner_ranger_rf$predict_newdata(mod_data_numeric)\nmod_data_numeric$pred <- predict(mod,newdata = mod_data_numeric,\n                                 type = 'response')\n#-----------------------------------------------------------------\n# Density plot of error\n#-----------------------------------------------------------------\ntitle   <- 'Density plot of renewal'\nx_label <- 'Prediction'\ny_label <- 'Density'\ndensity_pred <- \nggplot(data = mod_data_numeric, \n       aes(x=pred,color=renewed,lty=renewed))    +\n  geom_density(size = 1.2)                       + \n  scale_color_manual(values = palette)           +\n  scale_x_continuous(label = scales::percent)    +\n  xlab(x_label)                                  + \n  ylab(y_label)                                  + \n  ggtitle(title)                                 +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# ROC curve\n#-----------------------------------------------------------------\nlibrary(pROC)\n#define object to plot\nroc_object <- roc(mod_data_numeric$renewed, mod_data_numeric$pred)\n\ntitle   <- 'ROC curve for renewals'\nx_label <- 'Specificity'\ny_label <- 'Sensitivity'\n\nroc_graph <-\nggroc(roc_object,colour = 'dodgerblue',size = 1.2) +\n  xlab(x_label)                                    + \n  ylab(y_label)                                    + \n  ggtitle(title)                                   +\n  graphics_theme_1"},{"path":"chapter7.html","id":"using-this-data","chapter":"7 Lead Scoring","heading":"7.3 Using this data","text":"Lead scores EASY use. Qualifying leads important analytics exercise can quickly easily deployed. thinking strategically simplest finest. First, need apply preferred model new data. Scored data typically placed quantiles deployed order based desire happen.","code":""},{"path":"chapter7.html","id":"building-cumulative-gains-charts","chapter":"7 Lead Scoring","heading":"7.3.1 Building cumulative gains charts","text":"know model efficacy practice? Let’s take sample model data pretend new group plan deploy renewal campaign. Let’s begin borrowing data last analysis. Let’s pretend individuals new, trying renew .curve look like renewal rates differed group?can see curve bulges middle. curve begins flatten, campaign less efficient. problems complex. data better, getting fantastic results see texts can challenging.","code":"\n#-----------------------------------------------------------------\n# Build data from cumulative gains chart\n#-----------------------------------------------------------------\nmod_data_sample <- \n  mod_data_numeric                                      %>% \n  dplyr::select(pred,renewed)                           %>%\n  dplyr::mutate(custId = seq(1:nrow(mod_data_numeric))) %>%\n  dplyr::sample_n(5000)                                 %>%\n  dplyr::arrange(desc(pred))\n\nqt <- quantile(mod_data_sample$pred,\n               probs = c(.1,.2,.3,.4,.5,.6,.7,.8,.9))\n\nf_apply_quant <- function(x){\nifelse(x >= qt[9],1,\n  ifelse(x >= qt[8],2,\n    ifelse(x >= qt[7],3,\n      ifelse(x >= qt[6],4,\n        ifelse(x >= qt[5],5,\n          ifelse(x >= qt[4],6,\n            ifelse(x >= qt[3],7,\n              ifelse(x >= qt[2],8,\n                ifelse(x >= qt[1],9,10)))))))))\n}\n\nmod_data_sample$group <- sapply(mod_data_sample$pred,\n                                function(x) f_apply_quant(x))\n\ntable(mod_data_sample$group,mod_data_sample$renewed)\n#>     \n#>       nr   r\n#>   1   53 447\n#>   2   52 448\n#>   3   56 444\n#>   4   52 448\n#>   5   52 449\n#>   6   71 428\n#>   7  101 399\n#>   8   83 417\n#>   9  124 376\n#>   10 299 201\n#-----------------------------------------------------------------\n# Build data for cumulative gains chart\n#-----------------------------------------------------------------\nmod_data_sample$renewedNum <- \n  ifelse(mod_data_sample$renewed == 'r',1,0)\nmod_data_sample$perpop <- \n  (seq(nrow(mod_data_sample))/nrow(mod_data_sample))*100\n\nmod_data_sample$percRenew <- \ncumsum(mod_data_sample$renewedNum)/sum(mod_data_sample$renewedNum)\n\ntitle   <- 'Cumulative gains chart'\nx_label <- 'Population'\ny_label <- 'Cumulative Renewal Percentage'\ncgc <- \nggplot(mod_data_sample,aes(y=percRenew,x=perpop))           +\n  geom_line(color = mod_data_sample$group,size = 1.2 )      +\n  geom_rug(color = mod_data_sample$group,sides = 'b' )      +\n  geom_abline(intercept = 0, slope = .01, size = 0.5,lty=3) +\n  xlab(x_label)                                             + \n  ylab(y_label)                                             + \n  ggtitle(title)                                            +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Build data for improved cumulative gains chart \n#-----------------------------------------------------------------\nmod_data_gain <- mod_data_sample %>%\n                 group_by(group) %>%\n                 summarise(cumRenewed = sum(renewedNum))\n\nnew_renewals <- c(500,490,455,400,300,200,120,90,70,20)\nmod_data_gain$cumRenewed <- new_renewals\n\nmod_data_gain$gain <- \n  cumsum(mod_data_gain$cumRenewed/sum(mod_data_gain$cumRenewed))\n\n\ntitle   <- 'Cumulative gains chart'\nx_label <- 'Group'\ny_label <- 'Gain'\n\ncgc_imp <- \nggplot(mod_data_gain,aes(y=gain,x=group))                  +\n geom_line(color = mod_data_gain$group,size = 1.2)         +\n scale_x_continuous(breaks = c(0,1,2,3,4,5,6,7,8,9,10))    +\n geom_abline(intercept = 0, slope = .01, size = 0.5,lty=3) +\n xlab(x_label)                                             + \n ylab(y_label)                                             + \n ggtitle(title)                                            +\n graphics_theme_1"},{"path":"chapter7.html","id":"key-concepts-and-chapter-summary-6","chapter":"7 Lead Scoring","heading":"7.4 Key concepts and chapter summary","text":"Systematically interacting potential customers core component direct marketing. covered primary concepts:RFM scoresLead scoring random forestCross ValidationModel OptimizationModel comparisonsEvaluating model efficacyHow use models practiceThese subjects cover depth specific techniques lead scoring model construction.Recency, Frequency, Monetary Value scores relatively simple construct called “poor man’s analytics.” However, easy interpret simple use.random forest great tool data short--wide. results easy interpret work, well logistic regression. bread--butter machine learning.Cross-validating results integral part analytics process considered building model.Models can optimized different ways depending model. includes regression models machine learning models.can use readily-available tools compare models . always best try couple different things.Leveraging lead scores simple makes salespeople effective.","code":""},{"path":"chapter8.html","id":"chapter8","chapter":"8 Promotions","heading":"8 Promotions","text":"can challenging measure efficacy promotions. becomes even difficult take expansive view promotions consider advertising form promotion. clubs begun eliminate many forms promotion measure outcomes. makes sense level, don’t go far. Major Sports brands unique ways tend carry team performs well. However, can’t always good. reconcile issues standpoint promotions? chapter give specific example measuring impact promotion cover nuances subject.teams promotions? Like marketing techniques, promotions exist increase sales. sports, typically response less anticipated demand capacity. However, concept can paradoxical. Promotions perform best team performing well field. take multi-year point--view marketing, likely make different decisions look fiscal-year standpoint. also brand considerations.Additionally, competing philosophies use promotions. franchise can pull many levers increase likelihood someone purchasing ticketing product, biggest one price. also important note typically cost 71, justifying return investment generally considered essential part process (assuming goal isn’t solely maximizing revenue).Typical sales promotions focus value: getting products less money. However, see myriad promotions sports:Giveaways bobbleheads hatsPost-game concertsBuy one ticket, get one freeLoyalty programs season ticket holdersFlash sales dynamic pricing promotionsAdditionally, promotions may also negative impacts brand integrity. example, constantly reverting price promotions can cause damage (Keller 2003).“objective value pricing uncover right blend product quality, product cost, product prices fully satisfies needs wants consumers profit targets firm.”Furthermore, many channels notoriously difficult evaluate. Advertisements promotions occur number places:OOH (home) refers billboards, etc.RadioTelevisionSEM (Search engine)Online agencies (Double-Click)Social MediaEarned media, television coveragePodcastsFoundation activities charitable venturesSocial platforms become increasingly important part advertisement. However, platforms walled gardens proprietary algorithms demonstrating ROAS. don’t incentive tell something isn’t working. SEM even bigger problem can confounding. know, type “Game Hen Tickets” google, seems like pretty easy serve ad can tracked sale. SEM beachhead secondary market sellers. secondary markets, StubHub, tend worry volume transactions. Since operate across spectrum tickets, impossible outspend.Media evaluation handled specific ways. example, . .-based methods using neural networks measure exposure become popular past several years. However, media equivalency tough estimate accurately typically propped arbitrary figure. ’ll discuss evaluating advertising later chapter.know marketing activity worthwhile? many cases, don’t. Baselining sales can complex activities naturally baked sales. effects, see salary figure 8.1, clearly outside marketer’s influence.following graph produced public data ticket sales average salary MLB teams. points colored based population region.\nFigure 8.1: Relationship ticket sales salary\nlevel correlation payroll tickets sold. However, isn’t clear size market impacts salary sales. However, small markets tend cluster near bottom salary range, large markets cluster near top. use clustering algorithm data see true, illustration. also begs question.can reasonably estimate ticket sales using macro factors payroll wins, impact marketing efforts ?capability sticky point, valid. never find president owner liquidates marketing department. Indeed lot push tickets. crucial factor many clubs likely investment team. reliance team payroll acute baseball number games. exposed perturbations performance events. NFL significant advantage parity (along structural components related salary floors caps).also lot complexity specific market conditions. Therefore, several questions can ask focusing ticket sales.many single-game tickets sold individuals outside database?recycle fans promotions?average ticket sales higher major promotion?turnstile higher major promotions? F&B sales impacted?Can identify causal link sales/turnstile/revenue major promotions?major promotions promote purchase less expensive tickets reducing yield?reasonably good evidence major promotions increase ticket sales, increased ticket sales efficient use marketing dollars? Additionally, type major promotion efficient? ’ll walk example evaluate major promotions talk measuring forms media exposure. chapter may seem repetitive. analyze season data slightly different way. Promotions likely small samples, putting us non-parametric world. scary place favorite analytics tools may work intended.","code":""},{"path":"chapter8.html","id":"measuring-the-impact-of-promotions","chapter":"8 Promotions","heading":"8.1 Measuring the impact of promotions","text":"Measuring impact promotions can difficult. analysis builds analyses already completed. ’ll begin now familiar season_data data set. great spot start evaluating promotions.Table 8.1: Dataset evaluating promotionsThis data allow us examine two typical promotions baseball team might conduct. Bobbleheads Concerts ubiquitous across sports. lead believe effective ticket drivers. However, many considerations.\nFigure 8.2: Ticket sales season\nLet’s take little closer look data. vast differences many games. saw chapter 6 variables used predict sales. type graphic can misleading. took sales averages, promotions appear increase ticket sales. However, product consistent. wide variation sales contains seasonality, trends, endogenous factors influencing turnout.code chunk produces tile plot figure 8.3. can see concerts occur weekend. true Bobbleheads. can control determine impacting sales? ’ll look data different ways clarity. often helpful experiment graphics. data may tell stories.\nFigure 8.3: Ticket sales season day week\nLet’s take brief aside talk color interpolation. several ways interpolate colors R. addition, virids library (Garnier 2021) exciting color package produces vivid results used correct context. Give try.appear difference days week. let’s look slightly different way.see predictable pattern assumed previous plot. unfortunately, analysis confounded many factors, can’t take sales face value.\nFigure 8.4: Ticket sales season\nplot excellent job discriminating promotions. Concerts, bobbleheads, promotions generate sales average day week considered. factor, opponent, impact ?\nFigure 8.5: Ticket sales season\n","code":"\n#-----------------------------------------------------------------\n# Season data structure\n#-----------------------------------------------------------------\ndata <- FOSBAAS::season_data\n\ndata_struct <- \n  data.frame(\n  variable = names(data),\n  class    = sapply(data, typeof),\n  values   = sapply(data, function(x) paste0(head(x)[1:2],  \n                                      collapse = \", \")),\n            row.names = NULL\n  )\n#-----------------------------------------------------------------\n# Promotions by season\n#-----------------------------------------------------------------\ndata$count <- seq(1:nrow(data))\n\nx_label  <- ('\\n 2022-2024 Home Games')\ny_label  <- ('Tickets Sold \\n')\ntitle   <- (\"M-Promos tend to have higher than average sales\")\nticket_sales <- \n  ggplot(data, \n         aes(x = count,y = ticketSales, \n             color = factor(promotion)),\n             group = promotion)                                  +\n  ggtitle(title)                                                 +\n  xlab(x_label)                                                  +                         \n  ylab(y_label)                                                  +                           \n  scale_y_continuous(labels = scales::comma)                     +\n  geom_point(aes(y=ticketSales,x=count), size=2)                 +\n  geom_smooth(data=subset(\n    data,promotion == 'bobblehead' | promotion == 'concert' | \n    promotion == 'other' | promotion == 'none'),\n    method='lm',formula=y~x,se=FALSE,fullrange=TRUE,size=1.2)    +\n  geom_vline(xintercept = 1, lty=4, color='grey30')              +\n  geom_vline(xintercept = 81, lty=4, color='grey30')             +\n  geom_vline(xintercept = 162, lty=4, color='grey30')            +\n  geom_vline(xintercept = 243, lty=4, color='grey30')            +\n  scale_color_manual(\n    breaks = c('bobblehead','concert','other','none'),\n    values=palette, name='Promotion: ',\n    labels=c(\"BHead\",\"Concert\",\"Other\",'None'))                  +    \n  annotate(\"text\", x = 40,  y = 1000, \n           label = \"2022\", color='black')                        +\n  annotate(\"text\", x = 120, y = 1000, \n           label = \"2023\", color='black')                        +\n  annotate(\"text\", x = 200, y = 1000, \n           label = \"2024\", color='black')                        +\n  graphics_theme_1                                               + \n  theme(\n    axis.text.x      = element_blank(),\n    legend.position  = \"bottom\",\n    panel.grid.major = element_blank(),  \n    panel.grid.minor = element_blank(), \n    axis.ticks.x=element_blank()\n  )\n#-----------------------------------------------------------------\n# Promotions by day of the week\n#-----------------------------------------------------------------\nx_label  <- ('\\n Promotion')\ny_label  <- ('Day of Week\\n')\ntitle    <- \n  ('Average Sales (10,000s) by promotion and day of week \\n')\n\npromos <- \n  data                                         %>% \n  group_by(dayOfWeek,promotion,season)         %>%\n  summarise(avgTickets = median(ticketSales))\n\ntile_sales <- \n  ggplot(promos, aes(y=dayOfWeek,x=promotion))                   +\n  facet_grid(.~season)                                           +\n  geom_tile(aes(fill = avgTickets))                              + \n  geom_text(aes(label = round((avgTickets/10000), 2)),\n                color='grey10')                                  +\n  scale_fill_gradient(low = \"white\", high = \"dodgerblue\", \n                      space = \"Lab\",\n                      na.value = \"grey10\", guide = \"colourbar\")  +\n  ggtitle(title)                                                 +\n  xlab(x_label)                                                  +                       \n  ylab(y_label)                                                  + \n  scale_y_discrete(limits=c('Mon','Tue','Wed','Thu',\n                            'Fri','Sat','Sun'))                  + \n  scale_x_discrete(limits = c('bobblehead','concert',\n                              'none','other'),\n                   labels=c('bh','concert','none','other'))      +\n  graphics_theme_1 + theme(\n    legend.position  = \"bottom\",\n    axis.text.x      = element_text(angle = 0, size = 10, \n                                    vjust = 0, color = \"grey10\"),\n    legend.title     = element_text(size = 10, face = \"plain\", \n                                    color = \"grey10\"), \n    legend.text      = element_text(size = 7, color = \"grey10\")\n    )\n#-----------------------------------------------------------------\n# Color interpolation\n#-----------------------------------------------------------------\nlibrary(viridis)\nscale_fill_viridis(direction = 1, option = \"B\",trans=\"log2\") \nscale_fill_distiller(palette = 'Greens',direction = 1)\n#-----------------------------------------------------------------\n# Promotions by season and day of week\n#-----------------------------------------------------------------\ndata$count <- seq(1:nrow(data))\n\nx_label  <- ('\\n 2022-2024 Home Games')\ny_label  <- ('Tickets Sold \\n ')\ntitle    <- (\"Impacts are less clear when DOW is considered\")\n\ndow_sales <- \n  ggplot(data, aes(x = count,y = ticketSales, \n                   color = factor(dayOfWeek),\n                   shape = factor(promotion),\n                   group =factor(dayOfWeek)))                +\n  ggtitle(title)                                             +\n  xlab(x_label)                                              +                            \n  ylab(y_label)                                              +\n  scale_x_continuous( breaks = 1:242)                        +\n  scale_y_continuous(labels  = scales::comma)                +\n  geom_point(aes(y=data$ticketSales,x=data$count), size=3.5) +\n  geom_vline(xintercept = 81,  lty = 4,  color ='grey30')    +\n  geom_vline(xintercept = 162, lty = 4, color ='grey30')     +\n  geom_vline(xintercept = 242, lty = 4, color ='grey30')     +\n  scale_color_manual(breaks = c(\"Mon\", \"Tue\",'Wed','Thu',\n                                'Fri','Sat','Sun'),\n                     values = palette,\n                     name   ='Day: ',\n                     labels = c(\"Mon\", \"Tue\",'Wed','Thu',\n                              'Fri','Sat','Sun'))            +   \n  scale_shape_manual(\n    breaks = c('bobblehead','none','concert','other'),\n    name='Promotion: ',\n    values = c(17,19,3,7),\n    labels=c(\"bh\",'none','con','other'))                     +   \n  annotate(\"text\", x = 40,  y = 1000, \n           label = \"2022\", color='grey10')                   +\n  annotate(\"text\", x = 120, y = 1000, \n           label = \"2023\", color='grey10')                   +\n  annotate(\"text\", x = 200, y = 1000, \n           label = \"2024\", color='grey10')                   +\n  graphics_theme_1                                           +   \n  theme(\n    axis.text.x      = element_blank(),\n    legend.position  = \"right\",\n    panel.grid.major = element_blank(),  \n    panel.grid.minor = element_blank(), \n    axis.ticks.x     = element_blank())\n#-----------------------------------------------------------------\n# Box plot, sales by day of week\n#-----------------------------------------------------------------\nx_label  <- ('\\n 2022-2024 games by DOW')\ny_label  <- ('Tickets Sold \\n ')\ntitle    <- (\"Sales by DOW and promotion\")\n\ndow_sales_box <- \n  ggplot(data, aes(x     = dayOfWeek, \n                   fill  = factor(promotion), \n                   color = factor(promotion)))               +\n  ggtitle(title)                                             +\n  xlab(x_label)                                              +                               \n  ylab(y_label)                                              +\n  scale_y_continuous(labels = scales::comma)                 +\n  scale_x_discrete(limits=c(\"Mon\", \"Tue\",'Wed','Thu',\n                            'Fri','Sat','Sun'))              + \n  geom_boxplot(aes(y=ticketSales))                           +\n  scale_color_manual(\n    breaks = c('bobblehead','concert','none','other'),\n    values=c('grey40','grey40','grey40','grey40'), \n    name='Concert: ',\n    labels=c('bobblehead','concert','none','other'),\n    guide = 'none')                                          +    \n  scale_fill_manual(\n    breaks = c('bobblehead','concert','none','other'),\n    values=c(palette), \n    name='Promotion: ',\n    labels=c('bobblehead','concert','none','other'))         +\n  graphics_theme_1                                           +   \n  theme(legend.position  = \"bottom\")"},{"path":"chapter8.html","id":"regressing-on-our-data","chapter":"8 Promotions","heading":"8.1.1 Regressing on our data","text":"seen, regression takes rigor correctly. works best lot data, luxury tend cases. ’ll go another example, look analysis slightly differently. ’ll also use different framework time. ? ? many ways thing R, prefer others.’ll use tidymodels (Kuhn Wickham 2022) package exercise 72. Tidymodels (like MLR3) simplify preprocessing evaluation. ’ll begin installing libraries little processing data., going build linear regression model. also know interested much promotions influence ticket sales. ’ll make one change data set function f_change_order() make results easier interpret complete exercise. regression diagnostics output.sloppy approach issue trying solve, ’ll often resort strange things working data. ’ll see mean following sections. want reorder factor levels graph, can factor function.’ll use rsample library (Silge et al. 2022) build training test set. many ways partition data, used different one every time. Find one like. ’ll split data twenty-five percent going holdout sample. aware try use model predict sales value new data, model won’t work. can frustrating. making aware. happen.Tidymodels leverages tidy principles builds recipes layers. easier work frameworks. Pipes just easier read understand. However, programmers might find procedural nature irritating.Piping steps like dummy coding variables easy system. makes preprocessing data mechanical exercise.can check recipe couple commands.select linear regression model deploy.Tidymodels uses concept workflows process regression steps.Finally, ’ll apply workflow fit model training data set.can also extract model.know sample size issues data. Can trust coefficients?can use coefficients model help us explain specific promotions’ impact relative variables. example, bobblehead worth 4,062 ticket sales.However, part story. Concerts, bobbleheads, promotions different costs. concert, costs high. lot consider:Talent feesProduction feesReplacing damaged turfLightingThis list far exhausting, electric bill stadium may shock . promotion bobblehead much discrete cost. per-unit fee incur costs storing dispensing item. makes calculus reasonably simple consider dimension ticket sales. Concerts may increase ancillary purchases, food beverage. Additionally, excess tickets sold bobblehead may least expensive seats stadium. one net revenue?","code":"\n#-----------------------------------------------------------------\n# preprocessing our data\n#-----------------------------------------------------------------\nlibrary(tidymodels)\nlibrary(readr)      \nlibrary(broom.mixed) \nlibrary(dotwhisker)  \nlibrary(skimr)\nlibrary(dplyr)\n\ndata <- FOSBAAS::season_data\ndata <- data[,c(\"gameNumber\",\"team\",\"month\",\"weekEnd\",\n                \"daysSinceLastGame\",\"promotion\",\"ticketSales\")]\n#-----------------------------------------------------------------\n# Alter promotions \n#-----------------------------------------------------------------\nf_change_order <- function(x){\n  if(x == \"none\"){\"anone\"}\n  else{x}\n}\ndata$promotion <- sapply(data$promotion,function(x) \n                         f_change_order(x))\n#-----------------------------------------------------------------\n# Alter factor levels \n#-----------------------------------------------------------------\ndata$promotion <- \n  factor(data$promotion, \n         levels=c('none', 'bobblehead', 'concert', 'other'))\n#-----------------------------------------------------------------\n# Splitting our data set\n#-----------------------------------------------------------------\nset.seed(755)\ndata_split <- initial_split(data, prop = .75)\ntrain_data <- rsample::training(data_split)\ntest_data  <- rsample::testing(data_split)\n#-----------------------------------------------------------------\n# Build a recipe\n#-----------------------------------------------------------------\nsales_rec <-  recipe(ticketSales ~ ., data = train_data)\n#-----------------------------------------------------------------\n# Add functions to the recipe\n#-----------------------------------------------------------------\nsales_rec <- recipe(ticketSales ~ ., data = train_data) %>% \n             update_role(gameNumber, new_role = \"ID\")   %>%\n             step_dummy(all_nominal(), -all_outcomes()) %>%\n             step_zv(all_predictors())\n#-----------------------------------------------------------------\n# Check the recipe \n#-----------------------------------------------------------------\n\ndata_test <- sales_rec                 %>% \n             prep()                    %>% \n             bake(new_data = test_data)\n\nhead(data_test)[c(4:9)]\n#> # A tibble: 6 × 6\n#>   ticketSales team_ATL team_BAL team_BOS team_CHC team_CIN\n#>         <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>\n#> 1       25759        0        0        0        0        0\n#> 2       26464        0        1        0        0        0\n#> 3       29787        0        1        0        0        0\n#> 4       35277        0        0        0        1        0\n#> 5       40594        0        0        0        1        0\n#> 6       32073        0        0        0        1        0\n#-----------------------------------------------------------------\n# Define a model\n#-----------------------------------------------------------------\nlm_model <- linear_reg()           %>% \n            set_engine('lm')       %>% \n            set_mode('regression')\n#-----------------------------------------------------------------\n# build a workflow\n#-----------------------------------------------------------------\nsales_wflow <- workflow()           %>% \n               add_model(lm_model)  %>% \n               add_recipe(sales_rec)\n#-----------------------------------------------------------------\n# build a workflow\n#-----------------------------------------------------------------\nset.seed(755)\nfolds        <- vfold_cv(train_data, v = 10)\nsales_fit_rs <- sales_wflow          %>% \n                fit_resamples(folds)\n\ncv_metrics <- collect_metrics(sales_fit_rs)\n#-----------------------------------------------------------------\n# Run the model and extract the results\n#-----------------------------------------------------------------\n\nsales_fit <- sales_wflow            %>% \n             fit(data = train_data)\n\nresults   <- sales_fit             %>% \n             extract_fit_parsnip() %>% \n             tidy()\n#-----------------------------------------------------------------\n# build a workflow\n#-----------------------------------------------------------------\nmodel         <- extract_fit_engine(sales_fit)\nmodel_metrics <- glance(model)"},{"path":"chapter8.html","id":"how-to-place-promotions-on-a-schedule","chapter":"8 Promotions","heading":"8.2 How to place promotions on a schedule","text":"covered top-approach forecasting chapter 6. know dates select particular promotion? calculus typically revolves around two considerations:Maximizing ticket impactMaximizing revenue impactHow know making takeoffs? Specific scenarios may make correct decision unclear. Let’s look example chapter 6. Now confirmed bobbleheads concerts increase ticket sales let’s add another bobblehead. add promotion? event scores produced season_2025 data set may give us clues. also many qualitative considerations discuss.Let’s start visualizing data. boundaries exist event scores? clustered data earlier use clusters identify candidates promotion.\nFigure 8.6: Event attractivness range cluster\ngoal maximize ticket sales, plan something promotions games possibility game sell . sellout within confidence interval sellout, disqualified consideration. Let’s take look candidate dates. ’ll assume want maximize sales. decided go route ancillary revenue associated concessions retail compensate ticket premium forgoing. However, also looking games potential terms higher prices. Let’s start identifying borders clusters. easy way use duplicated function. , ’ll reverse output makes sense header.Look scatter plot predicted prices vs. ticket sales. know interaction sales prices. also know much promotion bobblehead may impact sales. ’ll use upper bound confidence interval give us idea games might best candidates bobblehead. example, select one games band, may miss opportunity sell tickets exceed capacity. hand, one games might attractive want maximize revenue price premium overwhelms capacity constraint.get lot fancier confidence interval estimates, analysis doesn’t warrant . Simple often good.\nFigure 8.7: Predicted sales prices cluster\ncan see games best candidates. Let’s look border games clusters 2 5. , let’s look candidate game see appropriate adding promotion.Table 8.2: Promotion candidateThis game late summer already commands high price. isn’t another major promotion night, plenty headroom sales. game looks like great night add promotion. However, considerations.Promotions often sponsors, may preference specific promotion takes place. also need consider promotional item. example, bobblehead may need six months lead time. analysis needs take place well advance can produced adequately messaged. Additionally, theme item may appropriate promotions occurring simultaneously. Finally, best consider happening region. competition another venue event night?","code":"\n#-----------------------------------------------------------------\n# Clustered events\n#-----------------------------------------------------------------\nseason_2025 <- read.csv('files/season_2025.csv')\n\nseason_2025$cluster <- factor(season_2025$cluster)\nx_label  <- ('\\n Game')\ny_label  <- ('Event Scores \\n')\ntitle   <- ('Event score clusters by event')\nes_box <- \n  ggplot2::ggplot(data = season_2025, \n                  aes(x = order,\n                      y = eventScore,\n                      color = cluster))             +\n  geom_point(size = 1,alpha = .5)                   +\n  geom_boxplot()                                    +\n  scale_color_manual(values = palette)              +\n  scale_y_continuous(label = scales::comma)         +\n  xlab(x_label)                                     + \n  ylab(y_label)                                     + \n  ggtitle(title)                                    +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Identify game breaks\n#-----------------------------------------------------------------\n\nseason_2025$border <- \n  ifelse(duplicated(season_2025$cluster) == TRUE,FALSE,TRUE)\n#-----------------------------------------------------------------\n# Identify candidate games\n#-----------------------------------------------------------------\nseason_2025$cluster <- factor(season_2025$cluster)\nx_label  <- ('\\n Pred Sales')\ny_label  <- ('Pred Price \\n')\ntitle   <- ('Predicted sales and price by cluster')\nes_scatter <- \n  ggplot2::ggplot(data      = season_2025, \n                  aes(x     = predTickets,\n                      y     = predPrices,\n                      color = cluster,\n                      shape = border))                      +\n  annotate(\"rect\", xmin = 45000 - confint(model)[36,2], \n                   xmax = 45000, \n                   ymin = 0,  ymax = 60,\n                   alpha = .4, fill = 'coral')              +\n  geom_point(size = 3,alpha = 1)                            +\n  geom_segment(aes(x = 39000, y = 50, \n                   xend = 37500, yend = 41),\n                   arrow = arrow(length = unit(0.5, \"cm\")),\n                   color = 'black')                         +\n  scale_color_manual(values = palette)                      +\n  scale_x_continuous(label = scales::comma)                 +\n  scale_y_continuous(label = scales::dollar)                +\n  xlab(x_label)                                             + \n  ylab(y_label)                                             + \n  ggtitle(title)                                            +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Get Confidence Intervals\n#-----------------------------------------------------------------\ncandidate <- subset(season_2025, \n                    season_2025$border == T & cluster == 2)"},{"path":"chapter8.html","id":"evaluating-external-and-internal-marketing-assets","chapter":"8 Promotions","heading":"8.3 Evaluating external and internal marketing assets","text":"marketing textbook, cover major analytics categories sports team, cover marketing. also covered several subjects directly related marketing, segmentation pricing. However, discussing something different . Although measuring media assets big business, phrase “’s Turtles way ” comes mind. get bottom, arbitrary figure likely used gauge value signage social media post. also can briefly talk internal marketing assets. pro sports team might sell tickets, media, merchandise, also marketing platform. ’ll discuss section.Honestly, sort analytics slightly dull. aren’t dealing concrete truths. marketing asset worth someone willing pay . Ultimately, worth tends gauged one two ways; many people can exposed quality exposure. eyes platform, better. also good putting advertisements front relevant audiences, become Google.Advertising big business, trying prove much worth pro sports team analogous astrology. Digital giants methods attribution. always looked methods skepticism. business sells advertising, incentive tell doesn’t work?Overall, assets usually evaluated form exposure equivalency. impressions-based formula look something like :\\[\\begin{equation}\n\\ {Value} = {Exposure} * {Quality} * {Costs}\n\\end{equation}\\]arbitrary cost talking references value associated outreach. much cost reach potential customer? don’t mean sound cynical, number difficult estimate.","code":""},{"path":"chapter8.html","id":"external-marketing-assets","chapter":"8 Promotions","heading":"8.3.1 External marketing assets","text":"Marketing assets incredibly varied. internet marketing platform. Additionally, messaging varies throughout year. instance, Brand messaging may predominate -season. However, ticket sales messaging predominant category season.lump digital marketing two categories—platform digital local digital.Platform digital marketing involves tech giants. sophisticated methods reach customers potential customers. Advertising social media still new scheme advertising. Increasingly, money spent areas. easier attribute sales spend using mediums tracking customers process easy. However, becoming difficult. can classify advertising platforms display, video, Social, Audio. major players arena include:Google Display NetworkFacebookInstagramTwitterAmazonYoutubeTikTokSpotifyEach platforms attribution methodologies. methodologies can also vary. want make effort attribute sales channels, combine capabilities old-school tricks. instance, special offer may advertised one channels. can attribute sales outlet Facebook? Three main methods leveraged:First touch/Last touch: fan purchases tickets, first last platform visited? ’ll attribute entire purchase platform.Weighted models: models attribute portion sale advertising touchpoint. Weighted models may also decay time. instance, fourteen days since touchpoint, longer attribute sale portion marketing engine.Algorithmic models: models custom-built might use regression tool accurately weight touchpoints.means analytics involved looks like flow chart diagram math equation. However, sometimes true. sports, typically deal relatively small transactions occur predictable cadence. better scenarios using algorithmic methods. expression think “Good enemy great.” wouldn’t overthink subject.Local digital marketing refers leveraging display local mechanisms. include city regional newspaper websites. Fandom decrease individual lives local market. Therefore, channels typically good readership local market.Local digital marketing refers leveraging display local mechanisms. include city regional newspaper websites. Fandom decrease individual lives local market. Therefore, channels typically good readership local market.Search engine marketing separate category display, issues context sports. discussed briefly earlier. efforts flatten secondary markets, form marketing may less critical clubs future. someone typing “game hen tickets” search engine, know wanted tickets. care buy ? Sometimes. However, losing strategy. depend individual clicking ad attribution, can’t beat secondary market makers terms spend.Search engine marketing separate category display, issues context sports. discussed briefly earlier. efforts flatten secondary markets, form marketing may less critical clubs future. someone typing “game hen tickets” search engine, know wanted tickets. care buy ? Sometimes. However, losing strategy. depend individual clicking ad attribution, can’t beat secondary market makers terms spend.Television advertising still crucial marketing budget. However, traditional broadcasts declined years. T.V. cable industries crisis. Younger consumers altering consumption habits alarming rates. Therefore, channel continue wane importance.Television advertising still crucial marketing budget. However, traditional broadcasts declined years. T.V. cable industries crisis. Younger consumers altering consumption habits alarming rates. Therefore, channel continue wane importance.Radio. Buying selling radio spots also multitude considerations. measured? Third-party researchers pay people allow track consumption habits extrapolate. Additionally, terrestrial radio broadcasts decreased popularity due subscription services.Radio. Buying selling radio spots also multitude considerations. measured? Third-party researchers pay people allow track consumption habits extrapolate. Additionally, terrestrial radio broadcasts decreased popularity due subscription services.Outdoor Advertising. --home advertising also varied. Highway signage biggest category outdoor advertising. many local players, huge players also dominate arena. Prices also vary considerably. advertisement inexpensive, widely available, can purchased precise increments.Outdoor Advertising. --home advertising also varied. Highway signage biggest category outdoor advertising. many local players, huge players also dominate arena. Prices also vary considerably. advertisement inexpensive, widely available, can purchased precise increments.Print. Traditional print advertising include taking advertisements newspapers magazines. channel dramatically decreased importance years.Print. Traditional print advertising include taking advertisements newspapers magazines. channel dramatically decreased importance years.Many channels difficult impossible measure. Building formal experiments around effective print ads aren’t top priority business. build strategy around marketing mix? sales cadence probably best place start. start know. know sales typically happen game--game basis. Understanding customer journey critical part marketing strategy. inform strategy anything. can also dramatically decrease spend channels aren’t readily measurable.also paradox sports related marketing. team good, appear marketing plays part success. high--high strategy works use price lever. However, effective marketing team isn’t good? spend amount places, expect level baseline success. wonder can validated. strategic standpoint, mean? means skeptical ruthlessly test experiment.","code":""},{"path":"chapter8.html","id":"internal-marketing-assets","chapter":"8 Promotions","heading":"8.3.2 Internal marketing assets","text":"Internal assets similar external assets. However, added wrinkle. association pro sports team creates added brand equity. doesn’t happen advertise Facebook. Additionally, tangential association brands advertising team. makes evaluating value piece signage social media post delivered team much difficult.calculate value piece signage outfield ballpark Nashville? process involve steps. likely need engage third party get information Television radio. large media research companies specialize sort research. basic process works like :Calculate amount exposure. many people see sign channel?Evaluate quality exposure. often asset visible.Calculate unit value audience channelBuild coefficient calculation takes account brand associationWhat sorts internal assets discussing? sports team large toolbox terms advertising assets. includes:-venue signageTelevision radio broadcastsCategory rights co-brandingSocial media postsCommunity outreach foundationsUniform patches logosVenue naming rightsEach assets varies. instance, signage broad category, best inventory located behind homeplate, outfield, around marquee. result, costs associated assets vary considerably. Additionally, value assets influenced -market reference prices competition.best place begin analyzing rate card. much inventory used, much isn’t utilized? imbalances? opportunities create new inventory, foul poles? Many sponsorship deals staggered multi-year terms. Exclusivities also common restrict access specific categories.Objectively justifying sponsorship can difficult. buyer’s perspective, reconciling opportunities may take work. building decision matrix options relatively simple, taking brand considerations account needed.","code":""},{"path":"chapter8.html","id":"key-concepts-and-chapter-summary-7","chapter":"8 Promotions","heading":"8.4 Key concepts and chapter summary","text":"Promotions response less demand supply can difficult evaluate. covered high-level topics:Evaluating promotion efficacybuilding model using TidyModels frameworkPutting promotions scheduleValuing internal external marketing productsWe learned promotions impact, appear effect. can vary market, promotions work better. Marginal costs associated promotion always considered. also qualitative considerations last promotion occurred, lead time, sponsorship considerations.tidyModels framework easy way take care mechanics model construction. handles various machine learning tasks flexible method try multiple techniques.Placing promotions schedule related pricing. complex interactions price demand. Sports unique product multiple qualitative considerations, win-loss record, playoff chances, seasonality, promotional items.Evaluating return marketing spend takes lot work. Many assets defy valuation. Pricing internal assets can challenging typically three considerations: viewership, engagement quality, brand association.","code":""},{"path":"chapter9.html","id":"chapter9","chapter":"9 Research","heading":"9 Research","text":"chapter discuss different research techniques understand consumer preferences composition. Ordinary people recoil research. tedious, painful, typically repetitive, opens scrutiny. Moreover, research complex, must worked research industry extensive experience. Finally, vast subject; must understand complete analyst.One main issues face results research projects often considered cursory fashion. person understands results best designer analyst. partly many internal consumers interested narrow band concerns. can discouraging much work goes project can easily disregarded.Furthermore, proper research enormous topic. chapters 5 6, mention writing entire book pricing segmentation easy due breadth subjects. easy fill multiple volumes consumer research. can find many tomes ebay sites traded significant discount. Purchase add library. continue reading chapter, ’ll discuss books used. Primary research techniques mostly stayed long time, references advised. chapter discuss two main topics:basics experiment design hypothesis testingPlanning designing surveysWe’ll able cover sliver topics techniques likely face point career. already discussed ANOVA mechanisms analyze experiments, didn’t discuss experiments designed. term statistical significance often mentioned context surveys. concept statistical significance often needs better understood. used narrow context around hypothesis testing mathematical artifact. doesn’t mean survey good valuable. ’ll discuss detail little later.included experiment design topic people need experience. Indeed, many business leaders aren’t well versed research practical way difficult time navigating consuming . Therefore, leave chapter critical pieces knowledge help better grasp research. paragraph might read cynically, food thought.can’t always trust researchDifferent techniques can lead practical outcomesSampling intricate involvedAnalyzing results takes special careConflicting outcomes can confound applicationIt isn’t always worth conductingOne main difficulties consumer research sports stated observed behavior can often different. distinction always true, can accurate sports. instance, season-ticket-holders may respond poorly survey might indicate price increases yet still renew high rates. People try game surveys hoping responses can tilt Fortuna’s wheel 73 favor.Additionally, research methods can combined advanced discovery, like purchasing data data broker74 Acxiom. ’ve spoken problems third-party data quite bit. example, research conducted fellows Deloitte found serious inaccuracies data purchased well-known data broker (John Lucker Bischof 2017).“survey findings suggest data brokers sell serious accuracy problems may less current complete data buyers expect need.Accurate data enormous problem. Garbage-, garbage-. ’ve seen practice conducting surveys comparing stated responses questions gender, ethnicity, age data purchased market. , whenever practical, “Trust, verify.”Despite issues encounter, research fundamental component business strategy. bigger problem, critical conducting worthwhile research becomes. Understanding frame questions form research can answer overlooked skill set. Getting good takes experience. Hopefully, chapter get thinking high level give rudiments toolbox can use effective confident conducting analyzing research.primarily conduct rather simplistic satisfaction surveys email deployed games periodically throughout season. Conducting surveys follows distinct process. ’ll loosely frame chapter around method designing survey borrowed “Designing Surveys, guide decisions procedures.” (Johnny Blair 2014) steps include:Planning survey designQuestionnaire design pretestingFinal design planningSample selection data collectiondata coding, analysis, reportingSince common research method likely employ emailed survey, focus . chapter demonstrate basics survey design, data preparation, analysis. ’ll also discuss philosophies creating applying results. However, since already demonstrated techniques use analysis, focus planning survey design.","code":""},{"path":"chapter9.html","id":"designing-experiments-and-hypothesis-testing","chapter":"9 Research","heading":"9.1 Designing experiments and hypothesis testing","text":"many ways design experiment, also extremely deep subject. Additionally, trying become expert clinical experiment design give case impostor syndrome. stuff full confusing statistics jargon combines multiple knowledge bases. Since putting together specific designs (fractional factorial design) relatively mechanical, can follow recipe correctly. Ensuring underlying assumptions regarding analysis plan fulfilled can much challenging. Luckily, aren’t subject peer review mechanism people conduct research profession. ’ll hit high points demonstrate practical examples. terms framework discussion, used “Designing experiments analyzing data” (Scott E. Maxwell 1990) Maxwell Delaney “Applied Survey Data Analysis” (Steven G. Heeringa 2010) Heeringa, West, Berglund high-level reference.Experiment design really validity. concerned results merit real-world applications. also multiple types validity, statistical conclusion validity75 construct validity76. Much analysis statistical conclusion validity. results meet specific criteria related testing hypothesis? Construct validity refers drawing correct conclusions analysis. many potential problems validity experiment. Always keep back mind.’ll also hear lot common terminologies designing experiment. common terms hear likely calls Treatment Control group. need basic structure /B ^ [https://en.wikipedia.org/wiki//B_testing] test. Understanding /B test sounds straightforward enough. One group receives stimulus (perhaps specific advertising mechanism), another doesn’t. use fact test whether stimulus measurable impact particular outcome, purchase intent. designs tests can simple case /B test, complex. Ultimately, subject findings tested groups statistical tests. looking level assuredness answering question whether valid differences individuals two populations. Afterward, interpret everything correctly. luck, can build causal models using results.","code":""},{"path":"chapter9.html","id":"hypothesis-testing","chapter":"9 Research","heading":"9.1.1 Hypothesis testing","text":"Hypothesis testing needs clarified sentence structure encounter. don’t statistics background, must take time understand hypothesis testing. next couple paragraphs touch high points. , steps statistics want consider. start specifying hypothesis. Let’s use example.want understand season ticket holders spend concessions non-season ticket holders. Null hypothesis mean spend season ticket holders - mean spend non-season ticket holders = 0. reject null hypothesis, implies mean spend two populations equal 0. Go ahead reread sentence. know doesn’t make sense. Please don’t blame . didn’t create stuff.Let’s take sample groups. ’ll create fake data exercise.Table 9.1: Sample spend dataWe use sample make inference general population. four possible outcomes hypothesis test:null hypothesis false. reject . means equal zero, season ticket holders spend differently regular fans.null hypothesis true. reject . Season ticket holders spend less average regular fans.null hypothesis true, reject . called type error.null hypothesis false, fail reject . called type II error.primary way test hypothesis using student’s t-test. let’s apply take look output.Table 9.2: t.test resultsIn case, reject null hypothesis differences means zero. alternative hypothesis true. also alternatives base t.test .also need consider sample sizes equal assumptions making accurate.","code":"\n#-----------------------------------------------------------------\n# Create spend data\n#-----------------------------------------------------------------\nsth <- as.data.frame(matrix(nrow=1000,ncol = 2))\nfan <- as.data.frame(matrix(nrow=1000,ncol = 2))\n\nnames(sth) <- c('spend','type')\nnames(fan) <- c('spend','type')\n\nset.seed(715)\nsth$spend <- rnorm(1000,25,7)\nfan$spend <- rnorm(1000,18,7)\nsth$type <- 'sth'\nfan$type <- 'fan'\n\n# Sample data\nset.seed(354)\nsth_samp <- sth[sample(nrow(sth), 400), ]\nfan_samp <- fan[sample(nrow(fan), 400), ]\n#-----------------------------------------------------------------\n# T Test\n#-----------------------------------------------------------------\nt_test <- t.test(sth_samp$spend,fan_samp$spend)"},{"path":"chapter9.html","id":"sampling","chapter":"9 Research","heading":"9.1.2 Sampling","text":"Covering hypothesis testing leads us another major consideration experiment design. validity experiment depend heavily sampling methodology. questions asking sample? Many questions related sample potentially bias results:sample representative population trying test?sample large enough apply broader population?sample need random?can imagine, get complex.preceding example, know many people sample? can use power analysis77 give us idea. ’ll evaluate data using pwr (Champely 2020) package. following example adapted Kabacoff (Kabacoff 2011).results test table 9.3.Table 9.3: t.test resultsAccording test, need 127 participants group calculate effect size .4 90% certainty five percent chance concluding difference isn’t. , reread sentence.example power test complex. several potential issues, underlying normality data, whether values independent, samples correlated, many . aren’t going go detail. However, must understand getting good sample takes rigor. Use reference follow process. ’ll able explain later (might just explaining ).","code":"\n#-----------------------------------------------------------------\n# Power analysis\n#-----------------------------------------------------------------\neffect_size <- 3/sd(sth_samp$spend)\nsignif_lvl  <- .05\ncertainty   <- .9 \n\npwr_test <- pwr::pwr.t.test(d         = effect_size,\n                            sig.level = signif_lvl,\n                            power     = certainty,\n                            type      = 'two.sample')"},{"path":"chapter9.html","id":"experimental-design","chapter":"9 Research","heading":"9.1.3 Experimental design","text":"One common experiment design called factorial design. 78. Factorial designs common , similarly everything else, can spiral complexity. goal ensure test possible combination factors test subject. full factorial design sometimes feasible cost trade-offs. may possible deploy many surveys like. case, fractional factorial design may used. 79 section cover simplified example. use something like sports designing conjoint formal experiment. also many designs, combinatorial design80. interested , must research methods independently. want aware . nothing else, can impress third-party researcher likely encounter.Let’s take look basic factorial design experiment. use AlgDesign (Wheeler 2022) library build design. Let’s assume two sections want look Dugout Seats Homeplate seats. want understand consumer preferences based section’s high, low, medium prices. Let’s say three possible prices.Use gen.factorial function produce factorial design based preceding options.Table 9.4: Factorial design surveyIn case, nine different combinations need put front survey recipients. numbers represent factor tested. first three rows indicate high, medium, low prices dugout seats tested low price home plate seats:survey give respondent option purchase either seat level:seat prefer indicated prices?case, show picture view seats. testing fans value view seats relative one another. may also mixed effects81 . careful sorts experiments. analyze results? ways. Let’s create data see .survey responses look like :Table 9.5: Factorial design surveyWe can create interaction plot following code.\nFigure 9.1: Interaction plot survey results\nplot suggests effect view based seating location inconsistent single-game ticket buyers season ticket holders. season ticket holders value views equivalent, single-game buyers may willing pay home plate seats given choice home plate dugout seats. can see complex designs can become. simple example gives basics designing experiment producing basic analysis.","code":"\n#-----------------------------------------------------------------\n# Factorial Design\n#-----------------------------------------------------------------\nlibrary(AlgDesign)\n\nfd <- gen.factorial(levels   = 3,\n                    nVars    = 2,\n                    center   = TRUE,\n                    factors  = FALSE,\n                    varNames = c(\"Dugout Seats\",\n                                 \"Homeplate Seats\"))\n#> Warning in xtfrm.data.frame(x): cannot xtfrm data frames\n#-----------------------------------------------------------------\n# Survey Responses\n#-----------------------------------------------------------------\nsurvey_responses <- \n  tibble::tibble(dugout    = factor(unlist(rep(fd[1],1000))),\n                 homePlate = factor(unlist(rep(fd[2],1000))),\n                 selection = NA)\n\nsurvey_responses$dugout <- sapply(survey_responses$dugout, \n                                  function(x) switch(x,'-1' = '75',\n                                                        '0' = '85',\n                                                        '1' = '95'))\n\nsurvey_responses$homePlate <- sapply(survey_responses$homePlate, \n                              function(x) switch(x,'-1' = '75',\n                                                    '0' = '85',\n                                                    '1' = '95'))\n\nsurvey_responses$respondent <- c(rep('sth',4500), rep('sin',4500))\n\nselection <- list()\nx         <- 1\nset.seed(755)\n\nwhile(x <= nrow(survey_responses)){\n  \n  s1 <- survey_responses[x,1] \n  s2 <- survey_responses[x,2]\n  \n  selection_list <- c(s1,s2)\n  \nselection[x] <- sample(selection_list, \n                       size = 1, \n                       prob = c(.2,.8))\nx <- x + 1\n  \n}\n\nsurvey_responses$selection <- unlist(selection)\nsurvey_responses$section  <- \n  ifelse(survey_responses$selection == survey_responses$dugout,\n         'dugout',\n         'homePlate')\n#-----------------------------------------------------------------\n# Interaction Plot\n#-----------------------------------------------------------------\nip_data <- survey_responses             %>%\n           group_by(respondent,section) %>%\n           summarise(meanSelection = mean(as.numeric(selection)))\n\ng_xlab  <- 'Respondent Type'                     \ng_ylab  <- 'Mean Price'                         \ng_title <- 'Interaction Plot, respondents and location'\n\nip_plot <- \nggplot(ip_data, aes(x = respondent,\n                y = meanSelection,\n                color = section,\n                group = section))                 +\n       geom_point(shape=2, size=3)                +\n       geom_line(size = 1.1)                      +\n       scale_y_continuous(label = scales::dollar) +\n       scale_color_manual(values = palette)       +\n       xlab(g_xlab)                               + \n       ylab(g_ylab)                               + \n       ggtitle(g_title)                           +\n       graphics_theme_1"},{"path":"chapter9.html","id":"planning-and-design","chapter":"9 Research","heading":"9.2 Planning and design","text":"Since covered segmentation chapter 4, ’ll base example around gathering information building psychographic segments. helpful example one many things. opportunities :Hypothesis test. instance, specific segments tend spend others?Look causal links attitudinal data long-term purchase behavior.Discover groups within population follow specific behavioral patterns.look correlations behavior specific activities.best way illustrate process creating specific scenario.Nashville Game Hens’ marketing department interested approaching marketing efforts sophisticatedly. want understand couple different things fanbase:understand market structure. coming games, compare region’s general population? , want break market structure logical segments can discovered multiple channels use marketing campaigns.understand something brand affinity means. brand perceived market?two broad problem statements. topics might cover multiple facets, price sensitivity, media consumption, general interests, financial status. can also see question two won’t allow us use internal data. forced collect . ’ll need carefully structure plan ensure answer questions asking. two statements didn’t ask questions needed . going interpret internal client contextually.need structure plan attacking effort can address request. concerned ? Let’s interpret questions:Question one concerned generalization fan. need create specific number groups people alike groups compare broader population sample includes fan base regional national population. can already tell difficult. one hundred thousand people sample, one-hundred-thousand segments. Attempting generalize arbitrary number boxes can well understood difficult.Question two concerned individual feels brand. competitive set may need uncovered. sports teams region measure evaluate brand? broader competitive set consider? -park experience impact brand? survey isn’t run---mill satisfaction survey. one going require work.Another main concern example influences decision-making process tension discoverability, accuracy, utility. analysis used target specific individuals? study something can cast broader segments people understand fit segmentation scheme? accurate results ? aren’t accurate, utility promises expensive project may significantly reduced. Make sure everyone clearly understands expectations trade-offs.","code":""},{"path":"chapter9.html","id":"understanding-the-makeup-of-a-clubs-fans","chapter":"9 Research","heading":"9.2.1 Understanding the makeup of a club’s fans","text":"brand perspective, sports unique. don’t typically see someone walking around city hat advertising favorite shampoo. Sports crosses vast swath individuals major cyclical components. Endemic concerns might include following:Several ticket classes relatively small populations -sized revenue contributionResponses may different season different times seasonThere multiple levels forms consumptionReferencing first point, corporations may make large portion season tickets. demographic information use ? distinction someone purchases tickets business fan purchases enjoy game? enjoying game mean? Individuals may consume number formats channels controlled club regulated club. Many implicit explicit signals imply fandom. mainly concerned ticket sales segmentation project?case, concerned ticket sales. seen techniques. factor analysis conducted section 5.4 example potentially helpful approach. data come survey asking someone attends game attend game. point plan data collected. knew use factor analysis derive insight survey, designed study accommodate technique.Ticket sales typically public information, teams tending announce sales figures82. let’s take moment analyze data MLB 2019.teams high number ticket sales. many individuals represent? make assumptions.Average 20,000 FSEs season ticketsSeason ticket holders buy average 4 ticketsSingle-game buyers come average 1.5 games buy three tickets per gameGroups larger 20 make 10% salesWe make many assumptions, point clear. look purchasers instead people attended games, results may look different. Additionally, aren’t considering fans watch TV follow media. default fans? Fans identify team don’t exhibit explicit signals fan. people purchase merchandise don’t consume primary product? Market structure complex industry.question remains. large actual market, precisely attempting segment? answer question might dreaded “Yes.” honest answer, like everything analytics, depends. project forks different directions depending answer. Let’s refer back question. two objectives:understand market structure. coming games, compare region’s general population?understand brand perceived interpret perception way useful making decisions.might obtain information question 1? Part data probably already exists.governmental data access, like census data?Observational researchEmailed surveysThird-party researchObservational research simple walking onto concourse game writing see. Technologies also deployed . example, facial recognition technologies 83 can even automate procedure large extent.get information number 2? Number two also looks like combination techniques:Emailed surveysThird-party dataWhat features interested looking ? stage, won’t know matter. instance, looking people might purchase season tickets, wealth factors like household income might necessary. minimum, ’ll want basic demographic features :AgeNumber childrenMarriage statusGenderEthnicityWhy want feature ethnicity? matter ethnic group individual belongs ? ethnicity behaves differently another, helpful advertising department terms channel location particular OOH efforts. Additionally, data used longitudinally variety purposes.Look carefully potential questionnaire data quality issues. Carefully considering question best way fix problems may manifest later. time spend considering question, better final results .","code":""},{"path":"chapter9.html","id":"selecting-the-correct-research-method","chapter":"9 Research","heading":"9.2.2 Selecting the correct research method","text":"sports team may several needs consumer brand-equity perspective. However, sports fandom can fickle thing. discussed, fandom exists many reasons. Switching teams challenging switching different hair treatment. Fans tend sticky, one main reasons research often takes back seat decision-making process. However, sports clubs face headwinds along several consumer behavior dimensions, brand loyalty media consumption (Jeff Fromm 2018). Properly developed deployed research method confront issues informed way. sports, slightly different needs research relative industries unique brand positioning fans fashionability ebbs flows along team fortunes. questions might include:competitive set?consumers like product?consumer’s constraints (financial, monetary)?fan’s awareness levels sponsors?fans think prices?perceived market relative properties?introduce loyalty program?Different needs different requirements terms research. Multiple types surveys designed specific tasks. specialty surveys must designed particular way require knowledge interpret. Examples include:Conjoint experiments84Van Westendorp85Max-Diff86Predictive market studies87While significant variation within techniques, relatively methods commonly deployed. Different designs also strengths weaknesses. example, Van Westendorp survey may demonstrate prices certain products (like seating section) perceived, doesn’t inform us willingness--pay (covered chapter 6. conjoint experiment might give better indication willingness pay. also design formal field experiments accomplish thing.studies require longer time horizons. Longitudinal studies measure change perception time. Like topics covered, focused research looking uncover causal links activity outcome goal. stage several substages need thought . planning process critical.","code":""},{"path":"chapter9.html","id":"building-and-designing-the-questionnaire","chapter":"9 Research","heading":"9.2.3 Building and designing the questionnaire","text":"’ve seen building surveys can extremely complex trying build durable study. section, ’ll focus building questionnaire. Coding survey questionnaires painful work. try follow following principles:Plan use analyze data.Don’t ask question use .Don’t survey much often.Pay attention survey burden terms lengthSome surveys may focused look uncover just one pieces insight. surveys may extensive exploratory. (Johnny Blair 2014) Many research tools can help design effective surveys. example, products Qualtrics88 Survey Monkey89 automatically analyze survey regarding burden. also contain banks questions can use resource.Often ’ll break extensive survey specific sections:DemographicsBrandSatisfactionProductAs rule, try keep surveys short relevant possible. Think carefully sequencing. instance, important thing need know? Put questions front, followed qualifying information.","code":""},{"path":"chapter9.html","id":"qualifying-your-sample","chapter":"9 Research","heading":"9.2.4 Qualifying your sample","text":"often critical qualify sample. Qualifying sample make results relevant. may reduce costs 90 deploying study. questions may behavioral demographic. section might ask questions :gender?MaleFemaleThis important might quota. example, sample 85% male may desirable. also guides asking questions increasingly controversial. Inclusivity also something consider. can find number guides writing inclusive surveys91. consistent. Additionally, might quota limits specific ethnicities.ethnicity?White CaucasianBlack African AmericanEastern IndianAsianNative American Pacific IslanderLatinxDid denote ethnicities correctly? ’s essential consider different nomenclatures go fashion. example, Latinx begun replace “Hispanic Latino.” judge merits replacing terms. Just aware everyone opinion . Build backstop awareness. Hispanic also problematic concept. makes someone Hispanic? also black white? important? ’ll make judgment calls based trying achieve survey.people may even poison sample. example, many surveys qualify respondents question :anyone family work marketing research?someone works marketing research, may understand mechanisms trying deploy. knowledge bias results certain types questions, conjoint analysis.sports, asking fan avidity widespread:fan Nashville Game Hens?However, asking specific avidity screening section often unwise. Instead, better method might ask question like :Please rate following activities terms interest:survey distributed third party, won’t giving away fact study Game Hens. also allows disqualify anyone interested attending game. depends trying accomplish.addition screening questions, dummy questions typically added survey guard indifferent respondents. instance, might embed question like survey:Please select fourth answer question:Answer 1Answer 2Answer 3Answer 4You surprised many people get question wrong. best disqualify responses someone gets question incorrect.","code":""},{"path":"chapter9.html","id":"typical-survey-questions","chapter":"9 Research","heading":"9.2.5 Typical survey questions","text":"specific questions often asked surveys. include demographic section usually interesting commonly included questionnaires. addition, preferred ways broach subject, essential discuss specifically.can track respondents, may already demographic information. survey doesn’t require answer question, essential consider information sequenced survey. good practice ask respondent like answer demographic questions:like answer demographic questions?Asking question prevent people ignoring rest survey aren’t interested answering demographic questions. Consider putting demographic questions end survey. Placing questions end survey ensure collect information deem important. Additionally, consider using data longitudinally. Changing format questions may make analysis difficult compare data older data sets.Let’s walk typical examples demographic questions. seems little pedantic, ask questions important.Instead asking age, prefer ask birth year calculate age:year birth?Obtaining birth year make easier compare data data collected past future. also makes easy place people demographic buckets. Additionally, ’ll able build accurate scatter plots data. can, collect numerical data. age can sensitive subject, haven’t seen problem simply asking birth year.Asking educational achievement also common. care? Education often proxy wealth. Someone may comfortable telling income, education isn’t taboo.highest level education completed?High schoolSome collegeCollegeGraduate degreeMarital status also commonly asked. ’ll want include category . typically see many different arrangements, separated, divorced, widowed. don’t typically include . asking something sports, ? Sports may salve loneliness, find widowers? don’t see lot practical application context. Keep simple omit can’t think possible use outside exploratory analysis. answers might simple proxies asking things sexuality.Marital Status?SingleMarriedPartnershipOtherAsking children also typical. sports seen family activities others. Additionally, stoking interest young people longer-term strategic play. much effort merchandisers put appealing young people sports?children age 18 living household?YesNoAlmost sports look develop youth, understanding many children attend games essential. Knowing age children also important. big difference five 16-year-old. respondent answers ‘yes’ preceding question, typically present something like following question:3.. Please select age(s).\n- 0 2\n- 3 5\n- 6 10\n- 11 15\n- 16 17In case, using multi-select. Multi-select challenging work back end require additional data wrangling. complicated question structure, less likely use . strike balance ideal practical.Geography especially important club. Season ticket holders likely live relatively small area surrounds stadium ballpark. Idiosyncrasies market also make understanding relative geography critical. example, lack rail infrastructure may make stadium less accessible certain parts region. generalized way get geography asking zip code.Please enter zip code:may need data point ticketing shipping info. best get actual address collect latitude longitude, zip code next best readily available piece information.Income tricky. survey isn’t anonymous, many people may choose answer question deceptively answer . ’ll include net present value company’s life insurance figure. use money. Additionally, question interesting continuous. can force later like. One big decisions granular make question.annual household income?$10,000$10,000 - $49,999$50,000 - $99,999$100,000 - $149,999$150,000 - $199,999$200,000 - $249,999More $250,000This data ordinal, meaning want displayed specific logical order. Often questions answers randomly reshuffled. Reordering questions answers done protect bias introduced someone habitually answering questions similarly. answers need randomized, question long, might make sense alphabetize answers. Let’s look example:answer best describes occupation?Administration/ManagerialAgricultureClerical/White CollarCraftsman/Blue CollarEducatorFinancialHomemakerLegalMedicalProfessional/TechnicalRetiredSales/ServiceSelf EmployedStudentOtherAre occupations logically organized? Agriculture occupation industry? care someone military works youth pastor church? color question. Consider omitting don’t specific use. likely attract large number responses. example, someone accountant, consider Clerical Finance? don’t typically like question.Many demographic questions conceived, preceding examples cover basics likely deploy. , , pay attention answers displayed data type. stuff always requires specific care.","code":""},{"path":"chapter9.html","id":"avidity-questions","chapter":"9 Research","heading":"9.2.5.1 Avidity questions","text":"Understanding avidity also crucial sports. Unfortunately, also tends misunderstood. can measure someone’s enthusiasm sports? Several explicit implicit signals might demonstrate avidity. individual following example avid fan?person purchases season tickets $700A person purchases season tickets $20,000A person purchases single game tickets $1,500These explicit signals likely demonstrate level affinity, avidity. also interactions play. just measuring wealth? cautious avidity. organize spend leveled buckets, aren’t measuring avidity.Implicit signals subtle.person purchases merchandise onlineA person visits website oftenA person enrolls child free kid’s club membershipThe person might purchased items gift. don’t know. might also ask person specific questions. common one might :big fan ?love Game Hens. live die .really like Game Hens. one favorite teams.identify Game Hens, casual fan.don’t really care Game Hens. fan.hate Hens. hope lose every game.think sort question Likert scale. Something like 1-5 1-7. typically used convention; can think lies underneath questions similarly. example, “love Game Hens. live die .” represents five. can see easy word questions. Pay attention . can easily write question ambiguous difficult understand.Additionally, include odd even number answers? even number responses forces someone pick one side. Forcing respondents onto one side fence helpful.","code":""},{"path":"chapter9.html","id":"brand-affinity","chapter":"9 Research","heading":"9.3 Brand affinity","text":"Gauging brand avidity difficult. ’ll explore tool look one brand avidity component called perceptual map. ’ll use library anacor (Mair De Leeuw 2022) accomplish part task. package built assist correspondence analysis92. little research topic.function used aggregated perceptual data created chapter two. question read like :feel following sports properties? Please check apply teams listed.initial data set took form multi-select table:code produces map figure 9.2\nFigure 9.2: Perceptual map Nashville Sports Properties\nread perceptual map? case, Chickens tend seen Fun Expensive relative properties. mean? Perhaps want seen innovative. seen innovative market may make us attractive specific sponsors. Whatever motivation, perceptual map allows visualize feelings around brand relative brands. can use time take measurements around feelings relative initiatives hope alter perception favor.","code":"\n#-----------------------------------------------------------------\n# Calculating scores for perceptual data\n#-----------------------------------------------------------------\nlibrary(anacor)\npd <- as.data.frame(FOSBAAS::perceptual_data)\nrow.names(pd) <- c('Chickens','Titans','Predators') \n\nanHolder <- anacor(pd,ndim=2)\n\nanHolderGG <- \n  data.frame(dim1 = c(anHolder$col.scores[,1],\n                      anHolder$row.scores[,1]), \n             dim2 = c(anHolder$col.scores[,2],\n                      anHolder$row.scores[,2]),\n             type = c(rep(1,length(anHolder$col.scores[,1])),\n                      rep(2,length(anHolder$row.scores[,1]))))\n#-----------------------------------------------------------------\n# Create a perceptual map\n#-----------------------------------------------------------------\nlibrary(scales)\nlibrary(RColorBrewer)\nlibrary(grid)\nlibrary(gridExtra)\n\nbasePal     <- c('grey60','mediumseagreen','grey40','steelblue1')\n\ng_xlab     <- '\\n PC1 (55.7%)'\ng_ylab     <- 'PC2 (44.3%) \\n'\ng_title    <- 'Nashville Area Sports Perceptual Map \\n'\n     \nPM <- \n  ggplot(data = anHolderGG,aes(x=dim1,y=dim2,\n                               colour=factor(type)))         +\n  geom_point(size = .5, fill = 'white', colour = 'White')    +\n  geom_segment(data = anHolderGG, aes(x = 0, y = 0, \n                                          xend = dim1, \n                                          yend = dim2), \n                  arrow = arrow(length=unit(0.2,\"cm\")), \n                  alpha = 0.75, color = \"steelblue2\")        +\n  geom_text(aes(label       =  rownames(anHolderGG)),\n                  size         = 3.2,\n                  position     = \"jitter\")                   +\n  scale_color_manual(values = basePal,\n                        name   = 'Perception',\n                        breaks = c(1,2),\n                        guide  = FALSE)                      + \n  xlab(g_xlab)                                               + \n  ylab(g_ylab)                                               + \n  ggtitle(g_title)                                           + \n  graphics_theme_1                                           +\n  geom_vline(xintercept = 0,lty=4,alpha = .4)                +\n  geom_hline(yintercept = 0,lty=4,alpha = .4)                 +\n  coord_cartesian(xlim = c(-.7,.9))              "},{"path":"chapter9.html","id":"other-topics","chapter":"9 Research","heading":"9.4 Other topics","text":"covered little chapter necessity. much material. covered high points discussed fundamental research components hypothesis testing sampling. also covered basic survey design couple examples survey analysis. didn’t talk causal models, examples research throughout book. often helpful think survey design lens answering specific question causal model. main subjects didn’t cover aware include following:Social listeningBrand personality studiesModels based acquisitionProduct recommendationDeveloping positioning productsIntercept surveysIn-depth interviewsfocus groupsCompetitive intelligenceIncentivizationThe obvious subject covered social listening, cover briefly . Social listening become large industry, dozens platforms can aid . monitoring responding social content essential, sentiment analysis little practical value. Additionally, word clouds typical consumption techniques social exciting surprising informative. Feel free disagree . sentiment comes practice.find social listening value lens media equivalency. sponsorship social team needs understand much content placed social channels value content. multiple ways , likely need help platform task. Like media content evaluation, ’ll need take volume, quality, arbitrary value estimate much return sponsors.going give nod focus groups. focus groups interesting sports? serve role public relations terms season ticket holders. especially true outside traditional focus group looking customer preferences constraints. Fan councils become important, care must taken ensure groups remain task. can tend degrade complaint centers.aren’t going mention topics . want learn , resources available everywhere. Hundreds agencies gladly work . Sports teams attractive marketing platforms, working prestigious. Take advantage prestige. trick know quality research entails find partner also understands . Finding competent help difficult task think. ’ve warned.","code":""},{"path":"chapter9.html","id":"key-concepts-and-chapter-summary-8","chapter":"9 Research","heading":"9.5 Key concepts and chapter summary","text":"Research foundation strategic initiatives. Whether improving operations process, pricing tickets, building pitch decks sponsorship team, producing credible research critical. covered five main topics chapter:Planning survey designQuestionnaire design pretestingFinal design planningSample selection data collectiondata coding, analysis, reportingResearch enormous topic, focusing widely used practices, can develop sound strategies leveraging creating research. covered several important concepts linked activities chapter 5 chapter 6.also covered sampling, typical question styles, data types, two specific examples covering brand affinity conjoint experiments. Keep principles mind deploy surveys:can’t always trust itDifferent techniques can lead practical outcomesSampling difficultAnalyzing results takes special careConflicting outcomes can confound applicationIt isn’t always worth conducting","code":""},{"path":"chapter10.html","id":"chapter10","chapter":"10 Operations","heading":"10 Operations","text":"sports team two fundamental core competencies, events management selling tickets. two efforts work takes place. Additionally, smooth operation critical brand perception. pillar product details-oriented business.overarching umbrella operations club context covers getting fans , around, facility safely, quickly, conveniently. design, system, technology components must constructed. also optimization considerations cost number point--sale units concessions retail, labor optimization security, food production, number seats particular sections, number bathrooms, etc. list extensive. Operations academic discipline studying systems. ’ll apply techniques overlapping academic fields Operations Research, Industrial Engineering, Operations Management analyze solve ubiquitous operations problems.Many constraints ballpark arena byproducts cost optimization. square footage campus size significantly influences can accomplish. Venues now designed advanced capabilities mind. capabilities might include following:Ordering food beverages phoneMore expansive automated options F&BDigital ticketsMore advanced security scanning technologiesSecurity cameras feature facial recognitionIntegrated commerce systems loyalty mechanismsIntegrated public transitWhile systems help solve existing problems, can exacerbate issues. instance, attempting food delivery without considering kitchen locations lead quality control issues. consider technology solutions carefully examining underlying execution capability. Businesses littered withered husks technology solutions failed deliver, ill fit, pet project inept executive, faded away internal external inability manage solution.Despite technological advances, venues always faced optimization problems around moving large numbers people systems quickly. addition, additive factors parking public transit, nearby entertainment districts, game day promotional items, food beverage service, security screenings must considered. following sections analyze problems detail.","code":""},{"path":"chapter10.html","id":"understanding-fundamental-issues-with-ingress","chapter":"10 Operations","heading":"10.1 Understanding fundamental issues with ingress","text":"Capacity constraints obvious unavoidable ingress. Lines inevitable 30,000 400,000 people trying get place simultaneously. Romans certainly understood issues well. Colosseum Rome designed 76 vomitoria accommodate 50,000 people93. Ironically, Colosseum efficient ingress egress modern stadium. Many venues require security check involving magnetometer. Bags must checked, tickets must scanned, building fire codes create constraints, concessions mechanical infrastructure take internal space. Romans didn’t worry running electricity HVAC throughout venues. issues extend outside building apply road traffic, ADA considerations, public transit, ride-sharing partnerships.Eliyahu Goldratt published book 1984 called Goal (Goldratt 2004), introduces us Theory Constraints. book gives us simple framework approaching problem ingress. interested Operations, must-read book. may appreciate depressing narrative, content outstanding. Goal, Goldratt introduces five-step Process -Going Improvement:Identify system’s bottlenecksDecide exploit bottlenecksSubordinate every decision ‘step two decisions.’Elevate systems bottlenecksif, previous step, bottleneck broken, go back beginning.system can one bottleneck94. makes process iterative. replace weakest link chain stronger one, another link become weakest. Let’s outline ingress process look data. Ingress process look similar virtually every venue worldwide. typical flowchart ingress procedure can studied figure 10.1.\nFigure 10.1: Typical gate entry process\nFigure 10.1 demonstrates typical gate procedure. Although recent technological advances made process faster, fundamental process followed event venues. sort data extract process? typically scan data ticketing process magnetometer passes failures. addition, ’ll know many people enter gate entry point pass security. Let’s take look aggregated data.Table 10.1: Aggregated scan dataThis data aggregated number scans minute summed bucketed. distribution scans can seen figure 10.2. information might helpful ?\nFigure 10.2: Distribution ticket scans gates\ngeneral, ticket scans typically follow specific cadence similar events. ballpark four points entry fans, point entry four scanners. entry process slightly probabilistic. scans take time failure requires adjustments additional scan, people move different paces. maximum capacity system?question difficult answer know gates likely busier others different times. looking entrance maximum throughput duress. gate represent maximum theoretical throughput potential. need multiple days observation get good average figure.Table 10.2: Maximum observed throughputThe highest observed average number scans per scanner thirteen per minute across ingress point. means person went system every four half seconds one minute. However, average may mean scans took ten seconds others took one second. can’t tell data. subsequent section, discuss queuing theory analyze wait time looking service times, inter-arrival times, line length, queuing discipline. illustrating broader point example.can see still need critical data. can also see problem sprawls becomes complex. data need? prefer see following:process components split timedInformation line-lengthSpecific information gate magnetometerInformation inter-arrival timesInformation staffing numbers procedureTrue positives false positive numbers scanner alarmsCan identify bottleneck? Judging diagram, magnetometer triggered, individual asked walk back . change process add staff wand guests triggered gate. also eliminate bags alter magnetometer settings measure differences. solutions require much diligence. ’ll expand example examine analogous project deeply; Reducing wait times concession stands.","code":"\n#-----------------------------------------------------------------\n# Access scan data\n#-----------------------------------------------------------------\nscan_data <- FOSBAAS::scan_data\n#-----------------------------------------------------------------\n# Scans\n#-----------------------------------------------------------------\nx_label <- 'observation'                                             \ny_label <- 'scans'                                          \ntitle   <- 'Ticket scans by minute at gates'\nscan_point <- \n  ggplot(data = scan_data,aes(x     = observations,\n                              y     = scans,\n                              color = date))                +\n  geom_point()                                              +\n  scale_x_continuous(label = scales::comma)                 +\n  scale_y_continuous(label = scales::comma)                 +\n  scale_color_manual(values = palette)                      +\n  xlab(x_label)                                             + \n  ylab(y_label)                                             + \n  ggtitle(title)                                            +\n  stat_smooth(method = \"lm\", formula = y ~ x + poly(x,2)-1) +\n  geom_vline(xintercept = 151, lty = 4)                     +\n  graphics_theme_1                                          +\n  guides(color = \n         guide_legend(override.aes = list(fill = \"grey99\")))\n#-----------------------------------------------------------------\n# Scans\n#-----------------------------------------------------------------\nmax_scans <- scan_data                                %>% \n             group_by(date)                           %>%\n             summarise(maxScans = max(scans),\n                       maxScansMean = (max(scans)/16),\n                       meanScans = mean(scans),\n                       medianScans = median(scans))\n "},{"path":"chapter10.html","id":"reducing-wait-times-at-concessions","chapter":"10 Operations","heading":"10.2 Reducing wait times at concessions","text":"Everyone spent time standing line food beverage sporting event. ask someone wrong system, might give couple predictable responses “many people” “people just don’t know .” reality, complex problem many components:System constraints (number grills, points--sale, etc.)Labor (experience, training, motivation)Back--house systems (buffering demand, menu design)Front--house systems (Queuing discipline, ordering, payment systems)interesting thing don’t acknowledge problem. Perhaps isn’t. can look system state don’t enough points sale. lines long_. points sale, spread lines, lines shorter. , course, incredibly reductive point view. let’s little realistic begin constructing narrative:Executives Nashville Game Hens noticed long lines concession stands games. Lines tend get noticeably worse weekends fans building. Additionally, customer satisfaction surveys indicate waiting line concession stands significant source dissatisfaction. concessions manager says, “problem don’t enough points--sale deal capacity reach 30,000 fans.” poses solution require significant capital investment add points sale. approached executive asked good idea…can solve problem making observations . Look data available. ’ll assume don’t anything. talk concessions manager, tells standard fill orders less ninety seconds. industry standard, achieves average.Without data, project proceed stages. However, can also take cues nature problem help devise plan approaching solution. least three identified project components involve process improvement. Since looking process improvement, let’s see can apply DMAIC problem. DMAIC stands Define, Measure, Analyze, Implement, Control. comes us Six Sigma95, process improvement protocol adapted many business problems. Six Sigma roots manufacturing, can appropriate tools .produce project charter laying complete project plan using framework. project plan needs include components. two concern include:Project objectivesSpecific goalsOther components charter cover include:TimelinesBudgetsStakeholdersPotential problemsI pointing seemingly irrelevant step several reasons. first reason project plan forces think solve problem. often challenging part. also forces create documentation around project. create essential documentation around projects, eventually used benefit. problem tend sprawl. Make habit building project charters projects involving multiple stakeholders external groups. make scoping projects much manageable help keep everyone task budget.","code":""},{"path":"chapter10.html","id":"establishing-objectives","chapter":"10 Operations","heading":"10.2.1 Establishing objectives","text":"Objectives can vague derive business case problem statement. Let’s take stab establishing goals. ’ll use concepts “Six Sigma” (Campe 2007) example.business case simply statement indicating project’s importance. demonstrate project needs addressed:Customer satisfaction linked repeat purchases demonstrably impacts year--year revenue. Additionally, customer satisfaction scores predictably degrade attendance exceeds 30,000. Surveys concluded improving scores can best achieved decreasing wait time concessions throughout venue. can demonstrably impact season ticket renewals 100 games left season. Additionally, capital budgets must completed next sixty days.statement establish?project importantIt needs completedWe limited amount time get started","code":""},{"path":"chapter10.html","id":"understanding-our-problem","chapter":"10 Operations","heading":"10.2.2 Understanding our problem","text":"’ll need little data begin understand problem, aim collect exploratory data. looking something help build problem statement. case, can look line length particular concept.typical concept eight fifteen items menu. Items purchased differential rates varying prep times. items can stored twenty minutes quality begins degrade. number points sale varies, concepts four twelve points purchase. Customers enter queue wait point sale becomes available. place order, pay, wait point purchase order fulfilled exiting system.Queuing systems fascinating topic. detailed analysis queuing systems covered chapter within section 10.5. However, unfamiliar queuing systems analyzed, recommend spending little time covering Queuing theory 96. Queuing theory can highly complex many applications across various disciplines. simplify , consider goal designing systems outcomes deterministic.Game Hens’ concession concepts typically follow multi-channel, single-phase (MCSP) queuing discipline. MCSP system, one-step (single phase) serving process register handles complete transaction, multiple points--sale (channels) serving queue. Additionally, process follows FIFO (first-, first-) queue discipline customer waiting extended amount time serviced first.Let’s extract key elements system:Process follows FIFO queue discipline function asynchronouslyOrdering, payment, fulfillment happen point--saleFulfillment difficult buffer due differential storage times demand levelsNow detail, can construct plan capturing measurements analysis.","code":""},{"path":"chapter10.html","id":"defining-our-problem-and-establishing-goals","chapter":"10 Operations","heading":"10.2.3 Defining our problem and establishing goals","text":"strategically oriented projects tend sprawl think . one different. lots considerations. Let’s create problem statement unambiguously explains project.Problem statement: Wait times concessions services across ballpark consistently receive low relative satisfaction scores (less 10% fans give highest rating) season ticket holders. Low satisfaction scores correlated higher likelihood specific fan renew season membership. vendor quoted industry standards service times ninety seconds per transaction, observed wait times often exceed figure 100%.Goals precise objectives. Since don’t data process, wait establish firm goals. initial goal collect necessary data hopes can reduce wait times. yet determine much may possible reduce wait times. Although industry standard, must determine figure merit.","code":""},{"path":"chapter10.html","id":"measurement-and-analysis","chapter":"10 Operations","heading":"10.3 Measurement and analysis","text":"seldom data need. case, ’ll need take note create plan gather don’t. Since take live measurements, ’ll need rubric budget. Additionally, timeline impacted fact must take measurements game.","code":""},{"path":"chapter10.html","id":"data-audit-and-capture","chapter":"10 Operations","heading":"10.3.1 Data audit and capture","text":"’ll want careful capture data. ’ll also want consistent thorough possible. Spend time thinking phase project. case, know must capture observational data. Additionally, data might make someone look bad. Since quoted ninety seconds number met, happens measurements differ? Keep sorts political impediments mind build rubric.many ways collect data context, fundamental components plan almost identical. Just google data collection plan. frameworks similar. start specific question trying answer lay specifics :Data typesWhat data measured?data measured?data captured?ensure consistency?initial goal analysis identify bottleneck. Next, gather data information systems. example, scan data captured gates, know many people park time. information may helpful.Leveraging points sale estimate wait times might possible. use .. track clock customers cameras installed concept. However, don’t, means using people stopwatches observe concept. seems barbaric, set firm rules around constitutes measurement collect enough data, able get reliable estimates.Additionally, concept information needed business perspective mature time. love able track every customer venue times, necessary? spend money understood every competing expense throughout business? use data? CEOs executives make decisions. Think problems broader perspective confronted . make distinction ideal practical.","code":""},{"path":"chapter10.html","id":"line-length-and-total-scans","chapter":"10 Operations","heading":"10.3.2 Line length and total scans","text":"Figure 10.3 requires two separate data sets. Scans data line length data. already seen scan data.Table 10.3: scan data exampleLine-length data looks different scan data. looks different data collected manually counting number people line every minute. data also confounded fact four people may line, one may order. Likewise, one person may ordering one person. data quirks, try write . ’ll explain someone point.(#tab:line_length_a)line length dataLet’s look see line length relationship scans. First, looking see number people entering building influences line length. following code produces graph figure 10.3.\nFigure 10.3: Relationship line length concession stand scans\ncan see pattern data line length peaks specific times, relationship cumulative scans isn’t tight; appears fans enter early tend rush concession stands fans come near beginning game take seat go concession stands later. lot variability data.","code":"\n#-----------------------------------------------------------------\n# Create scan data\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\nscans_a <- f_get_scan_data(x_value = 230,\n                           y_value = 90,\n                           seed    = 714,\n                           sd_mod  = 10)\n#-----------------------------------------------------------------\n# Create line-length data\n#-----------------------------------------------------------------\nline_length_a <- f_get_line_length(seed = 755,\n                                   n    = 300,\n                                   u1   = 22,\n                                   sd1  = 8,\n                                   u2   = 8 ,\n                                   sd2  = 5)\n\nline_length_a$action_time <- f_get_time_observations(17,21)\nline_length_a$date <- \"4/1/2024\"\n#-----------------------------------------------------------------\n# Line length and scans\n#-----------------------------------------------------------------\nscans_a$cumScans <- cumsum(scans_a$scans)\ndata  <- dplyr::left_join(scans_a,line_length_a, \n                          by = \"action_time\")\ndata$color <- as.numeric(as.character(gsub(\":\",\n                                           \"\",\n                                           data$action_time)))\n\nx_label  <- ('\\n Cumulative Scans')\ny_label  <- ('Line Length \\n')\ntitle    <- ('Line Length and cumulative scans')\nlegend   <- ('Time')\nline_length <- \n  ggplot(data, aes(y     = lineLength, \n                   x     = cumScans, \n                   color = color))                           +    \n  geom_point()                                               +\n  scale_color_gradient(breaks = c(2100,2000,1900,1800,1700),\n                       labels = c(\"9:00\",\"8:00\", \"7:00\", \n                                  \"6:00\",\"5:00\"),\n                       high = 'dodgerblue',\n                       low  = 'coral',\n                       name = legend)                        +\n  scale_x_continuous(label = scales::comma)                  +\n  xlab(x_label)                                              + \n  ylab(y_label)                                              + \n  ggtitle(title)                                             +\n  stat_smooth(method = \"loess\", formula = y ~ x)             +\n  geom_vline(xintercept = 13068, lty = 4)                    +\n  graphics_theme_1  "},{"path":"chapter10.html","id":"analyzing-the-results","chapter":"10 Operations","heading":"10.3.3 Analyzing the results","text":"can analyze data much fashion analyzed data previous chapters. However, ’ll introduce new tool called generalized additive model. careful technique. might find silver bullet can easily model many distributions, little challenging math equation provided regression. ’ll use mgcv (Wood 2022) library access gam function. often case, ’ll need data preparation applying tool.can access summary game model look standard statistics:Table 10.4: gam model outputWe can see data defy accurate fit looking figure 10.4.\nFigure 10.4: GAM output line length data\naverage fit data fair, predictive power won’t good given point graph. Ultimately, isn’t good relationship line length scans ballpark. However, see data appears multi-modal. example, lines decrease near start game. decrease line length first pitch makes sense early arrivers place orders take seats.","code":"\n#-----------------------------------------------------------------\n# Generalized additive model\n#-----------------------------------------------------------------\nlibrary(mgcv)\nscans_a$cumScans <- cumsum(scans_a$scans)\ndata             <- dplyr::left_join(scans_a,line_length_a, \n                              by = \"action_time\")\n\ndata$color <- as.numeric(as.character(gsub(\":\",\n                                           \"\",\n                                           data$action_time)))\n\ngam1 <- mgcv::gam(lineLength ~ s(cumScans, bs='ps', sp=.2), \n                  data = data)\n\nnewPredict <- cbind(data, predict(gam1, interval = 'confidence'))\n\ngr <- \n  ggplot(newPredict, aes(y     = lineLength, \n                         x     = cumScans,\n                         color = color))                      +    \n  geom_point(alpha=.7)                                        +\n  scale_color_gradient(breaks = c(2100,2000,1900,1800,1700),\n                       labels = c(\"9:00\",\"8:00\", \"7:00\", \n                                  \"6:00\",\"5:00\"),\n                       high = 'dodgerblue',\n                       low  = 'coral',\n                       name = 'Time')                         +\n  geom_line(aes(y = `predict(gam1, interval = \"confidence\")`,\n                x = cumScans),\n                color = 'dodgerblue',size = 1.2)              +\n  scale_x_continuous(label = scales::comma)                   +\n  xlab('Cumulative Scans')                                    + \n  ylab('Line-Length')                                         + \n  ggtitle('Results of GAM on Line-Length Data')               +\n  graphics_theme_1   "},{"path":"chapter10.html","id":"understanding-wait-times","chapter":"10 Operations","heading":"10.3.4 Understanding wait times","text":"Now data line length, can closely examine drives wait time. can simulate results measuring broader process. built function f_get_wait_times() provide us data. function breaks wait times three parts:Order time: amount time takes place orderPayment time: long took fulfill payment order placedfulfillment time: long took fulfill order paymentWe capture information direct measurements. include function creates data can see used simple, exponentially distributed data process components. may may true practice, seen similar patterns actual data.function produce many results want, order, payment fulfillment times represented exponentially distributed times seconds.Table 10.5: Wait time dataWe can use data build simulation deconstruct total time change inputs. Deconstructing entire time allow us demonstrate impact initiative mobile ordering. Mobile ordering enable decouple orders payment fulfillment effectively. careful think technology first . Mobile ordering may help decouple orders payments fulfillment. ways accomplish goal? Additionally, might help demonstrate merits effective buffering system simplified menu.","code":"\n#-----------------------------------------------------------------\n# Simulate wait times function\n#-----------------------------------------------------------------\nf_get_wait_times <- function(seed,n = 300,time,rate1,rate2,rate3){\nset.seed(seed)\n\norder_times       <- rexp(n, rate = rate1)\npayment_times     <- rexp(n, rate = rate2)\nfulfillment_times <- rexp(n, rate = rate3)\ntotal_time        <- order_times       + \n                     payment_times     + \n                     fulfillment_times\n\nwait_times <- data.frame(transaction  = seq(1,n, by = 1),\n                         orderTimes   = order_times,\n                         paymentTimes = payment_times,\n                         fulfillTimes = fulfillment_times,\n                         totalTime    = total_time)\nwait_times[] <- apply(wait_times,2,function(x) round(x,0))\nreturn(wait_times)\n}\n#-----------------------------------------------------------------\n# Create wait times data set\n#-----------------------------------------------------------------\nwait_times_a <- f_get_wait_times(seed  = 755,\n                                 n     = 300,\n                                 rate1 = .03,\n                                 rate2 = .06,\n                                 rate3 = .15)"},{"path":"chapter10.html","id":"analyzing-the-distributions-of-wait-time-components","chapter":"10 Operations","heading":"10.3.4.1 analyzing the distributions of wait time components","text":"variance processes problematic depending queuing discipline. First, let’s look distribution wait times.\nFigure 10.5: Distribution wait time components\ncan see graph order times appear widest variance. long tail extending toward 200-second mark. Variance kills interdependent processes. Variance enemy looking find. reduce variance, make entire process deterministic improve wait times.","code":"\n#-----------------------------------------------------------------\n# Simulate wait times function\n#-----------------------------------------------------------------\nwait_dist <- \n  wait_times_a                                            %>% \n    select(orderTimes,paymentTimes,fulfillTimes)          %>%\n    tidyr::pivot_longer(cols = c('orderTimes',\n                                 'paymentTimes',\n                                 'fulfillTimes'),\n                        names_to = \"measurement\",\n                        values_to = \"seconds\")            %>%\n                   mutate(scale_seconds = scale(seconds))\n\nw_dist <-  \n    ggplot(wait_dist, aes(x = seconds, fill= measurment)) +    \n    geom_density(alpha=.75)                               +\n    geom_rug(color='coral',sides = \"b\")                   +\n    scale_fill_manual(values=palette)                     + \n    xlab('Seconds')                                       +\n    ylab('Density')                                       + \n    ggtitle('Distribution of wait-time components')       +\n    graphics_theme_1 "},{"path":"chapter10.html","id":"simulating-a-component-of-our-process","chapter":"10 Operations","heading":"10.3.5 Simulating a component of our process","text":"Like many topics discussed, simulation large complicated topic. However, also beneficial problem facing. already looked couple ways simulate data. used rexp rnorm multiple times create simulated data sets. see figure 10.5. creating random numbers based parameters exponential normal distributions. data set want simulate different conditions? know distribution fit approximate data?Many software packages build simulations , helpful understand might function base level. ’ll create couple basic transaction simulations evaluate well specific changes process might impact overall system. show software package might help solve problem. underlying knowledge makes better decision-maker.Simulation daunting subject. Non-linear least squares fits can frustrating. saw chapter {#chapter6}, getting complex math easy.Table 10.6: Simulated wait times dataLet’s begin looking correlations process component. want see particular component correlates closely total time. packages build correlation tables , go ahead build manually.’ll reorder factors graph give us aesthetic seek.’ll use forcats (Wickham 2022a) library access fct_reorder function. help order plot.\nFigure 10.6: Correlation table\nOrder time component highly correlated Total-Time. factors correlated . infer sorts things data. Order time highly associated total time work can occur order. Fulfillment time may least correlated total time done excellent job buffering inventory. looking fundamental relationship variables. fact order time highly correlated variable important. ’ll likely base solution around mitigating long order times impact.","code":"\n#-----------------------------------------------------------------\n# Simulate wait times function\n#-----------------------------------------------------------------\nwait_times <- FOSBAAS::wait_times_data[1:300,]\n#-----------------------------------------------------------------\n# Build correlation table\n#-----------------------------------------------------------------\nwt_cor <- wait_times[,-1]\nnames(wt_cor) <- c('Order Time','Payment Time',\n                   'Fulfillment Time','Total Time')\n\nwt_cor_result      <- cor(wt_cor)\nwt_cor_result      <- round(as.data.frame(wt_cor_result), 2)\nwt_cor_result$type <- row.names(wt_cor_result)\n\ncor_long <- \n  tidyr::pivot_longer(wt_cor_result,\n                      cols = c('Order Time','Payment Time', \n                               'Fulfillment Time','Total Time'))\n#-----------------------------------------------------------------\n# Build correlation table\n#-----------------------------------------------------------------\ncor_long$order <- factor(cor_long$type, \n                         levels=c('Fulfillment Time',\n                                  'Payment Time',\n                                  'Order Time',\n                                  'Total Time'))\n#-----------------------------------------------------------------\n# Correlation table \n#-----------------------------------------------------------------\nlibrary(forcats)\ncorrelation_table <- \ncor_long                                                 %>%\nmutate(name = fct_reorder(name, value, .fun='sum'))      %>%\nggplot(aes(x = order, y = name,fill = value))            +\n  geom_tile()                                            +\n  geom_text(aes(label=value))                            +\n  scale_fill_gradient2(low  = \"mediumseagreen\", \n                     mid  = \"white\", \n                     high = \"dodgerblue\")                +\n  xlab('')                                               + \n  ylab('')                                               + \n  ggtitle('Correlation table')                           +\n  graphics_theme_1                                       +\n  theme(axis.text.x = element_text(size = 8),\n        axis.text.y = element_text(size = 8))"},{"path":"chapter10.html","id":"analyzing-the-variance-between-wait-time-components","chapter":"10 Operations","heading":"10.3.5.1 Analyzing the Variance between wait time components","text":"Variance enemy. system components linked depend correlated. result, also vast amount variance.\nFigure 10.7: Variance components\ncloser look specified quantiles paints clearer picture.Table 10.7: quantiles wait timesTwenty-five percent orders take forty-two seconds . long order times appear biggest problem. accurate say process component likely causes variance system. Let’s construct simulation looking probability table observations. can build function create table little extra information. also beneficial data. One day transactions undoubtedly create sample-size issues. Let’s simulate thousand results. ’ll pretend collecting information weeks. Keep mind also cause problems. using regression , might want consider mixed effects 97 model.Since total time comprises three numerical process components, regression explain variance well. regressed data, model look amazing. Let’s try .Table 10.8: Summary stats time modelAs expected, model almost perfect. use equation estimate total wait times almost perfectly. see results like , probably something wrong.","code":"#-----------------------------------------------------------------\n# Box plot of wait times\n#-----------------------------------------------------------------\nwait_long <- \n  tidyr::pivot_longer(wait_times,cols = c('orderTimes',\n                                          'paymentTimes', \n                                          'fulfillTimes',\n                                          'totalTime'))\nwait_box <- \nwait_long                                             %>%\n  mutate(name = fct_reorder(name, value, .fun='sum')) %>%\n  ggplot(aes(x = name, y = value)) +\n  geom_boxplot(fill = 'dodgerblue') +\n  xlab('\\n Transaction component')                              + \n  ylab('Wait time in seconds)                                  + \n  ggtitle('Variance of transaction times')                      +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Quantiles\n#-----------------------------------------------------------------\nquantiles <-\napply(wait_times[,-1],\n      2,\n      function(x) quantile(x, probs = c(.1,0.25, 0.5, 0.75,.9)))\n#-----------------------------------------------------------------\n# Simple linear model for total times\n#-----------------------------------------------------------------\ntime_mod <- lm(totalTime ~ fulfillTimes + paymentTimes + \n                           orderTimes,data = wait_times)\n\nstats_time_mod <- \ntibble::tibble(\n  st_error     = unlist(summary(time_mod)[6]),\n  r_square     = unlist(summary(time_mod)[8]),\n  adj_r_square = unlist(summary(time_mod)[9]),\n  f_stat       = unlist(summary(time_mod)$fstatistic[1])\n)"},{"path":"chapter10.html","id":"building-a-simulation-of-our-data","chapter":"10 Operations","heading":"10.3.5.2 Building a simulation of our data","text":"Finally, let’s build simulation. section demonstrate basics get started. method follow flexible can used many different ways. ’ll begin accessing wait_times_distribution data.Let’s sample wait times basis simulated data.Now can think distribution carefully. can use ecdf function compute empirical cumulative distribution function. output inverse quantile function.Table 10.9: Demonstrating relationship quantiles ecdfAbout 78 percent observations less 50 seconds. Therefore, plotting function visualize just simple.\nFigure 10.8: Order time ecdf\ncan use function simulate data fits distribution. Let’s take random sample data range order times.can use histogram data compare actual order data.\nFigure 10.9: Simulated vs. actual results\nsimulated data approximates actual data well. ; know used exponential distribution fit data.can’t trust results one simulation. ’ll want create many simulations average results. Let’s go ahead put together code base allow us produce reproducible experiment. First, let’s ask question want answer:decreased order times fifty percent, change average wait time?now five-hundred simulations phase total wait. Let’s take look averages.Table 10.10: Reducing impact order timesThe total average time almost identical observed. can reduce order time fifteen seconds, total time reduce 41 seconds. However, seen, tells part story. real problem variance. However, distributions fact payment, fulfillment, orders correlated, variance unlikely hurt us often.Let’s use simulation calculate many orders fulfilled specific time frames per point--sale duress. Duress means queue always full. Let’s ask question answer different ways.one hour, much additional throughput register able process order taking reduced 50%?average wait time fifty-six seconds, can fulfill sixty-four orders per register per hour (60 seconds/56 seconds) * 60 minutes. number look like use samples simulation?Let’s graph results count list see many orders fulfilled thirty simulated hours.\nFigure 10.10: Simulated vs. actual results\nthirty simulations, eight fell level average service time. compound analysis multiple registers, inter-arrival times, different line lengths, can see reducing variance key consistently maximizing throughput. also discussed slack time orders, fatigued employees, factors add variance process.","code":"\n#-----------------------------------------------------------------\n# Access the distribution data\n#-----------------------------------------------------------------\nlibrary(FOSBAAS)\nwait_times_distribution <- FOSBAAS::wait_times_distribution_data\n#-----------------------------------------------------------------\n# Get a sample of the data\n#-----------------------------------------------------------------\nset.seed(755)\nwait_sample <- wait_times_distribution %>%\n               sample_frac(size = .7)\n#-----------------------------------------------------------------\n# Cumulative distribution function\n#-----------------------------------------------------------------\norders <- wait_sample$orderTimes \ncdf    <- ecdf(orders)  \n#-----------------------------------------------------------------\n# Cumulative distribution function\n#-----------------------------------------------------------------\ncdf_out <- cdf(50)\nqua_out <- quantile(wait_sample$orderTimes,probs = cdf_out)\n\necdf_quant <- tibble::tibble(ecdf_50 = cdf_out,\n                             quantile_ecdf_50 = qua_out)\n#-----------------------------------------------------------------\n# Cumulative distribution function plot\n#-----------------------------------------------------------------\ncdf_plot <- \nggplot(wait_sample, aes(orders))            + \n  stat_ecdf(geom = \"step\",\n            color = 'dodgerblue',\n            size = 1.1)                     +\n  xlab('\\n Orders')                         + \n  ylab('Percentage observations')           + \n  ggtitle('ECDF Order Time')                +\n  geom_vline(xintercept = 50.22143,lty = 2) +\n  geom_hline(yintercept = .7785714,lty = 2) +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Simulate orders \n#-----------------------------------------------------------------\nset.seed(715)\nn <- 400 # observations\nsim_orders <- rexp(n, rate=1/mean(wait_sample$orderTimes))\n#-----------------------------------------------------------------\n# Compare histograms\n#-----------------------------------------------------------------\nhist_tab <- \n tibble::tibble(sim_data = sim_orders,\n                actual_data = sample(wait_sample$orderTimes,400))\n\nhist_tab_long <- hist_tab %>% \n                 tidyr::pivot_longer(cols = c('sim_data',\n                                              'actual_data'),\n                                     names_to = \"measurement\",\n                                     values_to = \"seconds\") \n\nhist_comp <- \nggplot(hist_tab_long , aes(seconds))     + \n  facet_grid(.~measurment)               +\n  geom_histogram(fill = 'dodgerblue')    +\n  xlab('\\n seconds')                     + \n  ylab('count')                          + \n  ggtitle('Simulated vs. actual values') +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Simulate, Iterate, and aggregate\n#-----------------------------------------------------------------\nn <- 500 # observations\n\nsim_orders  <- list()\nsim_pay     <- list()\nsim_fulfill <- list()\n# Iterate\nfor(i in 1:500){\nset.seed(i + 715)\n# Simulate\n  sim_orders[[i]]  <-  \n    rexp(n, rate=1/mean(wait_sample$orderTimes))\n  sim_pay[[i]]     <-  \n    rexp(n, rate=1/mean(wait_sample$paymentTimes))\n  sim_fulfill[[i]] <-  \n    rexp(n, rate=1/mean(wait_sample$fulfillTimes))\n}\n# Aggregate\nmean_order   <- mean(sapply(sim_orders, mean))\nmean_pay     <- mean(sapply(sim_pay, mean))\nmean_fulfill <- mean(sapply(sim_fulfill, mean))\nmean_total   <- mean_order + mean_pay + mean_fulfill\n#-----------------------------------------------------------------\n# Aggregated results\n#-----------------------------------------------------------------\nmean_chart <- tibble::tibble(order   = mean_order,\n                             payment = mean_pay,\n                             fulfill = mean_fulfill,\n                             total   = mean_total)\n#-----------------------------------------------------------------\n# Results with variance\n#-----------------------------------------------------------------\ntotal_time_samp <- sim_orders[[1]]  +\n                   mean_pay[[1]]    + \n                   mean_fulfill[[1]]\ncount_list <- list()\nset.seed(715)\n\nfor(j in 1:30){\n  time            <- 0\n  count           <- 1\n  seconds_in_hour <- 60*60\n    while(time <= seconds_in_hour){\n      i           <- sample(total_time_samp,1)\n      time        <- time + i\n      count       <- count + 1\n    }\n  count_list[j]   <- count - 1\n}\n#-----------------------------------------------------------------\n# Observe variance in fans serviced per hour\n#-----------------------------------------------------------------\ncounts <- tibble::tibble(fans_serviced = unlist(count_list),\n                             simulation    = seq(1:30))\n\nservice_per_hour <- \nggplot(counts , aes(x = simulation,\n                    y = fans_serviced))  + \n  geom_line(color = 'dodgerblue')        +\n  xlab('\\n simulation')                  + \n  ylab('Fans serviced')                  + \n  ggtitle('Simulated services per hour') +\n  geom_hline(yintercept = 64,lty = 4)    +\n  graphics_theme_1"},{"path":"chapter10.html","id":"fitting-distributions","chapter":"10 Operations","heading":"10.4 Fitting distributions","text":"Let’s look ways fit distributions data. First, ’ll build helper function give us frequency table values.’ve already created data set. Access package.Table 10.11: Frequency table order timeThis frequency table represents histogram. First, let’s take look graph.\nFigure 10.11: ecdf\ncode gives us cumulative distribution graph looked earlier. ecdf helpful, many ways simulate data. First, let’s look sampling strategies build distributions. following function produce frequency table us return specified number simulated values.can use function simulate 1,000 ticket sales.can observe whole distribution following code.\nFigure 10.12: Secondary Prices\nsimulated distribution follows exact pattern.\nFigure 10.13: Simulated prices\ncan also simulate data several built-distributions. example, following code produces distribution combines beta, exponential, normal distribution.\nFigure 10.14: Simulated prices\ncan see, easy simulate distributions R. can use tools construct Monte Carlo simulations sophisticated -analysis. following sections cover fitting different models data. libraries much work. Let’s look potential fits data begin three standard options:exponential fitA polynomial fitA generalized additive modelWe’ll also attempt fit logit curve demonstrate important point.Remember fit polynomial three degrees, asking trouble. demonstrate good might look, bad idea. asking unpredictable model. please don’t .’ll something little differently fit logit curve. ’ll use SSlogis function stats package estimate starting points curve. valuable function, keep mind.get error. ? logit curve poor fit data set. Bad fits common problem. can’t fit typical distribution well? options. included fifth-degree polynomial fit (, terrible practice) generalized additive model, ’ll introduce one . Let’s fit spline98.Now fit models, can judge best? can start looking well fit data.\nFigure 10.15: Variance components\nmodels fit cumulative data well, except exponential polynomial fit. several ways evaluate models one another. linear models, ANOVA typically first stop. non linear models, AIC 99 BIC typically used.Running get_diagnostics creates following table.Table 10.12: Model ComparisonsThe exponential line isn’t best fit. Plus, select random numbers frequency plug equation simulated distribution won’t approximate data well. let’s simulate results using polynomial fit.can write function spit new value based polynomial model.bad function. know ? better generalize something like :better, get idea. function work polynomial function length less readable hard-coded version. Functions aren’t worth much going hard-code .plug result earlier, get following:close 50 got ecdf function. Let’s simulate values graph .Wow. Figure 10.16 demonstrates horrible fit! one reason don’t use high-order polynomials modeling data.\nFigure 10.16: Polynomial fit\njust covered basics fitting distributions. now tools explain y terms x various ways. main takeaway must careful fit. methods work better others; aren’t careful, can things might make look silly.","code":"\n#-----------------------------------------------------------------\n# Function to build a frequency table\n#-----------------------------------------------------------------\nf_build_freq_table <- function(variable){\n  \n  pr          <- as.data.frame(table(variable))\n  pr$prob     <- pr$Freq/sum(pr$Freq)\n  pr$variable <- as.numeric(as.character(pr$variable))  \n  \n  return(pr)\n\n}\norder_freq <- f_build_freq_table(wait_sample$orderTimes)\n#sum(order_freq$prob)\n#-----------------------------------------------------------------\n# Access frequency table data\n#-----------------------------------------------------------------\nfreq_table  <- FOSBAAS::freq_table_data\n#-----------------------------------------------------------------\n# Graph frequency table data\n#-----------------------------------------------------------------\nfreq_table$cumprob <- cumsum(freq_table$prob)\nfreq_table_graph <- \n  ggplot(freq_table,aes(x = variable,y=cumprob)) +\n  geom_line(size = 1.2,color = 'dodgerblue')     +\n  xlab('\\n Seconds')                             + \n  ylab('Percent of Values')                      + \n  ggtitle('Table of values')                     +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Function to build frequency table\n#-----------------------------------------------------------------\nf_get_prices <- function(roundedPrice,n){\n  \n  freq_table <- as.data.frame(prop.table(table(roundedPrice)))\n  as.numeric(as.character(sample(freq_table$roundedPrice, \n                                 n, \n                                 prob=freq_table$Freq, \n                                 TRUE)))\n  \n}\n#-----------------------------------------------------------------\n# Access secondary market data\n#-----------------------------------------------------------------\nprice_data  <- FOSBAAS::secondary_data\nprice_table <- f_get_prices(price_data$price,1000)\n#-----------------------------------------------------------------\n# Actual secondary prices\n#-----------------------------------------------------------------\nx_label  <- ('\\n Ticket Prices')\ny_label  <- ('Count \\n')\ntitle    <- ('Distribution of Secondary Ticket Prices')\nhist_sales <- \n  ggplot2::ggplot(data = price_data,\n                  aes(x = price))                      +\n  geom_histogram(binwidth = 5, fill = 'steelblue')     +\n  geom_rug(color = 'coral')                            +\n  scale_x_continuous(label = scales::dollar)           +\n  scale_y_continuous(label = scales::comma)            +\n  xlab(x_label)                                        + \n  ylab(y_label)                                        + \n  ggtitle(title)                                       +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Simulated secondary prices\n#-----------------------------------------------------------------\nprice_table <- as.data.frame(price_table)\n\nx_label  <- ('\\n Simulated Ticket Prices')\ny_label  <- ('Count \\n')\ntitle    <- ('Simulated Distribution: n = 1,000')\nhist_sales <- \n  ggplot2::ggplot(data = price_table,\n                  aes(x = price_table))                +\n  geom_histogram(binwidth = 5, fill = 'steelblue')     +\n  geom_rug(color = 'coral')                            +\n  scale_x_continuous(label = scales::dollar)           +\n  scale_y_continuous(label = scales::comma)            +\n  xlab(x_label)                                        + \n  ylab(y_label)                                        + \n  ggtitle(title)                                       +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Simulated combined distribution\n#-----------------------------------------------------------------\ndist <- c(rbeta(600, 0.5, 5, ncp = 2),\n          rexp(300,.57),\n          rnorm(100,.2,.05)) \n\nx_label  <- ('\\n Values')\ny_label  <- ('Count \\n')\ntitle    <- ('Simulated Combined Distribution: n = 1,000')\nhist_sales <- \n  ggplot2::ggplot(data = dist_table,\n                  aes(x = dist))                       +\n  geom_histogram(binwidth = .5, fill = 'steelblue')    +\n  geom_rug(color = 'coral')                            +\n  scale_x_continuous(label = scales::comma)            +\n  scale_y_continuous(label = scales::comma)            +\n  xlab(x_label)                                        + \n  ylab(y_label)                                        + \n  ggtitle(title)                                       +\n  graphics_theme_1\n#-----------------------------------------------------------------\n# Apply fits\n#-----------------------------------------------------------------\nlibrary(mgcv)\nfreq_table$cumprob <- cumsum(freq_table$prob)\n#-----------------------------------------------------------------\n# Exponential fit\nfit_ex <- \n  nls(variable ~ a*cumprob^m, data = freq_table, \n      start = list(a = 300,m=.15)) \nfreq_table$pred_exp <- predict(fit_ex)\n#-----------------------------------------------------------------\n# Polynomial fit\nfit_py <- \n  lm(freq_table$variable~poly(freq_table$cumprob,5,raw=TRUE))\nfreq_table$pred_poly <- predict(fit_py)\n#-----------------------------------------------------------------\n# GAM fit\nfit_gm <- \n  mgcv::gam(variable ~ s(cumprob),data = freq_table)\nfreq_table$pred_gam <- predict(fit_gm)\n#-----------------------------------------------------------------\n# Apply logit fit\n#-----------------------------------------------------------------\nfit_lt <- nls(variable ~ SSlogis(cumprob, Asym, xmid, scal), \n              freq_table)\ncof    <- coef(summary(fit_lt))\n\nfit <- nls(variable ~ A/(1 + exp(((-I+cumprob)/S))), \n           data = freq_table,  \n           start = list(A = cof[1],I= cof[2],S = -cof[3]), \n           control = list(maxiter  =  10000), trace=TRUE)\n#-----------------------------------------------------------------\n# Spline fit\n#-----------------------------------------------------------------\nfit_sp <- with(freq_table, smooth.spline(cumprob, variable))\nfreq_table$pred_sp <- predict(fit_sp)$y\n#-----------------------------------------------------------------\n# Observe fit data\n#-----------------------------------------------------------------\n\ndist_fits <- \nggplot(freq_table,aes(y = cumprob,x = variable))            +\n       geom_point(alpha = .5,size = 1)                      +\n  geom_line(aes(x = pred_exp), size = 1.1 , lty = 2, \n            color = 'dodgerblue')      +\n  geom_line(aes(x = pred_poly), size = 1.1, lty = 3, \n            color = 'mediumseagreen') + \n  geom_line(aes(x = pred_gam), size = 1.1, lty = 4, \n            color = 'coral')  +\n  geom_line(aes(x = pred_sp), size = 1.1, lty =5, \n            color = 'orchid')  +\n  ylab('\\n Cumulative Probability')                          + \n  xlab('Order time in seconds')                              + \n  ggtitle('Distribution of order times')                     + \n  graphics_theme_1\n#-----------------------------------------------------------------\n# Compare fits\n#-----------------------------------------------------------------\nmodels <- list(expon  = fit_ex,\n               poly   = fit_py,\n               gam    = fit_gm)\n\nget_diagnostics <- function(mods){\n  mods <- models\n  aics   <- lapply(mods, function(x) AIC(x))\n  bics   <- lapply(mods, function(x) BIC(x))  \n  frame <- as.data.frame(matrix(nrow = length(mods), ncol = 3))\n    frame[,1] <- names(mods)\n    frame[,2] <- unlist(aics)\n    frame[,3] <- unlist(bics)\n  names(frame) <- c('model','AIC','BIC')\n  return(frame)\n \n}\n\nmodels_table <- get_diagnostics(models)\n#-----------------------------------------------------------------\n# get fit\n#-----------------------------------------------------------------\nf_get_fifth_degree_fit <- function(new_var,dist_fit){\n  var <-  coef(dist_fit)[1]              + \n         (coef(dist_fit)[2] * new_var    + \n         (coef(dist_fit)[3] * new_var^2) + \n         (coef(dist_fit)[4] * new_var^3) +\n         (coef(dist_fit)[5] * new_var^4) + \n         (coef(dist_fit)[6] * new_var^5))\n  return(var)\n}\n#-----------------------------------------------------------------\n# Generalized polynomial fit\n#-----------------------------------------------------------------\nf_get_poly_fit <- function(new_var,dist_fit){\n\n  len_poly   <- length(dist_fit$coefficients)\n  exponents  <- seq(1:(len_poly-1))\n  value_list <- list()\n\n  for(i in 1:length(exponents)){\n    value_list[i] <- coef(dist_fit)[i+1] * new_var^exponents[i]\n  }\n  sum(do.call(sum, value_list),coef(dist_fit)[1])\n}\n#-----------------------------------------------------------------\n# Equation output\n#-----------------------------------------------------------------\nf_get_fifth_degree_fit(.7785714,fit_py)\n#> (Intercept) \n#>    45.59297\n#-----------------------------------------------------------------\n# get fit\n#-----------------------------------------------------------------\npoly_fit <- \nsapply(seq(0,1,by=.005),\n       function(x) f_get_fifth_degree_fit(x,fit_py))\n\npoly_values <- tibble::tibble(y = seq(0,1,by=.005),\n                              x = poly_fit)\npoly_graph <- \nggplot(poly_values,aes(x=x,y=y))             +\n  geom_line(size = 1.5, lty = 3, \n  color = 'mediumseagreen')                  +\n  ylab('\\n Cumulative Probability')          + \n  xlab('Order time in seconds')              + \n  ggtitle('Simulated order times')           + \n  graphics_theme_1"},{"path":"chapter10.html","id":"queuing","chapter":"10 Operations","heading":"10.5 Understanding queuing systems","text":"Queuing vast complicated field study, principles can applied many problems, Computer Engineering queuing systems amusement parks airports. face sports. Queuing systems typically three parts described “Operations Management” Heizer Rendee: (Jay Heizer 2014)Arrivals inputs systemQueue discipline, waiting line itselfThe service facilityEach components specific characteristics. instance, arrivals consider following:size arrival populationThe behavior arrivalsThe pattern arrivalsAnalyzing queuing models tends make heavy use simulation requires specific forms data collection. However, underlying assumptions correct, analyzing data becomes exercise plugging good data equations. ’ll adapt examples “Fundamentals Queuing Systems” (Thomopoulos 2012).Let’s imagine concessions concept six points sale queuing space can accommodate 50 people. queuing parlance, represents (M/M/k/N) queue representing multi-server, finite capacity system. 1/lambda represents average time arriving customers 1/mu represents average service times. assuming exponentially distributed inter-arrival times service times. reality, ’ll make observations determine distribution service time inter-arrival times. Let’s begin defining terms, ’ll work problem using R.Average time arrivals:\n\\[\\begin{equation}\n\\tau_{} = {1}/{\\lambda}\n\\end{equation}\\]Average time service units:\n\\[\\begin{equation}\n\\tau_{s} = {1}/{\\mu}\n\\end{equation}\\]Average number arrivals per unit time:\n\\[\\begin{equation}\n\\lambda\n\\end{equation}\\]Average number units processed unit time continuously busy service facility:\n\\[\\begin{equation}\n\\mu\n\\end{equation}\\]number service facilities:\n\\[\\begin{equation}\nk\n\\end{equation}\\]Utilization ratio:\n\\[\\begin{equation}\n\\rho = \\tau_{}/\\tau_{s} = \\lambda/\\mu \\text{ : } \\rho/k \\text{< 1 needed ensure system equilibrium}\n\\end{equation}\\]number units system:\n\\[\\begin{equation}\nn \\text{ : n} \\geq \\text{0}\n\\end{equation}\\]can easily translate equations figures R. encounter Greek letters, recommendation write . sort analysis can quickly get confusing. Let’s carefully define initial inputs.Imagine concession concept six points sale space 50 people line. takes 60 seconds service customer, customer arrives line every 110 seconds.R several tricks can use make calculations easier. Since variables vectorized, adding sequences easy. need loop. First, need calculate probability n (defined ) equals 0. given slightly alarming-looking equation:Probability n = 0:\n\\[\\begin{equation}\nP_{0} = 1/\\{\\sum_{n=0}^{k-1} \\rho^n/n! + \\rho^k/k![(k^{N-k+1} - \\rho^{N-k+1})/(k - \\rho)k^{N-k}]\\}\n\\end{equation}\\]Pay attention order operations. tricky part translating equation getting parenthesis correct places.running code, see P0 = 0.4305395. complete exercise calculating probability n units system two equations different levels n. n = (0,k)\n\\[\\begin{equation}\nP_{n} = \\rho_{n}/n!P_0\n\\end{equation}\\]n = (k + 1,N)\n\\[\\begin{equation}\nP_{n} = \\rho_{n}/[k!k^{n-k}]P_{0}\n\\end{equation}\\]can write helper function help us reconcile two options.following function accepts inputs spits desired output.Executing function returns dataframe representing list values.Table 10.13: f_get_MMKN outputWe now basic understanding analyze queues. stuff can get complex, multitude software packages can take care . However, plugging information equations, difficult part getting good data choosing correct models accommodate . lots variation; instance, inter-arrival times may normally distributed instead exponentially distributed. ’ll adapt model accommodate variation.","code":"\n#-----------------------------------------------------------------\n# Define terms for queuing equation\n#-----------------------------------------------------------------\nlambda                # Average arrivals per unit of time\nmu                    # Average number of units processed \nk      <- 6           # Number of points of sale\nN      <- 50          # Number that the queue can accommodate\ntau_a  <- 1/lambda    # Average time between arrivals: 110 seconds\ntau_s  <- 1/mu        # Average service time: 90 seconds\nrho    <- tau_a/tau_s # Utilization ratio\nn                     # Units in the system\n#-----------------------------------------------------------------\n# M/M/k/N Queue Inputs\n#-----------------------------------------------------------------\nk        = 2          # Number of points of sale at the concept\nN        = 5          # Number of people the queue can accommodate\ntau_a    = 10         # time between arrivals: 40 seconds/60\ntau_s    = 8          # Service time: 90 seconds/60 = 1.5\nlambda   = 1/tau_a    # Customer Arrivals per minute\nmu       = 1/tau_s    # Serviced customers per minute\n#-----------------------------------------------------------------\n# Translate to per-hour figures\n#-----------------------------------------------------------------\nlambda_h = 60/tau_a   # Per hour\nmu_h     = 60/tau_s   # Per hour\nrho      = lambda/mu  # Utilization ratio\n#-----------------------------------------------------------------\n# M/M/k/N Calculating the Probability that n = 0\n#-----------------------------------------------------------------\n# Create the sequence for the P0 equation\nn   = seq(0, N-1, by = 1 )\n# Translate the equation into R:\nP0 <- \n 1/ sum(((rho^n)/factorial(n)) + ((rho^k)/\n(factorial(k)*((k^(N-k+1)) - (rho^(N-k+1))/((k-rho)*(k^(N-k)))))))\n#-----------------------------------------------------------------\n# M/M/k/N  Probability of n units in the system\n#-----------------------------------------------------------------\n# For n = (0,k)\nnk0 = seq(0, k, by = 1 )\nPnk0 <- rho^nk0/factorial(nk0)*P0\n\nsum(Pnk0)\n\n# For n = (k + 1,N)\nnk1 = seq(k + 1, N, by = 1 )\nPnk1 <- rho^nk1/(factorial(k)*k^(nk1-k))*P0\n\nsum(Pnk1)\n\nround(sum(Pnk0,Pnk1),2)\n#-----------------------------------------------------------------\n# Calculate figures\n#-----------------------------------------------------------------\nlambda_e <- lambda*(1 - Pnk1) # Lambda Effective\nrho_e    <- lambda_e/mu       # rho Effective\n# Expected in queue\nLq = sum((n-k)*Pnk0)\nLq = sum((n-k)*Pnk1)\nLs = rho_e\n# expected units in the system\nL = Ls + Lq\n# Expected service time\nWs = Ls/lambda_e # minutes in service\nWq = Lq/lambda_e # minutes in queue\nW = L/lambda_e   # minutes in system\n\n#-----------------------------------------------------------------\n# Calculate figures\n#-----------------------------------------------------------------\nf_get_MMKN <- function(k,N,ta,ts){\n  \n  lambda = 1/ta #: per minute\n  mu     = 1/ts #: per minute\n  rho    = lambda/mu #: utilization ratio\n  \n#-----------------------------------------------------------------\n  # Probability of n units in the system\n  # for\n  n = seq(0, N-1, by = 1 )\n  P0 <- 1/ sum(((rho^n)/factorial(n)) + \n               ((rho^k)/(factorial(k)*((k^(N-k+1)) - \n                   (rho^(N-k+1))/((k-rho)*(k^(N-k)))))))\n  \n  # Probability of n units in the system\n  # for\n  n = seq(0, k, by = 1 )\n  Pn0 <- rho^n/factorial(n)*P0\n  \n  # for\n  n = seq(k + 1, N, by = 1 )\n  Pn1 <- rho^n/(factorial(k)*k^(n-k))*P0\n  \n  Pn      <- c(Pn0,Pn1)\n  \n#-------------------------------------------------------------------\n  # calculations\n  len     <- max(length(Pn))\n\n  lambda_e  <- lambda*(1 - Pn[len])\n  rho_e    <- lambda_e/mu\n  \n  # Expected in queue\n  Ls = rho_e #   Ls = 1*Pn[2] + 2*sum(Pn[-c(1,2)])\n  \n  # for\n  n = seq(k+2, N + 1, by = 1 )\n  Lq = sum((n-(k+1))*Pn[n]) # Lq = 1*Pn[4] + 2*Pn[5] + 3*Pn[6]\n\n  # expected units in the system\n  L = Ls + Lq\n  \n  # Expected service time\n  Ws = Ls/lambda_e # minutes in service\n  Wq = Lq/lambda_e # minutes in queue\n  W  = Wq + Ws   # minutes in system\n  \n#-------------------------------------------------------------------\n  # Build output\n  frame <- data.frame(matrix(nrow = 7,ncol =2))\n  names(frame) <- c('Metric','Value')\n  \n  metric <- c('Servers:','System Capacity:',' time between arrivals:',\n              'Average service time:','Minutes in service:',\n              'Minutes in queue:','Minutes in system:')\n  values <- c(k,N,ta,ts,Ws,Wq,W)\n  \n  frame[,1] <- metric\n  frame[,2] <- values\n\n  return(frame)\n  \n}\n#-----------------------------------------------------------------\n# Run our function\n#-----------------------------------------------------------------\nFOSBAAS::f_get_MMKN(2,5,10,8)"},{"path":"chapter10.html","id":"key-concepts-and-chapter-summary-9","chapter":"10 Operations","heading":"10.6 Key concepts and chapter summary","text":"Analytic techniques can applied wide variety operations problems. Additionally, Operations broad field study covers many different arenas. chapter covered two major subjects: Simulation Queuing. examples covered increasing throughput concessions. scratched surface, introduced several topics:Simulation: Monte Carlo simulations relatively simple create incredibly useful evaluating various topics.Distribution fitting: Leveraging distributions critical simulation. Learn think distributions, point estimates. Distributions everywhere.Queuing analysis: Queues exist everywhere, electrical circuits concession stands. Analyzing queuing system can give insight improve .essential project management tipsWe also learned think problems. easy intellectually lazy take time evaluate situation honestly. Operations critical thinking regarding system many interrelated components. also widely studied field, can conceptualize problem correctly, able find way analyze improve .","code":""},{"path":"Chapter11.html","id":"Chapter11","chapter":"11 Epilogue","heading":"11 Epilogue","text":"covered variety subjects related foundational elements business strategy sports business setting. setting specific working club. focused building strategies increase ticket sales revenue benefit business operations. Additionally, shown several concepts two different parts. Chapters 1-4 cover foundational elements needed analyze data. Chapters 5-10 looks examples problems face demonstrates techniques can use solve .Chapter 1 introduced us think use data. Using data fundamental business strategyChapter 2 introduced us specific data sets create using R language. knowledge programming still critical analysis.Chapter 3 demonstrated build interpret foundational graphs using R. also covered summarizing exploring data.Chapter 4 discussed frame think problems face. also saw essential project management techniques.Chapter 5 introduced us customer segmentation. look several methods discuss use data.Chapter 6 covers basic pricing forecasting principles.Chapter 7 extends segmentation lead scoring.Chapter 8 demonstrated consider promotions covers brand management conceptsChapter 9 introduces formal research principles including hypothesis testing sampling.Chapter 10 covered techniques related stadium operations. Understanding simulation critical solving wide variety operations problems.covered lot material yet scratch surface. struggled even cover subjects real depth. Let’s look four goals discussed beginning book discuss whether accomplished.give analytic toolboxTo teach sports team’s business worksTo apply knowledge real problems achieve desired outcomesTo build reference manual solutions common problemsUltimately, knowledge coalesces strategic thought. aren’t born understanding business strategy. foundations strategy built experience. Analytics tightly coupled strategy. touches every functional unit business helps produce answers questions strategist manager devises. Let’s review analytics toolbox.","code":""},{"path":"Chapter11.html","id":"your-analytics-toolbox","chapter":"11 Epilogue","heading":"11.1 Your analytics toolbox","text":"covered lot code book introduced several concepts. high level, covered following:manipulate summarize data coderegressionHow implement machine learning algorithmsFormal research methodsSimulation applicationsBuilding graphicsProblem-solving framing projectsWe also covered several techniques, code can use implement techniques, interpret results. However, lot covered.Neural Networks ..exotic forms regression (mixed effects models, etc.)Bayesian methodsOther ensemble learning methodsDeploying web apps reporting information disseminationWe deliberately didn’t cover subjects. need cover everything. understand think problem, applying specific algorithm trivial data sets proper place. also hundreds algorithms, -less things context:Predict numerical valuePredict classDemonstrate structureWe didn’t cover tools convolutional neural networks computer vision speech recognition. don’t need , specialists can better us. instance, can deploy facial recognition entering suite. can also deploy speech recognition natural language processing replace sales staff. might want spatial analysis park using digital twin. Ultimately, vendors work much efficiently. need know tools leverage . don’t want salesperson wowing executives “..” isn’t even right tool.","code":""},{"path":"Chapter11.html","id":"sports-business","chapter":"11 Epilogue","heading":"11.2 Sports business","text":"also discussed think problems understand distinctions sports industries. Sports unique, outcomes often control. can’t just apply techniques analysis learned business school sports business constricted many ways. Geographic boundaries, agency agreements, labor issues, many unique circumstances exist. heart, many leagues simply trade organizations. work revolves around constrained optimization exercise. ’s similar operating mutual hedge fund. Sometimes ’s , sometimes ’s . job navigate seas think problems advance abstractly. show team path success.Individuals also attracted sports variety reasons, influence thought. Clubs deal public relations issues along variety avenues. Politicians can appropriate inopportune times. brand critical, aren’t always control. public relations scandal? didn’t consider covering strategic problem, understood. proactively protect brand public relations corporate communication? issues impact corporate sponsorship ticket sales? measures defensive offensive. know making right choices? may know. best can.","code":""},{"path":"Chapter11.html","id":"problems-faced-by-a-club","chapter":"11 Epilogue","heading":"11.3 Problems faced by a club","text":"Additionally, covered many real-world problems. first part book concerned main subjects:Understanding context analytics strategy within sports business (specifically club)Building understanding standard data sets (introduce R data)Exploring data (Understanding build common graphs use)Framing Projects (Understanding think problems strategic fashion)second half book covered application analytic techniques interpretation results. covered:SegmentationPricingLead scoringPromotionsResearchOperationsWe covered subjects related functional departments exercises make covered possible. functions live .T. live Executive level. related subjects include:Asset valuation corporate sponsorship departmentsBuilding dashboards business intelligence functionsConstructing ETLsAdditionally, didn’t cover higher-order strategic problems. problems might involve customers, asset classes, legal environment. Examples strategic issues include:Media rightsdigital rightsOther issues politics legal issuesGrowing top--funnel audiences (looking Gen Z Gen Alpha)Business development growthMergers aquisitions","code":""},{"path":"Chapter11.html","id":"how-do-i-learn-more","chapter":"11 Epilogue","heading":"11.4 How do I learn more","text":"Google . Seriously. million resources learning R Python online. can take free classes access entire books subjects like programming free. can said machine learning techniques. However, comes warning. Techniques change rapidly. Techniques random forests support vector machines may destined scrap pile neural networks become easier deploy. TensorFlow (tensorflow 2020) accelerated trend several years ago.Additionally, student, take classes network. Contact someone club ask can work project school. Finally, already work industry, expand horizons. Don’t get locked working narrow projects. Look expand influence throughout office. broader work experience, better ’ll everything .also scores books various subjects related analytics. topic broad naturally gravitate toward specific components. may prefer visualization building databases. might fall love modeling statistics. Figure parts analytics like good , purchase used book, read , try duplicate can find. R, Python, TensorFlow, many technologies can used free. Download get started data. Thousands data sets can even found embedded R Python. R, can find simple command:can see available data sets using variation preceding function:Find interesting data sets use . Many packages also excellent documentation full-scale vignettes. Please take advantage . ’ll surprised much learn time focus.","code":"\ndata(package='ggplot2')\ndata(package = .packages(all.available = TRUE))"},{"path":"Chapter11.html","id":"what-does-the-future-hold","chapter":"11 Epilogue","heading":"11.5 What does the future hold","text":"Many examples book solved one degree another. problems well understood within outside sports. instance, explored pricing rudimentary standpoint. Dynamically pricing tickets based variable demand levels happening industry (applying specific algorithms) forty years. Probably much longer. technique commodity, good thing. nascency analytics revolution, analyst may considered malevolent presence board room. now necessity, influence continue grow. skills needed wield analytics team prerequisite future leadership. Working field best way get experience. However, might even helpful use skill sets directly specializing another vertical finance. life winding path isn’t linear.time, see functions integrated CRM systems, skill sets become commoditized students continue flock analytcs oriented degree programs. neither bad good. look analytics similar vein Computer Engineering degrees. don’t need one-million people Computer Engineering degrees. know call second-rate computer engineer? bartender. might also run call center .T. work. point need good people build better processors. Analytics similar. don’t need armies; need people good .Furthermore, analytics begin change workforce sports. Chatbots increasingly take sales service personnel. Accountants similarly doomed. formulaic, short time. workforce changes aren’t limited sports. better voting universal basic income machines coming white-collar jobs next, already .Media continue change rapidly. GenZ GenAlpha (born 2000) continue shape media consumption content strategy. factors impact sports consumption media , even critically, -person. trick attracting attention-deficit-disordered youths? Nobody knows yet, literature says something like (Jeff Fromm 2018):“Start humanizing brand. means giving brand personality consumers can engage .”Communication rules changed. Interactions hardware seem almost silly:“Pivitols learned swipe speak. Attempting swipe unswipable–like T.V. screens pages magazine–assumed image front broken.”admit swiped magazine page. Gen Z interacts social media technology much intuitively older generations. Sports adapting continue adapt. still trying figure answer, interfacing young people huge strategic problem must considered investment. club level, isn’t easy think marketing initiatives context investment. , don’t, might find media equivalent professional fishing code equivalent FoxPro. Remember FoxPro?Stadium operations also continue change. New venues purposefully constructed provide fans convenient immersive experiences. ’ve concerned years terrorist attack large public sporting event revolutionize approach ticketing, ingress, egress. thing happened 2001 attacks World Trade Center permanently altered air travel 100. ’ll see massive changes . Whether changes improve experience debate.Additionally, everything incremental. Nothing new concepts book. Neural Networks around decades. Looking back old textbook college, “Decision Support Systems Intelligent Systems,” published 2001 (Efraim Turban 2001), reviewed chapter Neural Networks. Fundamentally, everything stayed . Incremental hardware, software, mathematics improvements pushed technologies . final chapter book, section entitled “future Management Support Systems.” couple predictions stood :Groupware technologies collaboration communication become easier use, powerful, less expensive. make electronic group support viable initiative even small organizations.Thank , COVID-19. Zoom, Slack, Microsoft Teams, many technologies now ubiquitous necessary. magical Metaverse might take concept even , interact digital version 3D environment. Let’s meeting top Denali. don’t know cool. Let’s look another prediction:Using voice technologies natural language processing facilitate usage MSS.Bingo. Amazon Alexa, Apple’s Siri, many others dramatically expanded use NLP. Thank , neural networks. technologies may supplant keyboard-based search unpredictably alter companies created . want share one :Frontline decision support technologies mainly supporting CRM become integral .T. medium-sized large corporations.accurate quote section. Salesforce become dominant player continues expanding capabilities acquisition (2019 Tableau 2020 Slack) 101. CRM platforms increasingly using NLP enable conversational intelligence. However, developments take long time mature. Almost 20 years quote written, growth still progressing rapidly.Uncertainty, egos, intelligence, luck breed innovation, problems clubs face complex ever. core business isn’t efficient, prohibits working potentially rewarding projects. book gives good idea implement basic analytical tasks serve strategic initiatives managers throughout organization. Please take learn improve .","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
